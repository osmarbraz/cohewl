{"cells":[{"cell_type":"markdown","metadata":{"id":"EKOTlwcmxmej"},"source":["# Perturbação de sentenças do CohQuAD Co pt-br\n","\n","Gera as perturbações de sentenças dos documentos pré-processados do conjunto de dados.\n","\n","- Utiliza os dados pré-processados de `original.zip`.\n","- Gera o arquivo `perturbado_pX_kY.zip` com as o documento com as perturbações.\n"," \n","Gera o arquivo `perturbado_pX_kY.csv`, onde X é o número de documentos perturbados e Y o valor de top K predições. Contêm a compactação do arquivo `perturbado.csv`.\n","\n","Cada linha de `perturbardo.csv` é formado por `[\"id\",\"perturbado\", \"documento_perturbado\", \"sentencas\"]`.\n"," - `\"id\"` é o idenficador da pergunta na base de dados original.\n"," - `\"perturbado\"` é uma lista com as sentenças perturbadas do documento. \n"," - `\"documento_perturbado\"` é um string com as sentenças perturbadas do documento. \n"," - `\"sentencas\"` uma lista com os dados das sentenças do documento perturbado. Cada elemento da lista é formado por:\n","    - `\"sentenca_perturbada\"` A sentença com a perturbação.\n","    - `\"sentenca_mascarada\"` A sentença mascarada.\n","    - `\"palavra_mascarada\"` A palavra da sentença que foi mascarada.\n","    - `\"token_predito\"` O token predito para a palavra mascarada.\n","    - `\"peso_predito\"` O peso do token predito para a palavra mascarada.\n","    - `\"token_predito_marcado\"` A token em sua forma original.\n"]},{"cell_type":"markdown","metadata":{"id":"OP33KWAtBMWs"},"source":["# 1 Preparação do ambiente\n","\n","Preparação do ambiente para execução do script."]},{"cell_type":"markdown","metadata":{"id":"PKUr9Vk4BNLC"},"source":["## 1.1 Tempo inicial de processamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXclHCRQBSF2"},"outputs":[],"source":["# Import das bibliotecas.\n","import time\n","import datetime\n","\n","# Marca o tempo de início do processamento\n","inicio_processamento = time.time()"]},{"cell_type":"markdown","metadata":{"id":"GOcN8hK-scnt"},"source":["## 1.2 Funções e classes auxiliares"]},{"cell_type":"markdown","metadata":{"id":"OPRnA-mk5-c4"},"source":["Verifica se existe o diretório cohebert no diretório corrente.   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fj5TaAH_5-nB"},"outputs":[],"source":["# Import das bibliotecas.\n","import os # Biblioteca para manipular arquivos\n","\n","# ============================  \n","def verificaDiretorioCoheBERT():\n","    \"\"\"\n","      Verifica se existe o diretório cohebert no diretório corrente.    \n","    \"\"\"\n","    \n","    # Verifica se o diretório existe\n","    if not os.path.exists(DIRETORIO_COHEBERT):  \n","        # Cria o diretório\n","        os.makedirs(DIRETORIO_COHEBERT)\n","        logging.info(\"Diretório Cohebert criado: {}\".format(DIRETORIO_COHEBERT))\n","    \n","    return DIRETORIO_COHEBERT"]},{"cell_type":"markdown","metadata":{"id":"yDCOeh2y5jOH"},"source":["Realiza o download e um arquivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5B1mvfAU5jZf"},"outputs":[],"source":["# Import das bibliotecas.\n","import requests # Biblioteca de download\n","from tqdm.notebook import tqdm as tqdm_notebook # Biblioteca para barra de progresso\n","import os # Biblioteca para manipular arquivos\n","\n","def downloadArquivo(url_arquivo, nome_arquivo_destino):\n","    \"\"\"\n","      Realiza o download de um arquivo de uma url em salva em nome_arquivo_destino.\n","    \n","      Parâmetros:\n","        `url_arquivo` - URL do arquivo a ser feito download.      \n","        `nome_arquivo_destino` - Nome do arquivo a ser salvo.      \n","    \"\"\"\n","    \n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","    \n","    # Realiza o download de um arquivo em uma url\n","    data = requests.get(url_arquivo, stream=True)\n","    \n","    # Verifica se o arquivo existe\n","    if data.status_code != 200:\n","        logging.info(\"Exceção ao tentar realizar download {}. Response {}.\".format(url_arquivo, data.status_code))\n","        data.raise_for_status()\n","        return\n","\n","    # Recupera o nome do arquivo a ser realizado o download    \n","    nome_arquivo = nome_arquivo_destino.split(\"/\")[-1]  \n","\n","    # Define o nome e caminho do arquivo temporário    \n","    nome_arquivo_temporario = DIRETORIO_COHEBERT + \"/\" + nome_arquivo + \"_part\"\n","    \n","    logging.info(\"Download do arquivo: {}.\".format(nome_arquivo_destino))\n","    \n","    # Baixa o arquivo\n","    with open(nome_arquivo_temporario, \"wb\") as arquivo_binario:        \n","        tamanho_conteudo = data.headers.get(\"Content-Length\")        \n","        total = int(tamanho_conteudo) if tamanho_conteudo is not None else None\n","        # Barra de progresso de download\n","        progresso_bar = tqdm_notebook(unit=\"B\", total=total, unit_scale=True)                \n","        # Atualiza a barra de progresso\n","        for chunk in data.iter_content(chunk_size=1024):        \n","            if chunk:                \n","                progresso_bar.update(len(chunk))\n","                arquivo_binario.write(chunk)\n","    \n","    # Renomeia o arquivo temporário para o arquivo definitivo\n","    os.rename(nome_arquivo_temporario, nome_arquivo_destino)\n","    \n","    # Fecha a barra de progresso.\n","    progresso_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"ksYnRk7zLGp0"},"source":["Remove tags de um documento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qwKjGvyLG4v"},"outputs":[],"source":["def remove_tags(documento):\n","    \"\"\"\n","      Remove tags de um documento\n","    \"\"\"\n","    \n","    import re\n","\n","    documentoLimpo = re.compile(\"<.*?>\")\n","    return re.sub(documentoLimpo, \"\", documento)"]},{"cell_type":"markdown","metadata":{"id":"4pduTsINLeaz"},"source":["Funções auxiliares de arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jirIzIstLea0"},"outputs":[],"source":["def carregar(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como um único parágrafo(texto).\n","    \n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.  \n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","    \n","    paragrafo = \"\"\n","    for linha in arquivo:\n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        # Remove as tags existentes no final das linhas\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          paragrafo = paragrafo + linha.strip() + \" \"\n","    \n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    # Remove os espaços em branco antes e depois do parágrafo\n","    return paragrafo.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC9Xppq-_R0w"},"outputs":[],"source":["def carregarLista(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como uma lista de sentenças(texto).\n","    \n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.   \n","        `encoding` - Codificação dos caracteres do arquivo.\n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","    \n","    sentencas = []\n","    for linha in arquivo:        \n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          sentencas.append(linha.strip())\n","    \n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    return sentencas "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkVk5LQT_G3f"},"outputs":[],"source":["def salvar(nome_arquivo,texto):                       \n","    \"\"\"\n","      Salva um texto em arquivo.\n","     \n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser salvo.\n","        `texto` - Texto a ser salvo.     \n","    \"\"\"\n","\n","    arquivo = open(nome_arquivo, \"w\")\n","    arquivo.write(str(texto))\n","    arquivo.close()"]},{"cell_type":"markdown","metadata":{"id":"603LYIYKBmq5"},"source":["Função auxiliar para formatar o tempo como `hh: mm: ss`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guy6B4whsZFR"},"outputs":[],"source":["# Import das bibliotecas.\n","import time\n","import datetime\n","\n","def formataTempo(tempo):\n","    \"\"\"\n","      Pega a tempo em segundos e retorna uma string hh:mm:ss\n","    \"\"\"\n","    # Arredonda para o segundo mais próximo.\n","    tempo_arredondado = int(round((tempo)))\n","    \n","    # Formata como hh:mm:ss\n","    return str(datetime.timedelta(seconds=tempo_arredondado))    "]},{"cell_type":"markdown","metadata":{"id":"zVKAapz7RCxk"},"source":["Classe(ModelArguments) de definição dos parâmetros do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgmN6RqDRDZS"},"outputs":[],"source":["# Import das bibliotecas.\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","from typing import List\n","\n","@dataclass\n","class ModeloArgumentosMedida:\n","    max_seq_len: Optional[int] = field(\n","        default=None,\n","        metadata={\"help\": \"max seq len\"},\n","    )    \n","    pretrained_model_name_or_path: str = field(\n","        default=\"neuralmind/bert-base-portuguese-cased\",\n","        metadata={\"help\": \"nome do modelo pré-treinado do BERT.\"},\n","    )\n","    modelo_spacy: str = field(\n","        default=\"pt_core_news_lg\",\n","        metadata={\"help\": \"nome do modelo do spaCy.\"},\n","    )\n","    versao_modelo_spacy: str = field(\n","        default=\"-3.2.0\",\n","        metadata={\"help\": \"versão do nome do modelo no spaCy.\"},\n","    )\n","    sentenciar_documento: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Dividir o documento em sentenças(frases).\"},\n","    )\n","    do_lower_case: bool = field(\n","        default=False,\n","        metadata={\"help\": \"define se o texto do modelo deve ser todo em minúsculo.\"},\n","    )    \n","    output_attentions: bool = field(\n","        default=False,\n","        metadata={\"help\": \"habilita se o modelo retorna os pesos de atenção.\"},\n","    )\n","    output_hidden_states: bool = field(\n","        default=False,\n","        metadata={\"help\": \"habilita gerar as camadas ocultas do modelo.\"},\n","    )\n","    usar_mcl_ajustado : bool = field(\n","        default=False,\n","        metadata={\"help\": \"habilita o carragamento de mcl ajustado.\"},\n","    )\n","    documentos_perturbados: int = field(\n","        default=\"1\",\n","        metadata={\"help\": \"Quantidade de documentos a serem perturbados a partir do original.\"},\n","    )\n","    top_k_predicao: int = field(\n","        default=\"100\",\n","        metadata={\"help\": \"Quantidade de palavras a serem recuperadas mais próximas da máscara.\"},\n","    )    "]},{"cell_type":"markdown","metadata":{"id":"SX6jTGkBMNvV"},"source":["Biblioteca de limpeza de tela\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95qYX7uzMNvX"},"outputs":[],"source":["# Import das bibliotecas.\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"iAPVtRXQqDim"},"source":["## 1.3 Tratamento de logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcopxbGZqDip"},"outputs":[],"source":["# Import das bibliotecas.\n","import logging # Biblioteca de logging\n","\n","# Formatando a mensagem de logging\n","logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"_GjYtXcMnSAe"},"source":["## 1.4 Identificando o ambiente Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMiH0E3OnRa1"},"outputs":[],"source":["# Import das bibliotecas.\n","import sys # Biblioteca para acessar módulos do sistema\n","\n","# Se estiver executando no Google Colaboratory\n","# Retorna true ou false se estiver no Google Colaboratory\n","IN_COLAB = \"google.colab\" in sys.modules"]},{"cell_type":"markdown","metadata":{"id":"RinFHFesVKis"},"source":["## 1.5 Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"MPngEboiVbfi"},"source":["Usando Colab GPU para Treinamento\n"]},{"cell_type":"markdown","metadata":{"id":"EjWE6WlvVbfj"},"source":["Uma GPU pode ser adicionada acessando o menu e selecionando:\n","\n","`Edit -> Notebook Settings -> Hardware accelerator -> (GPU)`\n","\n","Em seguida, execute a célula a seguir para confirmar que a GPU foi detectada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtaYZmc3Vbfj"},"outputs":[],"source":["# Import das bibliotecas.\n","import tensorflow as tf\n","\n","# Recupera o nome do dispositido da GPU.\n","device_name = tf.test.gpu_device_name()\n","\n","# O nome do dispositivo deve ser parecido com o seguinte:\n","if device_name == \"/device:GPU:0\":\n","    logging.info(\"Encontrei GPU em: {}\".format(device_name))\n","else:\n","    logging.info(\"Dispositivo GPU não encontrado\")\n","    #raise SystemError(\"Dispositivo GPU não encontrado\")"]},{"cell_type":"markdown","metadata":{"id":"iYRrUo2XWa8G"},"source":["Nome da GPU\n","\n","Para que a torch use a GPU, precisamos identificar e especificar a GPU como o dispositivo. Posteriormente, em nosso ciclo de treinamento, carregaremos dados no dispositivo.\n","\n","Vale a pena observar qual GPU você recebeu. A GPU Tesla P100 é muito mais rápido que as outras GPUs, abaixo uma lista ordenada:\n","- 1o Tesla P100\n","- 2o Tesla T4\n","- 3o Tesla P4 (Não tem memória para execução 4 x 8, somente 2 x 4)\n","- 4o Tesla K80 (Não tem memória para execução 4 x 8, somente 2 x 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrjqDO6nWa8J"},"outputs":[],"source":["# Import das bibliotecas.\n","import torch\n","\n","def getDeviceGPU():\n","    \"\"\"\n","      Retorna um dispositivo de GPU se disponível ou CPU.\n","    \n","      Retorno:\n","        `device` - Um device de GPU ou CPU.       \n","    \"\"\"\n","        \n","    # Se existe GPU disponível.\n","    if torch.cuda.is_available():\n","        \n","        # Diz ao PyTorch para usar GPU.    \n","        device = torch.device(\"cuda\")\n","        \n","        logging.info(\"Existem {} GPU(s) disponíveis.\".format(torch.cuda.device_count()))\n","        logging.info(\"Iremos usar a GPU: {}.\".format(torch.cuda.get_device_name(0)))\n","\n","    # Se não.\n","    else:        \n","        logging.info(\"Sem GPU disponível, usando CPU.\")\n","        device = torch.device(\"cpu\")\n","        \n","    return device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChDxmtXsKwjf"},"outputs":[],"source":["# Recupera o device com GPU ou CPU\n","device = getDeviceGPU()"]},{"cell_type":"markdown","metadata":{"id":"fGf59D0yVNx9"},"source":["Memória\n","\n","Memória disponível no ambiente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iC5-pSAVh7_"},"outputs":[],"source":["# Importando as bibliotecas.\n","from psutil import virtual_memory\n","\n","ram_gb = virtual_memory().total / 1e9\n","logging.info(\"Seu ambiente de execução tem {: .1f} gigabytes de RAM disponível\\n\".format(ram_gb))\n","\n","if ram_gb < 20:\n","  logging.info(\"Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \\\"Alterar tipo de tempo de execução\\\"\")\n","  logging.info(\"e selecione High-RAM. Então, execute novamente está célula\")\n","else:\n","  logging.info(\"Você está usando um ambiente de execução de memória RAM alta!\")"]},{"cell_type":"markdown","metadata":{"id":"wijMXooQQLcQ"},"source":["## 1.6 Monta uma pasta no google drive para carregar os arquivos de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysnDDapMQK8K"},"outputs":[],"source":["# import necessário\n","from google.colab import drive\n","\n","# Monta o drive na pasta especificada\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"u66iRrtwMrqy"},"source":["## 1.7 Instalação do wandb"]},{"cell_type":"markdown","metadata":{"id":"dQd3BrhvMzZs"},"source":["Instalação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejzpgGrFM0-j"},"outputs":[],"source":["!pip install --upgrade wandb"]},{"cell_type":"markdown","metadata":{"id":"oOd2MbBiDq93"},"source":["## 1.8 Instalação do spaCy\n","\n","https://spacy.io/\n","\n","Modelos do spaCy para português:\n","https://spacy.io/models/pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaMM4WdxgvQ7"},"outputs":[],"source":["# Instala o spacy\n","!pip install -U pip setuptools wheel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4p3Rz2qDq94"},"outputs":[],"source":["# Instala uma versão específica\n","!pip install -U spacy==3.2.0"]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["## 1.9 Instalação do BERT"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["Instala a interface pytorch para o BERT by Hugging Face. \n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RfUN_KolV-f"},"outputs":[],"source":["!pip install -U transformers==4.5.1"]},{"cell_type":"markdown","metadata":{"id":"8bGda5JgMtQe"},"source":["# 2 Parametrização"]},{"cell_type":"markdown","metadata":{"id":"ifrYNTwGwKal"},"source":["## Gerais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uiH9pNpwI6g"},"outputs":[],"source":["# Definição dos parâmetros a serem avaliados\n","#Quantidade de documentos a serem perturbados a partir do original.\n","DOCUMENTOS_PERTURBADOS = 1\n","\n","#Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","TOP_K_PREDICAO = 1\n","\n","# Gera a seleção aleatória da palara perturbada entre as topk palavras da predição\n","SELECAO_ALEATORIA_TOP_K = False"]},{"cell_type":"markdown","metadata":{"id":"mhByVujAwNAU"},"source":["## Específicos"]},{"cell_type":"markdown","metadata":{"id":"Mhkc9sW21zV7"},"source":["Parâmetros do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ15-ylRRRdD"},"outputs":[],"source":["# Definição dos parâmetros do Modelo.\n","model_args = ModeloArgumentosMedida(     \n","    max_seq_len = 512,\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\",\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\",\n","    \n","    #pretrained_model_name_or_path = \"bert-large-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-cased\"\n","    pretrained_model_name_or_path = \"neuralmind/bert-large-portuguese-cased\",\n","    #pretrained_model_name_or_path = \"neuralmind/bert-base-portuguese-cased\",    \n","    #pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-multilingual-uncased\",\n","\n","    #modelo_spacy = \"en_core_web_lg\",\n","    #modelo_spacy = \"en_core_web_md\",\n","    #modelo_spacy = \"en_core_web_sm\",\n","    modelo_spacy = \"pt_core_news_lg\",\n","    #modelo_spacy = \"pt_core_news_md\",\n","    #modelo_spacy = \"pt_core_news_sm\",\n","\n","    versao_modelo_spacy = \"3.2.0\",\n","    sentenciar_documento = True,\n","    do_lower_case = False, # default True  \n","    output_attentions = False, # default False\n","    output_hidden_states = True, # default False, se True retorna todas as camadas do modelo para as operações de soma e concatenação\n","    usar_mcl_ajustado = False, # Especifica se deve ser carregado um MCL ajustado ou pré-treinado. Necessário especificar o tipo do modelo em pretrained_model_name_or_path. \n","    documentos_perturbados = DOCUMENTOS_PERTURBADOS, # Quantidade de documentos a serem perturbados a partir do original.\n","    top_k_predicao = TOP_K_PREDICAO, # Conjunto de valores: 1, 10, 100 e 1000. Quantidade de palavras a serem recuperadas mais próximas da máscara.\n",")"]},{"cell_type":"markdown","metadata":{"id":"ecwtQDArKvZn"},"source":["## Nome do diretório dos arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtNygH9qKvmp"},"outputs":[],"source":["# Diretório do cohebert\n","DIRETORIO_COHEBERT = \"COHQUAD_CO_PTBR\""]},{"cell_type":"markdown","metadata":{"id":"SUxlx7Sx4yxj"},"source":["## Define o caminho para os arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gQpxAO74yxj"},"outputs":[],"source":["# Diretório local para os arquivos pré-processados\n","DIRETORIO_LOCAL = \"/content/\" + DIRETORIO_COHEBERT + \"/\"\n","\n","# Diretório no google drive com os arquivos pré-processados\n","DIRETORIO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/\""]},{"cell_type":"markdown","metadata":{"id":"L7G3-MOsQ1N_"},"source":["# 3 spaCy"]},{"cell_type":"markdown","metadata":{"id":"35GwcgkOlWi3"},"source":["## 3.1 Download arquivo modelo\n","\n","https://spacy.io/models/pt"]},{"cell_type":"markdown","metadata":{"id":"PWd_9X0nOYnF"},"source":["### Função download modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjWGu-9D5URZ"},"outputs":[],"source":["def downloadSpacy(model_args):\n","    \"\"\"\n","      Realiza o download do arquivo do modelo para o diretório corrente.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.       \n","    \"\"\"\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","        \n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Nome arquivo compactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","    \n","    # Url do arquivo\n","    URL_ARQUIVO_MODELO_COMPACTADO = \"https://github.com/explosion/spacy-models/releases/download/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO\n","\n","    # Realiza o download do arquivo do modelo\n","    logging.info(\"Download do arquivo do modelo do spaCy.\")\n","    downloadArquivo(URL_ARQUIVO_MODELO_COMPACTADO, DIRETORIO_COHEBERT + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"Uu_LkF7Nfm8_"},"source":["## 3.2 Descompacta o arquivo do modelo"]},{"cell_type":"markdown","metadata":{"id":"XAc1tSwvOc4d"},"source":["### Função descompacta modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq9PnXO77bPQ"},"outputs":[],"source":["# Import das bibliotecas.\n","import tarfile # Biblioteca de descompactação\n","\n","def descompactaSpacy(model_args):\n","    \"\"\"\n","      Descompacta o arquivo do modelo.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.       \n","    \"\"\"\n","    \n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","    \n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    \n","    # Nome do arquivo a ser descompactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","    \n","    logging.info(\"Descompactando o arquivo do modelo do spaCy.\")\n","    arquivo_tar = tarfile.open(NOME_ARQUIVO_MODELO_COMPACTADO, \"r:gz\")    \n","    arquivo_tar.extractall(DIRETORIO_COHEBERT)    \n","    arquivo_tar.close()\n","    \n","    # Apaga o arquivo compactado\n","    if os.path.isfile(NOME_ARQUIVO_MODELO_COMPACTADO):        \n","        os.remove(NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"STHT2c89qvwK"},"source":["## 3.3 Carrega o modelo"]},{"cell_type":"markdown","metadata":{"id":"3iFBoyWMOgKz"},"source":["### Função carrega modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePOccj0s8WMg"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy # Biblioteca do spaCy\n","\n","def carregaSpacy(model_args):\n","    \"\"\"\n","    Realiza o carregamento do Spacy.\n","    \n","    Parâmetros:\n","      `model_args` - Objeto com os argumentos do modelo.           \n","    \"\"\"\n","    \n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","                  \n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Caminho raoz do modelo do spaCy\n","    DIRETORIO_MODELO_SPACY =  DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY\n","\n","    # Verifica se o diretório existe\n","    if os.path.exists(DIRETORIO_MODELO_SPACY) == False:\n","        # Realiza o download do arquivo modelo do spaCy\n","        downloadSpacy(model_args)\n","        # Descompacta o spaCy\n","        descompactaSpacy(model_args)\n","\n","    # Diretório completo do spaCy\n","    DIRETORIO_MODELO_SPACY = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\"\n","\n","    # Carrega o spaCy. Necessário somente \"tagger\" para encontrar os substantivos\n","    nlp = spacy.load(DIRETORIO_MODELO_SPACY)\n","    logging.info(\"spaCy carregado.\")\n","\n","    # Retorna o spacy carregado\n","    return nlp "]},{"cell_type":"markdown","metadata":{"id":"cAk5hHx7OnHn"},"source":["### Carrega o modelo spaCy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbELnrpgA4T1"},"outputs":[],"source":["# Carrega o modelo spaCy\n","nlp = carregaSpacy(model_args)"]},{"cell_type":"markdown","metadata":{"id":"Gk9I_DX5coOI"},"source":["# 4 BERT"]},{"cell_type":"markdown","metadata":{"id":"MBGTMy8Ic7GK"},"source":["## 4.1 Modelo Pré-treinado BERT"]},{"cell_type":"markdown","metadata":{"id":"uiuxdXe9t1BX"},"source":["### Funções Auxiliares"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Huw0x5kt1Le"},"outputs":[],"source":["def getNomeModeloBERT(model_args):\n","    '''    \n","    Recupera uma string com uma descrição do modelo BERT para nomes de arquivos e diretórios.\n","    \n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.       \n","    \n","    Retorno:\n","    `MODELO_BERT` - Nome do modelo BERT.\n","    '''\n","\n","    # Verifica o nome do modelo(default SEM_MODELO_BERT)\n","    MODELO_BERT = \"SEM_MODELO_BERT\"\n","    \n","    if 'neuralmind' in model_args.pretrained_model_name_or_path:\n","        MODELO_BERT = \"_BERTimbau\"\n","        \n","    else:\n","        if 'multilingual' in model_args.pretrained_model_name_or_path:\n","            MODELO_BERT = \"_BERTmultilingual\"\n","            \n","    return MODELO_BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYJB4ik7t5xe"},"outputs":[],"source":["def getTamanhoBERT(model_args):\n","    '''    \n","    Recupera uma string com o tamanho(dimensão) do modelo BERT para nomes de arquivos e diretórios.\n","    \n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.       \n","    \n","    Retorno:\n","    `TAMANHO_BERT` - Nome do tamanho do modelo BERT.\n","    '''\n","    \n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = \"_large\"\n","    \n","    if 'base' in model_args.pretrained_model_name_or_path:\n","        TAMANHO_BERT = \"_base\"\n","        \n","    return TAMANHO_BERT  "]},{"cell_type":"markdown","metadata":{"id":"rHt4e5pAcEMd"},"source":["### Função download Modelo Pre-treinado BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peDUrV2ccEXA"},"outputs":[],"source":["# Import das bibliotecas.\n","import zipfile # Biblioteca para descompactar\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def downloadModeloPretreinado(model_args):\n","    \"\"\"\n","      Realiza o download do modelo BERT(MODELO) e retorna o diretório onde o modelo BERT(MODELO) foi descompactado.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \n","      Retorno:\n","        `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\" \n","    \n","    # Nome diretório base modelo BERT\n","    NOME_DIRETORIO_BASE_MODELO = \"modeloBERT\"\n","    \n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","    \n","    # Recupera o nome ou caminho do modelo\n","    MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in MODELO:\n","        URL_MODELO = MODELO\n","\n","    # Se a variável foi setada.\n","    if URL_MODELO:\n","\n","        # Diretório do modelo.\n","        DIRETORIO_MODELO = DIRETORIO_COHEBERT + \"/\" + NOME_DIRETORIO_BASE_MODELO\n","        \n","        # Recupera o nome do arquivo do modelo da url.\n","        NOME_ARQUIVO = URL_MODELO.split(\"/\")[-1]\n","\n","        # Nome do arquivo do vocabulário.\n","        ARQUIVO_VOCAB = \"vocab.txt\"\n","        \n","        # Caminho do arquivo na url.\n","        CAMINHO_ARQUIVO = URL_MODELO[0:len(URL_MODELO)-len(NOME_ARQUIVO)]\n","\n","        # Verifica se o diretório de descompactação existe no diretório corrente\n","        if os.path.exists(DIRETORIO_MODELO):\n","            logging.info(\"Apagando diretório existente do modelo!\")\n","            # Apaga o diretório e os arquivos existentes                     \n","            shutil.rmtree(DIRETORIO_MODELO)\n","        \n","        # Realiza o download do arquivo do modelo        \n","        downloadArquivo(URL_MODELO, NOME_ARQUIVO)\n","\n","        # Descompacta o arquivo no diretório de descompactação.                \n","        arquivo_zip = zipfile.ZipFile(NOME_ARQUIVO, \"r\")\n","        arquivo_zip.extractall(DIRETORIO_MODELO)\n","\n","        # Baixa o arquivo do vocabulário.\n","        # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente.\n","        URL_MODELO_VOCAB = CAMINHO_ARQUIVO + ARQUIVO_VOCAB\n","        # Coloca o arquivo do vocabulário no diretório do modelo.        \n","        downloadArquivo(URL_MODELO_VOCAB, DIRETORIO_MODELO + \"/\" + ARQUIVO_VOCAB)\n","        \n","        # Apaga o arquivo compactado\n","        os.remove(NOME_ARQUIVO)\n","\n","        logging.info(\"Diretório {} do modelo BERT pronta.\".format(DIRETORIO_MODELO))\n","\n","    else:\n","        DIRETORIO_MODELO = MODELO\n","        logging.info(\"Variável URL_MODELO não setada.\")\n","\n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"V74WUpHqcfoI"},"source":["### Copia o modelo do BERT ajustado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQMpf9yycf8f"},"outputs":[],"source":["# Import das bibliotecas.\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def copiaModeloAjustado(model_args):\n","    \"\"\" \n","      Copia o modelo ajustado BERT do GoogleDrive para o projeto.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \n","      Retorno:\n","        `DIRETORIO_LOCAL_MODELO_AJUSTADO` - Diretório de download ajustado do modelo.\n","    \"\"\"\n","\n","    # Verifica o nome do modelo BERT a ser utilizado\n","    MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = getTamanhoBERT(model_args)\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Diretório local de salvamento do modelo.\n","    DIRETORIO_LOCAL_MODELO_AJUSTADO = DIRETORIO_COHEBERT + \"/modelo_ajustado/\"\n","\n","    # Diretório remoto de salvamento do modelo no google drive.\n","    DIRETORIO_REMOTO_MODELO_AJUSTADO = \"/content/drive/MyDrive/Colab Notebooks/Data/CSTNEWS/validacao_classificacao/holdout/modelo/\" + MODELO_BERT + TAMANHO_BERT\n","\n","    # Copia o arquivo do modelo para o diretório no Google Drive.\n","    shutil.copytree(DIRETORIO_REMOTO_MODELO_AJUSTADO, DIRETORIO_LOCAL_MODELO_AJUSTADO) \n","   \n","    logging.info(\"Modelo BERT ajustado copiado.\")\n","\n","    return DIRETORIO_LOCAL_MODELO_AJUSTADO"]},{"cell_type":"markdown","metadata":{"id":"eaneOhAKcO-3"},"source":["### Verifica de onde utilizar o modelo do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTy1TXz3cPKS"},"outputs":[],"source":["def verificaModelo(model_args):\n","    \"\"\" \n","    Verifica de onde utilizar o modelo.\n","    \n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","    \n","    Retorno:\n","    `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\" \n","\n","    DIRETORIO_MODELO = None\n","    \n","    if model_args.usar_mcl_ajustado == True:        \n","        # Diretório do modelo\n","        DIRETORIO_MODELO = copiaModeloAjustado()\n","        \n","        logging.info(\"Usando modelo BERT ajustado.\")\n","        \n","    else:\n","        DIRETORIO_MODELO = downloadModeloPretreinado(model_args)\n","        logging.info(\"Usando modelo BERT pré-treinado.\")        \n","        \n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"6tKcaIfReqdy"},"source":["## 4.2 Tokenizador BERT"]},{"cell_type":"markdown","metadata":{"id":"e8n7Z5s-QZF8"},"source":["### Função carrega Tokenizador BERT\n","\n","O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzAuptkwQZR3"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertTokenizer # Importando as bibliotecas do tokenizador BERT.\n","\n","def carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o tokenizador do DIRETORIO_MODELO.\n","      O tokenizador utiliza WordPiece.\n","      Carregando o tokenizador do diretório \"./modelo/\" do diretório padrão se variável `DIRETORIO_MODELO` setada.\n","      Caso contrário carrega da comunidade\n","      Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí..), que são necessárias a língua portuguesa.\n","      O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados.\n","    \n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.           \n","        `model_args` - Objeto com os argumentos do modelo.       \n","    \n","      Retorno:\n","        `tokenizer` - Tokenizador BERT.\n","    \"\"\"\n","\n","    tokenizer = None\n","    \n","    # Se a variável DIRETORIO_MODELO foi setada.\n","    if DIRETORIO_MODELO:\n","        # Carregando o Tokenizador.\n","        logging.info(\"Carregando o tokenizador BERT do diretório {}.\".format(DIRETORIO_MODELO))\n","\n","        tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, do_lower_case=model_args.do_lower_case)\n","\n","    else:\n","        # Carregando o Tokenizador da comunidade.\n","        logging.info(\"Carregando o tokenizador BERT da comunidade.\")\n","\n","        tokenizer = BertTokenizer.from_pretrained(model_args.pretrained_model_name_or_path, do_lower_case=model_args.do_lower_case)\n","\n","    return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"GYRV9KfHQE6v"},"source":["## 4.3 Carrega o modelo e tokenizador BERT\n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"markdown","metadata":{"id":"-pZZrUKRhR3e"},"source":["### Função carrega modelo BERT medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JUEyjCChUQh"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertForMaskedLM # Importando as bibliotecas do Modelo BERT.\n","\n","def carregaModeloMedida(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o modelo e retorna o modelo.\n","    \n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.           \n","        `model_args` - Objeto com os argumentos do modelo.   \n","    \n","      Retorno:\n","        `model` - Um objeto do modelo BERT carregado.\n","    \"\"\"\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in model_args.pretrained_model_name_or_path:\n","        URL_MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Se a variável URL_MODELO foi setada\n","    if URL_MODELO:        \n","        # Carregando o Modelo BERT\n","        logging.info(\"Carregando o modelo BERT do diretório {} para cálculo de medidas.\".format(DIRETORIO_MODELO))\n","\n","        model = BertForMaskedLM.from_pretrained(DIRETORIO_MODELO,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","        \n","    else:\n","        # Carregando o Modelo BERT da comunidade\n","        logging.info(\"Carregando o modelo BERT da comunidade {} para cálculo de medidas.\".format(model_args.pretrained_model_name_or_path))\n","\n","        model = BertForMaskedLM.from_pretrained(model_args.pretrained_model_name_or_path,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"-uFDhRTZe2Js"},"source":["### Função carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVtAUbUBe2iS"},"outputs":[],"source":["def carregaBERT(model_args):\n","    \"\"\" \n","      Carrega o BERT para cálculo de medida ou classificação e retorna o modelo e o tokenizador.\n","      O tipo do model retornado pode ser BertModel ou BertForSequenceClassification, depende do tipo de model_args.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.       \n","          - Se model_args = ModeloArgumentosClassificacao deve ser carregado o BERT para classificação(BertForSequenceClassification).\n","          - Se model_args = ModeloArgumentosMedida deve ser carregado o BERT para cálculo de medida(BertModel).\n","\n","      Retorno:    \n","        `model` - Um objeto do modelo BERT carregado.       \n","        `tokenizer` - Um objeto tokenizador BERT carregado.       \n","    \"\"\"\n","            \n","    # Verifica a origem do modelo\n","    DIRETORIO_MODELO = verificaModelo(model_args)\n","    \n","    # Variável para conter o modelo\n","    model = None\n","    \n","    # Carrega o modelo para cálculo da medida\n","    model = carregaModeloMedida(DIRETORIO_MODELO, model_args)\n","                \n","    # Carrega o tokenizador. \n","    # O tokenizador é o mesmo para o classificador e medidor.\n","    tokenizer = carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args)\n","    \n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"x5NTxBRKfAcT"},"source":["### Carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYMLJJYSQHY3"},"outputs":[],"source":["# Carrega o modelo e tokenizador do BERT\n","model, tokenizer = carregaBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"d7KprWqyZBQZ"},"source":["### Recupera detalhes do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6sPjTQnuQV2"},"outputs":[],"source":["# Verifica o nome do modelo BERT a ser utilizado\n","MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","# Verifica o tamanho do modelo(default large)\n","TAMANHO_BERT = getTamanhoBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"guw6ZNtaswKc"},"source":["# 5 Perturbação das sentenças\n"]},{"cell_type":"markdown","metadata":{"id":"7aZJTfpz1mIk"},"source":["## 5.1 Carregamento dos arquivos de dados"]},{"cell_type":"markdown","metadata":{"id":"_wTNIiyE1mIl"},"source":["### 5.1.1 Especifica os nomes dos arquivos de dados originais\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rp3C_XK1mIl"},"outputs":[],"source":["# Nome do arquivo\n","NOME_ARQUIVO_ORIGINAL = \"original.csv\"\n","NOME_ARQUIVO_ORIGINALCOMPACTADO = \"original.zip\"\n","NOME_ARQUIVO_ORIGINAL_POS = \"originalpos.csv\"\n","NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO = \"originalpos.zip\""]},{"cell_type":"markdown","metadata":{"id":"HvkGO02lmaY-"},"source":["### 5.1.2 Cria o diretório local para receber os dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFYIHcIHE985"},"outputs":[],"source":["# Biblioteca para acessar o sistema de arquivos\n","import os\n","\n","#Cria o diretório para receber os arquivos Originais e Perturbados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL[:-1]\n","\n","if not os.path.exists(dirbase):  \n","    # Cria o diretório\n","    os.makedirs(dirbase)    \n","    logging.info(\"Diretório criado: {}.\".format(dirbase))\n","else:    \n","    logging.info(\"Diretório já existe: {}.\".format(dirbase))"]},{"cell_type":"markdown","metadata":{"id":"rGnyGoyu1mIl"},"source":["### 5.1.3 Copia os arquivos do Google Drive para o Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"uuiMbbAfmqcb"},"source":["Copia os arquivos do google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCl5TpK91mIl"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINALCOMPACTADO\" \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINALCOMPACTADO\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\"\n","\n","  logging.info(\"Terminei a cópia.\")"]},{"cell_type":"markdown","metadata":{"id":"rFCvZ6CUmt-9"},"source":["Descompacta os arquivos.\n","\n","Usa o unzip para descompactar:\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens \n","*   `-d` Diretório de destino\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbHl3d88mouc"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINALCOMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a descompactação.\")"]},{"cell_type":"markdown","metadata":{"id":"WkWm5KWc1mIm"},"source":["### 5.1.4 Carregamento das lista com os dados dos arquivos originais"]},{"cell_type":"markdown","metadata":{"id":"Fvoe9l2o1mIm"},"source":["#### Carrega o arquivo dos dados originais e POS\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QRHlixdHEDTb"},"outputs":[],"source":["# Import das bibliotecas.\n","import pandas as pd\n","\n","# Abre o arquivo e retorna o DataFrame\n","lista_documentos_originais = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_originais_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJ5STBZPLlie"},"outputs":[],"source":["lista_documentos_originais.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LDhF8NkFjG5"},"outputs":[],"source":["lista_documentos_originais_pos.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"T5WkUTU-9a5M"},"source":["#### Corrigir os tipos de colunas dos dados originais e POS\n","\n","Em dados originais:\n","- coluna 1 - `sentenças` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados originais pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lj9sJVavMccj"},"outputs":[],"source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","# Verifica se o tipo da coluna não é list e converte\n","lista_documentos_originais[\"sentencas\"] = lista_documentos_originais[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","lista_documentos_originais_pos[\"pos_documento\"] = lista_documentos_originais_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","logging.info(\"TERMINADO CORREÇÃO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO CORREÇÃO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"]},{"cell_type":"markdown","metadata":{"id":"lRNu1gPKuRxd"},"source":["## 5.2 Gerando as perturbações"]},{"cell_type":"markdown","metadata":{"id":"NsBImnwiGFVE"},"source":["### 5.2.1 Especifica os nomes dos arquivos de dados perturbados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gSzrHQRGJpW"},"outputs":[],"source":["# Nome do arquivo\n","NOME_ARQUIVO_PERTURBADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_COMPACTADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\""]},{"cell_type":"markdown","metadata":{"id":"70dvLWiZePmo"},"source":["### 5.2.2 Gerando as perturbações"]},{"cell_type":"markdown","metadata":{"id":"bkPRy5TyVqUG"},"source":["Conta o número de ocorrências do elemento na lista."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp9X6-3VN90o"},"outputs":[],"source":["def contaElemento(lista, elemento):\n","    \"\"\" \n","      Conta o número de ocorrências do elemento na lista.\n","          \n","      Parâmetros:\n","        `lista` - Lista com os elementos.\n","        `elemento` - Elemento a ser contado a ocorrência na lista.\n","\n","      Retorno:    \n","        `cont` - Quantidade de ocorrências de elmento na lista.\n","    \"\"\"\n","    cont = 0\n","    # Percorre a lista\n","    for i, linha in enumerate(lista):      \n","      # Verifica se o elemento existe na lista\n","      if linha in elemento:\n","        # conta o elemento\n","        cont = cont + 1\n","    return cont"]},{"cell_type":"markdown","source":["#### Gera sentença mascarada aleatória"],"metadata":{"id":"a7wlD9LWi6Sx"}},{"cell_type":"markdown","metadata":{"id":"kJyTCimeVubM"},"source":["Gera a sentença mascarada com [MAKS] para usar com MLM do BERT."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQBCxLcqBsU8"},"outputs":[],"source":["# Import das bibliotecas.\n","from random import randint # Biblioteca para o sorteio\n","\n","def getSentencaMascarada(sentenca, \n","                         sentenca_token,\n","                         sentenca_pos, \n","                         classe=[\"VERB\",\"NOUN\",\"AUX\"], \n","                         qtde=1):\n","  \"\"\" \n","      Gera a sentença mascarada com [MAKS] para usar com MLM do BERT.\n","      Considera determinadas classes morfossintática das palavras e uma quantidade(qtde) de palavras a serem mascaradas.\n","          \n","      Parâmetros:\n","        `sentenca` - Sentença a ser mascarada.\n","        `sentenca_token` - Lista com os tokens da sentença.\n","        `sentenca_pos` - Lista com as POS dos tokens da sentença.\n","        `classe` - Lista com as classes morfossintática das palavras a serem mascarada com [MASK].\n","        `qtde` - Quantidade de mascarada a serem realizadas nas palavras das sentenças.\n","                 Seleciona aleatoriamente a(s) palavra(s) a ser(em) mascarada(s) se a qtde \n","                 for menor que quantidade de palavras das classes na sentença.\n","\n","      Retorno:    \n","        `sentenca_mascarada` - Sentença mascarada.\n","        `palavra_mascarada` - Lista com as palavras substituidas pela máscara.\n","\n","  \"\"\"\n","  sentenca_mascarada = \"\"\n","  palavra_mascarada = \"\"\n","\n","  # Verifica a quantidade de trocas a ser realizada\n","  if qtde != 0:\n","\n","    # Conta o número de palavras das classes especificadas\n","    if len(classe) > 1:\n","      # Se tem duas classes usa a primeira para contar se existe uma palavra\n","      # Pega o primeiro para realizar a conta\n","      classe_conta = [classe[0]]\n","      conta_mascara = contaElemento(sentenca_pos, classe_conta)\n","      \n","      # Senão encontrar pega a segunda classe\n","      if conta_mascara == 0:\n","        #Pega a segunda classe\n","        classe_conta = [classe[1]]\n","        conta_mascara = contaElemento(sentenca_pos, classe_conta)\n","\n","        # Senão encontrar pega a terceira classe\n","        if conta_mascara == 0:\n","          #Pega a terceira classe\n","          classe_conta = [classe[2]]\n","          conta_mascara = contaElemento(sentenca_pos, classe_conta) \n","      \n","      # Usa a classe para gerar a sentença mascarada\n","      classe = classe_conta\n","    else:\n","      conta_mascara = contaElemento(sentenca_pos, classe)\n","    \n","    # Verifica se existe palavras das classes a serem mascaradas\n","    if conta_mascara != 0:    \n","      # Verifica a quantidade de trocas é menor que a quantidade palavras a serem trocadas encontradas\n","      if qtde < conta_mascara:\n","        # A quantidade de trocas é menor que a quantidade de palavras existentes\n","        # Precisa sortear as posições que serão trocadas pela máscara dentro da quantidade\n","               \n","        roleta = []\n","        # preenche a roleta com o indice das palavras as serem mscaradas\n","        for i in range(conta_mascara):\n","            roleta.append(i)\n","\n","        # Sorteia as posições das trocas\n","        posicao = []\n","        for i in range(qtde):\n","            posicao_sorteio = randint(0, len(roleta)-1)\n","            # Guarda o número sorteado\n","            posicao.append(roleta[posicao_sorteio])\n","            # Remove o elemento sorteado da roleta\n","            del roleta[posicao_sorteio]\n","        \n","        # Conta o número das trocas realizadas\n","        troca = 0\n","\n","        # Substitui o elemento pela máscara\n","        for i, token in enumerate(sentenca_token):            \n","            # Se a classe da palavra é a desejada\n","            if sentenca_pos[i] in classe:\n","                # Verifica se a troca deve ser realizada para a posição\n","                if troca in posicao:      \n","                  # Trocar palavra da classe por [MASK]\n","                  sentenca_mascarada = sentenca_mascarada + \"[MASK]\" + \" \"    \n","                  # Guarda a palavra que foi mascarada\n","                  palavra_mascarada = token                                  \n","                else:                  \n","                  # Adiciona o token\n","                  sentenca_mascarada = sentenca_mascarada + token + \" \"\n","                # Avança para a próxima troca\n","                troca = troca + 1\n","            else:\n","              # Adiciona o token\n","                sentenca_mascarada = sentenca_mascarada + token + \" \"\n","      else:        \n","        # Trocar todas as palavras pela mascará, pois a quantidade\n","        # de trocas é igual a quantidade de mascarás existentes na sentença\n","\n","        # Substitui o elemento da classe pela mascará\n","        for i, token in enumerate(sentenca_token):\n","            #print(token, sentenca_pos[i])        \n","            # Se a classe da palavra é a desejada\n","            if sentenca_pos[i] in classe:\n","                # Trocar palavra da classe por [MASK]\n","                sentenca_mascarada = sentenca_mascarada + \"[MASK]\" + \" \"    \n","                # Guarda a palavra que foi mascarada\n","                palavra_mascarada = token \n","            else:\n","                sentenca_mascarada = sentenca_mascarada + token + \" \"\n","    else:\n","      # Não existe palavras da classe especificada      \n","      print(\"Não existe palavras da classe especificada.\")\n","      print(\"sentenca:\",sentenca)\n","      print(\"sentenca_pos:\",sentenca_pos)\n","      sentenca_mascarada = sentenca    \n","  else:\n","    # Quantidade trocas igual a 0\n","    print(\"Não foi especificado uma quantidade de trocas.\")\n","    sentenca_mascarada = sentenca\n","\n","  # Retira o espaço em branco do início e fim da sentença\n","  sentenca_mascarada = sentenca_mascarada.strip(\" \")\n","\n","  return sentenca_mascarada, palavra_mascarada"]},{"cell_type":"markdown","source":["#### Gerar perturbação palavra aleatória"],"metadata":{"id":"sVoxgoMaMQU-"}},{"cell_type":"markdown","metadata":{"id":"h6sISvtuV9eQ"},"source":["Gera as palavras da perturbação da máscara da sentença. Considera determinadas classes morfossintática das palavras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbOsuSX7BkhU"},"outputs":[],"source":["# Import das bibliotecas\n","import torch\n","from random import randint # Biblioteca para o sorteio\n","\n","def getPerturbacaoPalavraSentencaAleatoria(sentenca, \n","                                           sentenca_token, \n","                                           sentenca_pos, \n","                                           classe=[\"VERB\",\"NOUN\",\"AUX\"], \n","                                           qtde=1, \n","                                           top_k_predicao = 100):\n","    \"\"\" \n","        Gera as palavras da perturbação da máscara da sentença.\n","        Considera determinadas classes morfossintática das palavras.\n","            \n","        Parâmetros:\n","          `sentenca` - Sentença a ser mascarada.\n","          `sentenca_token` - Lista com os tokens da sentença.\n","          `sentenca_pos` - Lista com as POS dos tokens da sentença.\n","          `classe` - Lista com as classes morfossintática das palavras a serem mascarada com [MASK].\n","          `qtde` - Quantidade de mascarada a serem realizadas nas palavras das sentenças.\n","                  Seleciona aleatoriamente a(s) palavra(s) a ser(em) mascarada(s) se a qtde \n","                  for menor que quantidade de palavras das classes na sentença.          \n","          `top_k_predicao` - Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","\n","        Retorno:    \n","          `sentenca_mascarada` - Sentença mascarada.\n","          `palavra_mascarada` - Palavra substituídas pela máscara.\n","          `token_predito` - Palavra prevista para a máscara.\n","          `token_peso` - Peso da palavra prevista.\n","          `posicao_sorteio` - Posição da palavra prevista na lista de previsões.\n","          `token_predito_marcado` - Token previsto marcado(##) para a máscara.\n","          `lista_previsoes` - Lista dos 'top_k_predicao' tokens preditos para a máscara.\n","    \"\"\"\n","\n","    #print(\"Sentença original:\", sentenca)\n","    sentenca_mascarada, palavra_mascarada = getSentencaMascarada(sentenca, sentenca_token, sentenca_pos, classe=[\"VERB\",\"NOUN\",\"AUX\"], qtde=1)\n","    \n","    # Adiciona os tokens especiais ao sentenca\n","    sentenca_marcado = \"[CLS] \" + sentenca_mascarada + \"[SEP]\"\n","    #print(\"sentenca_marcado:\", sentenca_marcado)\n","\n","    # Divide as palavras em tokens\n","    sentenca_tokenizado = tokenizer.tokenize(sentenca_marcado)    \n","    #print(\"sentenca_tokenizado:\", sentenca_tokenizado)\n","\n","    # Retorna o índice da mascara de atenção\n","    mascara_atencao_indice = sentenca_tokenizado.index(\"[MASK]\")\n","    #print(\"mascara_atencao_indice:\", mascara_atencao_indice)\n","\n","    # Mapeia os tokens em seus índices do vocabulário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(sentenca_tokenizado)\n","    #print(\"tokens_indexados:\", tokens_indexados)\n","    \n","    # Converte as entradas de lista para tensores do torch\n","    tokens_tensores = torch.tensor([tokens_indexados])\n","    \n","    # Realiza a predição dos tokens\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = model(tokens_tensores)\n","\n","    # Recupera a predição com os embeddings da última camada oculta    \n","    predicao = outputs[0]\n","    \n","    # Normaliza os pesos das predições nos embeddings e calcula sua probabilidade\n","    probabilidades = torch.nn.functional.softmax(predicao[0, mascara_atencao_indice], dim=-1)    \n","    # Retorna os k maiores elementos de determinado tensor de entrada ao longo de uma determinada dimensão de forma ordenada descrescentemente.\n","    \n","    # Se existe mais de uma top_k_predição    \n","    if top_k_predicao != 1:\n","\n","      # Recupera as top_k_predicao predições em ordem de orobabilidades\n","      top_k_predicao_pesos, top_k_predicao_indices = torch.topk(probabilidades, top_k_predicao, sorted=True)\n","      #print(\"top_k_predicao_pesos:\",top_k_predicao_pesos)\n","      #print(\"top_k_predicao_indices:\",top_k_predicao_indices)\n","      #print(\"len(top_k_predicao_indices):\",len(top_k_predicao_indices))\n","\n","      # Sorteia uma predição do intervalo\n","      posicao_sorteio = randint(0, top_k_predicao-1)    \n","      #print(\"posicao_sorteio:\",posicao_sorteio)\n","\n","      # Recupera as predições    \n","      # Mapeia os índices do vocabulário para os seus tokens\n","      token_predito = tokenizer.convert_ids_to_tokens([top_k_predicao_indices[posicao_sorteio]])[0]\n","      # Recupera os pesos da predição\n","      token_peso = top_k_predicao_pesos[posicao_sorteio]\n","      #print((posicao_sorteio+1), \"[MASK]: \", token_predito, \" | peso:\", float(token_peso))\n","           \n","      # Se o token predito for igual a palavra que foi substituída pela máscara ou desconhecida ([UNK]) sorteia outra palavra\n","      while (palavra_mascarada.lower() == token_predito.lower()) or (token_predito == \"[UNK]\"):\n","          # Sorteia uma predição do intervalo\n","          posicao_sorteio = randint(0, top_k_predicao-1)    \n","          #print(\"posicao_sorteio:\",posicao_sorteio)\n","\n","          # Recupera as predições    \n","          # Mapeia os índices do vocabulário para os seus tokens\n","          token_predito = tokenizer.convert_ids_to_tokens([top_k_predicao_indices[posicao_sorteio]])[0]\n","          # Recupera os pesos da predição\n","          token_peso = top_k_predicao_pesos[posicao_sorteio]\n","          #print((posicao_sorteio+1), \"[MASK]: \", token_predito, \" | peso:\", float(token_peso))\n","    \n","    else:\n","      # Se existe somente uma predição, esta não pode ser igual a palavra mascarada,\n","      # portanto é necessário aumentar a quantidade de top_k predições para gerar uma predição diferente \n","      # da palavra mascarada.\n","              \n","      # Recupera as top_k_predicao predições em ordem de orobabilidades\n","      top_k_predicao_pesos, top_k_predicao_indices = torch.topk(probabilidades, top_k_predicao, sorted=True)\n","      #print(\"top_k_predicao_pesos:\",top_k_predicao_pesos)\n","      #print(\"top_k_predicao_indices:\",top_k_predicao_indices)\n","      #print(\"len(top_k_predicao_indices):\",len(top_k_predicao_indices))\n","\n","      # Sorteia uma predição do intervalo\n","      posicao_sorteio = randint(0, top_k_predicao-1)    \n","      #print(\"posicao_sorteio:\",posicao_sorteio)\n","\n","      # Recupera as predições    \n","      # Mapeia os índices do vocabulário para os seus tokens\n","      token_predito = tokenizer.convert_ids_to_tokens([top_k_predicao_indices[posicao_sorteio]])[0]\n","      # Recupera os pesos da predição\n","      token_peso = top_k_predicao_pesos[posicao_sorteio]\n","      #print((posicao_sorteio+1), \"[MASK]: \", token_predito, \" | peso:\", float(token_peso))\n","\n","      # Se o token predito for igual a palavra que foi substituída pela máscara ou desconhecida ([UNK]) sorteia outra palavra\n","      while (palavra_mascarada.lower() == token_predito.lower()) or (token_predito == \"[UNK]\"):\n","          \n","          # Incrementa a quantidade de predições para pegar uma palavra diferente\n","          top_k_predicao = top_k_predicao + 1\n","\n","          # Recupera as top_k_predicao + 1 predições em ordem de orobabilidades\n","          top_k_predicao_pesos, top_k_predicao_indices = torch.topk(probabilidades, top_k_predicao, sorted=True)\n","          #print(\"top_k_predicao_pesos:\",top_k_predicao_pesos)\n","          #print(\"top_k_predicao_indices:\",top_k_predicao_indices)\n","          #print(\"len(top_k_predicao_indices):\",len(top_k_predicao_indices))\n","\n","          # Sorteia uma predição do intervalo\n","          posicao_sorteio = randint(0, top_k_predicao-1)    \n","          #print(\"posicao_sorteio:\",posicao_sorteio)\n","\n","          # Recupera as predições    \n","          # Mapeia os índices do vocabulário para os seus tokens\n","          token_predito = tokenizer.convert_ids_to_tokens([top_k_predicao_indices[posicao_sorteio]])[0]\n","          # Recupera os pesos da predição\n","          token_peso = top_k_predicao_pesos[posicao_sorteio]\n","          #print((posicao_sorteio+1), \"[MASK]: \", token_predito, \" | peso:\", float(token_peso))\n","\n","    token_predito_marcado = token_predito\n","    if \"##\" in token_predito:      \n","      # Remove \"##\" do token\n","      token_predito = token_predito[2:]\n","\n","    # Lista das predições\n","    lista_predicoes = []\n","    for i, indice_predicao in enumerate(top_k_predicao_indices):\n","        # Mapeia os índices do vocabulário para os seus tokens\n","        token_predito1 = tokenizer.convert_ids_to_tokens([indice_predicao])[0]\n","        token_peso1 = top_k_predicao_pesos[i]\n","        lista_predicoes.append([(i+1), token_predito1, float(token_peso1)])        \n","      \n","    return sentenca_mascarada, palavra_mascarada, token_predito, token_peso, posicao_sorteio, token_predito_marcado, lista_predicoes"]},{"cell_type":"markdown","source":["#### Gerar perturbação palavra sequencial"],"metadata":{"id":"T4TlEyO8nspd"}},{"cell_type":"code","source":["# Import das bibliotecas\n","import torch\n","from random import randint # Biblioteca para o sorteio\n","\n","def getPerturbacaoPalavraSentencaSequencial(sentenca_mascarada, \n","                                            palavra_mascarada,                                            \n","                                            top_k_predicao = 100):\n","  \n","    \"\"\" \n","        Gera as palavras da perturbação da máscara da sentença.\n","        Considera determinadas classes morfossintática das palavras.\n","            \n","        Parâmetros:\n","          `sentenca_mascarada` - Sentença mascarada.\n","          `palavra_mascarada` - Palavra substituídas pela máscara.\n","          `top_k_predicao` - Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","\n","        Retorno:    \n","          `lista_predicoes` - Lista com as top_k_predições da sentença mascarada. Cada registro possui:\n","              `indice` - ìndice predição.\n","              `sentenca_mascarada` - Sentença mascarada.\n","              `palavra_mascarada` - Palavra substituídas pela máscara.\n","              `token_predito` - Palavra prevista para a máscara.\n","              `token_peso` - Peso da palavra prevista.\n","              `posicao_sorteio` - Posição da palavra prevista na lista de previsões.\n","              `token_predito_marcado` - Token previsto marcado(##) para a máscara.          \n","    \"\"\"\n","\n","    # Adiciona os tokens especiais ao sentenca\n","    sentenca_marcado = \"[CLS] \" + sentenca_mascarada + \"[SEP]\"\n","    #print(\"sentenca_marcado:\", sentenca_marcado)\n","\n","    # Divide as palavras em tokens\n","    sentenca_tokenizado = tokenizer.tokenize(sentenca_marcado)    \n","    #print(\"sentenca_tokenizado:\", sentenca_tokenizado)\n","\n","    # Retorna o índice da mascara de atenção\n","    mascara_atencao_indice = sentenca_tokenizado.index(\"[MASK]\")\n","    #print(\"mascara_atencao_indice:\", mascara_atencao_indice)\n","\n","    # Mapeia os tokens em seus índices do vocabulário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(sentenca_tokenizado)\n","    #print(\"tokens_indexados:\", tokens_indexados)\n","    \n","    # Converte as entradas de lista para tensores do torch\n","    tokens_tensores = torch.tensor([tokens_indexados])\n","    \n","    # Realiza a predição dos tokens\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = model(tokens_tensores)\n","\n","    # Recupera a predição com os embeddings da última camada oculta    \n","    predicao = outputs[0]\n","    \n","    # Normaliza os pesos das predições nos embeddings e calcula sua probabilidade\n","    probabilidades = torch.nn.functional.softmax(predicao[0, mascara_atencao_indice], dim=-1)    \n","    \n","    # Retorna os k maiores elementos de determinado tensor de entrada ao longo de uma determinada \n","    # dimensão de forma ordenada descrescentemente.    \n","    # Adiciona 20 elementos em topkpredicao para pular os tokens desconhecidos([UNK])\n","    MARGEM_UNK = 20\n","    top_k_predicao_pesos, top_k_predicao_indices = torch.topk(probabilidades, top_k_predicao + MARGEM_UNK, sorted=True)\n","    #print(\"top_k_predicao_pesos:\",top_k_predicao_pesos)\n","    #print(\"top_k_predicao_indices:\",top_k_predicao_indices)\n","    #print(\"len(top_k_predicao_indices):\",len(top_k_predicao_indices))\n","\n","    # Lista das predições\n","    lista_predicoes = []\n","    indice_token = 0\n","    for i, indice_predicao in enumerate(top_k_predicao_indices):\n","\n","        # Mapeia os índices do vocabulário para os seus tokens\n","        token_predito = tokenizer.convert_ids_to_tokens([indice_predicao])[0]\n","        token_peso = top_k_predicao_pesos[i]\n","\n","        # Pula o token se for desconhecido e existir tokens disponíveis\n","        if token_predito != \"[UNK]\" and indice_token < (top_k_predicao):\n","          \n","          # Guarda o token original        \n","          token_predito_marcado = token_predito\n","          \n","          # Se o token tiver ##\n","          if \"##\" in token_predito:      \n","              # Remove \"##\" do token     \n","              token_predito = token_predito[2:]\n","\n","          # Guarda o token\n","          lista_predicoes.append([indice_token, sentenca_mascarada, palavra_mascarada, token_predito, float(token_peso), token_predito_marcado])\n","\n","          # Incrementa para o próximo token\n","          indice_token = indice_token + 1\n","      \n","    return lista_predicoes"],"metadata":{"id":"RkTpSL8UnwYb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Gera perturbacao sentença aleatória"],"metadata":{"id":"R6VUrXR4MVyh"}},{"cell_type":"markdown","metadata":{"id":"kGVhA-NSWIYW"},"source":[" Gera a sentença com a perturbação. Considera determinadas classes morfossintática das palavras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgJ2hx5DV99t"},"outputs":[],"source":["def getPerturbacaoSentencaAleatoria(sentenca, \n","                                    sentenca_token, \n","                                    sentenca_pos, \n","                                    classe=[\"VERB\",\"NOUN\",\"AUX\"], \n","                                    qtde=1, \n","                                    top_k_predicao = 100):\n","  \"\"\" \n","      Gera a sentença com a perturbação.\n","      Considera determinadas classes morfossintática das palavras.\n","          \n","      Parâmetros:\n","        `sentenca` - Sentença a ser mascarada.\n","        `sentenca_token` - Lista com os tokens da sentença.\n","        `sentenca_pos` - Lista com as POS dos tokens da sentença.\n","        `classe` - Lista com as classes morfossintática das palavras a serem mascarada com [MASK].\n","        `qtde` - Quantidade de mascarada a serem realizadas nas palavras das sentenças.\n","                Seleciona aleatoriamente a(s) palavra(s) a ser(em) mascarada(s) se a qtde \n","                for menor que quantidade de palavras das classes na sentença.\n","        `top_k_predicao` - Quantidade de palavras a serem recuperadas mais próximas da máscara.                \n","\n","      Retorno:    \n","        `sentenca_perturbada` - Sentença com a perturbação.\n","        `sentenca_mascarada` - Sentença mascarada.\n","        `palavra_mascarada` - Palavra substituídas pela máscara.\n","        `token_predito` - Token previsto para a máscara.\n","        `token_predito_marcado` - Token previsto marcado(##) para a máscara.\n","        `lista_predicoes` - Lista dos tokens preditos para a máscara.\n","        \n","  \"\"\"\n","\n","  # Recupera a sentença mascarada e o token pervisto\n","  sentenca_mascarada, palavra_mascarada, token_predito, token_peso, posicao_sorteio, token_predito_marcado, lista_predicoes = getPerturbacaoPalavraSentencaAleatoria(sentenca, sentenca_token, sentenca_pos, classe, qtde, top_k_predicao)\n","  \n","  # Se existir o token especial [MASK]\n","  if \"[MASK]\" in sentenca_mascarada:\n","    \n","      # Substituir a mascará pelo token predito\n","      sentenca_perturbada = sentenca_mascarada.replace(\"[MASK]\", token_predito)\n","  \n","  return sentenca_perturbada, sentenca_mascarada, palavra_mascarada, token_predito, token_peso, posicao_sorteio, token_predito_marcado, lista_predicoes"]},{"cell_type":"markdown","source":["#### Gera documentos perturbados aleatória"],"metadata":{"id":"IGc-CVt6Mmfe"}},{"cell_type":"markdown","metadata":{"id":"ZBJ5NKjuWMak"},"source":["Gera a perturbação para todas as sentenças dos documentos do conjunto de dados.\n","\n","Todas as perturbações de um documento são diferentes uma das outras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUSm9qvGQ8JB"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook # Biblioteca para barra de progresso\n","\n","def getDocumentosPerturbadosAleatorio(lista_documentos_originais, \n","                                      model_args):\n","\n","  # Lista para armazenar os documentos perturbados\n","  lista_documentos_perturbados = []\n","\n","  # Barra de progresso dos dados\n","  dados_bar = tqdm_notebook(lista_documentos_originais.iterrows(), desc=f\"Dados\", unit=f\"registro\", total=len(lista_documentos_originais))\n","\n","  # Percorre a lista de documentos\n","  for i, linha_documento in dados_bar:\n","    #if i < 2:     \n","      #print(\"linha_documento:\",linha_documento)\n","      # Recupera o id do documento\n","      id_documento = linha_documento[0]     \n","      #print(\"id_documento:\",id_documento)    \n","      # Recupera o documento \n","      documento = linha_documento[2]\n","      #print(\"documento:\",documento) \n","      \n","      # Recupera as sentenças do documento\n","      lista_sentenca_documento = linha_documento[1]\n","      # Localiza a POSTagging do documento\n","      lista_pos_documento = lista_documentos_originais_pos.iloc[i][1]  \n","\n","      #print(\"lista_sentenca_documento:\",lista_sentenca_documento)\n","      #print(\"len(lista_sentenca_documento):\",len(lista_sentenca_documento))\n","      #print(\"lista_pos_documento:\",lista_pos_documento)\n","      #print(\"len(lista_pos_documento):\",len(lista_pos_documento))\n","\n","      # Gera os documentos perturbados a partir do original\n","      for j in range(0, model_args.documentos_perturbados):\n","\n","        # Guarda os dados das sentenças perturbadas\n","        registro_sentencas_perturbadas = []\n","        # Lista com as sentenças perturbadas do documento\n","        lista_sentenca_documento_perturbado = []\n","        # Concatena o texto do documento perturbado\n","        documento_perturbado = \"\"\n","\n","        # Percorre as sentenças do documento\n","        for k, sentenca in enumerate(lista_sentenca_documento):      \n","          #print(\"sentenca Original:\",sentenca)\n","\n","          # Carrega as POSTagging da sentença\n","          sentenca_token = lista_pos_documento[k][0]\n","          sentenca_pos = lista_pos_documento[k][1]\n","          sentenca_verbos = lista_pos_documento[k][2]\n","        \n","          #print(\"sentenca_token:\",sentenca_token)\n","          #print(\"len(sentenca_token):\",len(sentenca_token))\n","          #print(\"sentenca_pos:\",sentenca_pos)\n","          #print(\"len(sentenca_pos):\",len(sentenca_pos))\n","          #print(\"sentenca_verbos:\",sentenca_verbos)\n","          #print(\"len(sentenca_verbos):\",len(sentenca_verbos))\n","\n","          # Gerar sentença com a perturbação\n","          sentenca_perturbada, sentenca_mascarada, palavra_mascarada, token_predito, token_peso, posicao_sorteio, token_predito_marcado, lista_predicoes = getPerturbacaoSentencaAleatoria(sentenca, sentenca_token, sentenca_pos, classe=[\"VERB\",\"NOUN\",\"AUX\"], qtde = 1, top_k_predicao = model_args.top_k_predicao)      \n","\n","          # Se a sentencaPermutada for igual a alguma já perturbada gera outra\n","          conta_repeticao = 0\n","          while sentenca_perturbada.lower() in lista_sentenca_documento_perturbado and conta_repeticao < 3:\n","              \n","              # Gerar sentença com a perturbação pois é igual a uma já existente.\n","              sentenca_perturbada, sentenca_mascarada, palavra_mascarada, token_predito, token_peso, posicao_sorteio, token_predito_marcado, lista_predicoes = getPerturbacaoSentencaAleatoria(sentenca, sentenca_token, sentenca_pos, classe=[\"VERB\",\"NOUN\",\"AUX\"], qtde = 1, top_k_predicao = model_args.top_k_predicao) \n","\n","              # Tenta gerar uma nova 3 vezes\n","              conta_repeticao = conta_repeticao + 1\n","\n","          #print(\"sentenca Original      :\",sentenca)      \n","          #print(\"     sentenca_perturbada:\",sentenca_perturbada) \n","          #print(\"     conta_repeticao    :\",conta_repeticao) \n","          #print(\"                    => palavra_mascarada:\",palavra_mascarada, \" predito:\", token_predito)\n","          # Cria lista das sentenças mascaradas, palavras mascaradas e os tokens preditos\n","          registro_sentencas_perturbadas.append([sentenca_mascarada, palavra_mascarada, token_predito, token_peso, posicao_sorteio, token_predito_marcado])\n","          # Lista da sentenças perturbadas do documento\n","          lista_sentenca_documento_perturbado.append(sentenca_perturbada)\n","          # Concatena em um texto as sentenças do  documento perturbado\n","          documento_perturbado = documento_perturbado + sentenca_perturbada\n","\n","        # Concatena o id do documento e o id da perturbação\n","        novo_id_documento  = str(id_documento) + \"_pert_\" + str(j)\n","\n","        # Guarda o documento perturbado\n","        lista_documentos_perturbados.append([novo_id_documento, \n","                                             lista_sentenca_documento_perturbado, \n","                                             documento_perturbado, \n","                                             registro_sentencas_perturbadas])\n","              \n","  print(\"TERMINEI PERTURBAÇÃO ALEATÓRIA.\")\n","  return lista_documentos_perturbados"]},{"cell_type":"markdown","source":["#### Gera perturbação documentos sequencial"],"metadata":{"id":"ZX6IeZslMwEu"}},{"cell_type":"code","source":["def gerarPerturbacoesSequencial(id_documento, sentenca_mascarada, palavra_mascarada, model_args):\n","    \n","  registro_sentencas_perturbadas = []\n","\n","  # Recupera as predições para a sentença mascarada\n","  lista_predicoes = getPerturbacaoPalavraSentencaSequencial(sentenca_mascarada,\n","                                                            palavra_mascarada,\n","                                                            top_k_predicao = model_args.top_k_predicao)                                                            \n","  #print(\"lista_predicoes:\", len(lista_predicoes))\n","  \n","  # Percorre a lista de predições e faz a substituicão\n","  for predicao in lista_predicoes:\n","    \n","    #print(\"predicao:\",predicao)\n","\n","    # Se existir o token especial [MASK] na sentença marcada\n","    if \"[MASK]\" in predicao[1]: \n","\n","      # Substituir a mascará pelo token predito\n","      sentenca_perturbada = sentenca_mascarada.replace(\"[MASK]\", predicao[3])\n","\n","      # Concatena o id do documento e o id da perturbação\n","      novo_id_documento  = str(id_documento) + \"_pert_\" + str(predicao[0])\n","\n","      # Guarda o registro da sentença perturbada\n","      registro_sentencas_perturbadas.append([novo_id_documento,    #0\n","                                             sentenca_perturbada,  #1\n","                                             predicao[1],          #2 sentenca_mascarada\n","                                             predicao[2],          #3 palavra_mascarada\n","                                             predicao[3],          #4 token_predito\n","                                             predicao[4],          #5 float(token_peso)\n","                                             predicao[5]])         #6 token_predito_marcado      \n","    else:\n","      print(\"Não existe máscara na sentença:\", sentenca_mascarada)\n","  \n","  return registro_sentencas_perturbadas"],"metadata":{"id":"1m4yJ6eVj9eA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook # Biblioteca para barra de progresso\n","\n","def getDocumentosPerturbadosSequencial(lista_documentos_originais, model_args):\n","\n","  # Lista para armazenar os documentos perturbados\n","  lista_documentos_perturbados = []\n","\n","  # Barra de progresso dos dados\n","  dados_bar = tqdm_notebook(lista_documentos_originais.iterrows(), desc=f\"Dados\", unit=f\"registro\", total=len(lista_documentos_originais))\n","\n","  # Percorre a lista de documentos\n","  for i, linha_documento in dados_bar:\n","    #if i < 3:     \n","      #print(\"linha_documento:\",linha_documento)\n","      # Recupera o id do documento\n","      id_documento = linha_documento[0]     \n","      #print(\"id_documento:\",id_documento)    \n","      # Recupera o documento \n","      documento = linha_documento[2]\n","      #print(\"documento:\",documento) \n","      \n","      # Recupera as sentenças do documento\n","      lista_sentenca_documento = linha_documento[1]\n","      # Localiza a POSTagging do documento\n","      lista_pos_documento = lista_documentos_originais_pos.iloc[i][1]  \n","\n","      #print(\"lista_sentenca_documento:\",lista_sentenca_documento)\n","      #print(\"len(lista_sentenca_documento):\",len(lista_sentenca_documento))\n","      #print(\"lista_pos_documento:\",lista_pos_documento)\n","      #print(\"len(lista_pos_documento):\",len(lista_pos_documento))\n","\n","      # Se o documento possui somente 1 sentença, não precisa selecionar uma aleatória para perturbar \n","      if len(lista_sentenca_documento) == 1:\n","\n","        # Percorre as sentenças do documento (neste caso somente 1 sentença)\n","        # Seleciona uma sentença para gerar a perturbação\n","        for k, sentenca in enumerate(lista_sentenca_documento):      \n","          #print(\"sentenca Original:\",sentenca)\n","\n","          # Carrega as POSTagging da sentença\n","          sentenca_token = lista_pos_documento[k][0]\n","          sentenca_pos = lista_pos_documento[k][1]\n","          sentenca_verbos = lista_pos_documento[k][2]\n","            \n","        #print(\"sentenca_token:\",sentenca_token)\n","        #print(\"len(sentenca_token):\",len(sentenca_token))\n","        #print(\"sentenca_pos:\",sentenca_pos)\n","        #print(\"len(sentenca_pos):\",len(sentenca_pos))\n","        #print(\"sentenca_verbos:\",sentenca_verbos)\n","        #print(\"len(sentenca_verbos):\",len(sentenca_verbos))\n","\n","        # Gerar a sentença mascarada para a sentença\n","        sentenca_mascarada, palavra_mascarada = getSentencaMascarada(sentenca, \n","                                                                     sentenca_token, \n","                                                                     sentenca_pos,\n","                                                                     classe=[\"VERB\",\"NOUN\",\"AUX\"], \n","                                                                     qtde=1)\n","        #print(\"sentenca_mascarada:\", sentenca_mascarada)\n","        #print(\"palavra_mascarada:\", palavra_mascarada)\n","          \n","        # Gerar as sentenças com as perturbações para a sentença mascarada\n","        lista_perturbacoes = gerarPerturbacoesSequencial(id_documento, \n","                                                         sentenca_mascarada, \n","                                                         palavra_mascarada, \n","                                                         model_args)\n","        #print(\"lista_perturbacoes:\", len(lista_perturbacoes))\n","\n","        # Percorre as perturbações\n","        for perturbacao in lista_perturbacoes: \n","          \n","          # Lista com as sentenças perturbadas do documento\n","          lista_sentenca_documento_perturbado = []\n","\n","          # Guarda os dados das sentenças perturbadas\n","          registro_sentencas_perturbadas = []\n","          \n","          # Concatena o texto do documento perturbado\n","          documento_perturbado = \"\"\n","\n","          # Guarda o registro da sentença perturbada\n","          registro_sentencas_perturbadas.append([perturbacao[2],  #0 novoId\n","                                                 perturbacao[3],  #1 sentenca_perturbada\n","                                                 perturbacao[4],  #2 sentenca_mascarada\n","                                                 perturbacao[5],  #3 palavra_mascarada\n","                                                 perturbacao[6]]) #4 token_predito_marcado            \n","        \n","          # Lista da sentenças perturbadas do documento\n","          lista_sentenca_documento_perturbado.append(perturbacao[1])\n","          # Concatena em um texto as sentenças do  documento perturbado\n","          documento_perturbado = documento_perturbado + perturbacao[1]\n","            \n","          # Guarda o documento perturbado\n","          lista_documentos_perturbados.append([perturbacao[0], \n","                                               lista_sentenca_documento_perturbado, \n","                                               documento_perturbado, \n","                                               registro_sentencas_perturbadas])\n","\n","      else:\n","        print(\"Documento com mais de uma sentença\")\n","        # Selecionar aleatoriamente uma sentença para perturbar\n","              \n","  print(\"TERMINEI PERTURBAÇÃO SEQUENCIAL.\")\n","  return lista_documentos_perturbados"],"metadata":{"id":"6Rinqy_QL9IF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Gera a perturbação"],"metadata":{"id":"SQXNmcqrljZF"}},{"cell_type":"code","source":["if SELECAO_ALEATORIA_TOP_K == True:  \n","  lista_documentos_perturbados = getDocumentosPerturbadosAleatorio(lista_documentos_originais, model_args)\n","\n","else:\n","  lista_documentos_perturbados = getDocumentosPerturbadosSequencial(lista_documentos_originais, model_args)"],"metadata":{"id":"QVb4YRzsLnGi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2brsc83ReC9W"},"source":["### 5.2.3 Cria o arquivo com as perturbações"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9hX13wgd63W"},"outputs":[],"source":["# Import das bibliotecas.\n","import pandas as pd\n","\n","# Cria o dataframe da lista\n","df_lista_documentos_perturbados = pd.DataFrame(lista_documentos_perturbados, columns = [\"id\", \"perturbado\", \"documento_perturbado\", \"sentencas\"])\n","\n","# Salva o arquivo perturbado\n","df_lista_documentos_perturbados.to_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO,  sep=\";\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJwgPw9ad-mN"},"outputs":[],"source":["print(len(df_lista_documentos_perturbados))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWXOQ2CM6aJV"},"outputs":[],"source":["df_lista_documentos_perturbados.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"WqzXU_Icqiqg"},"source":["### 5.2.4 Compacta e copia o arquivo perturbado para uma pasta do GoogleDrive"]},{"cell_type":"markdown","metadata":{"id":"37e0qS7Dkwou"},"source":["Compacta o arquivo gerado da comparação para facilitar o envio para o GoogleDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f2VhAHXkwow"},"outputs":[],"source":["!zip -o -q -j \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO\"\n","\n","logging.info(\"Terminei compactação.\")"]},{"cell_type":"markdown","source":["Copia o arquivo compactado e os arquivos individuais para o GoogleDrive"],"metadata":{"id":"JJH7kEhiWmi9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"URD2iAO3qiqg"},"outputs":[],"source":["# Import das bibliotecas.\n","import os\n","import datetime\n","\n","# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","    # Recupera a hora do sistema.\n","    data_e_hora_str = datetime.datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n","   \n","    # Copia o arquivo atual para um backup no google drive\n","    !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_COMPACTADO$data_e_hora_str\"\n","    \n","    # Copia o arquivo perturbado\n","    !cp \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" \"$DIRETORIO_DRIVE\"\n","    \n","    logging.info(\"Terminei a cópia.\")"]},{"cell_type":"markdown","metadata":{"id":"TkncKDN1i7kq"},"source":["### 5.2.5 Carrega os dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWf1pJbyBbUz"},"outputs":[],"source":["# Import das bibliotecas.\n","import pandas as pd\n","\n","# Abre o arquivo e retorna o DataFrame\n","df_lista_documentos_perturbados = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO, sep=\";\", encoding=\"UTF-8\")\n","\n","print(len(df_lista_documentos_perturbados))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDO7gHyiBbU5"},"outputs":[],"source":["df_lista_documentos_perturbados.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"Yj0ya60zrm8t"},"source":["# 6 Finalização"]},{"cell_type":"markdown","metadata":{"id":"Bcjt085lZGUr"},"source":["## 6.1 Tempo final de processamento\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H50_GKJwpDha"},"outputs":[],"source":["# Pega o tempo atual menos o tempo do início do processamento.\n","final_processamento = time.time()\n","tempo_total_processamento = formataTempo(final_processamento - inicio_processamento)\n","\n","print(\"\")\n","print(\"  Tempo processamento:  {:} (h:mm:ss)\".format(tempo_total_processamento))"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU","timestamp":1585340447636},{"file_id":"1FsBCkREOaDopLF3PIYUuQxLR8wRfjQY1","timestamp":1559844903389},{"file_id":"1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y","timestamp":1554843110227}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}