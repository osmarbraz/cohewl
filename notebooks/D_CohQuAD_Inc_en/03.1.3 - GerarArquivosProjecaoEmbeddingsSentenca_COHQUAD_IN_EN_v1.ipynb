{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMiVkZQnhMogx/rEcMeJgiN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"78HE8FLsKN9Q"},"source":["# Gera os arquivos para o Embedding Projector das sentenças do conjunto de dados CohQuAD In en  com BERT Transformers by HuggingFace.\n","\n","Gera os arquivos para Embedding Projector(https://projector.tensorflow.org/).\n","\n","Pode ser configurado para utilizar o BERTimbau **Large** e **Base**.\n","\n","Gera arquivos **records_sentenca.tsv** com:\n","- Os embeddings do token [CLS]\n","- Os embeddings da concatenação das 4 últimas camadas do BERT ou da última camada.\n","  - Embeddings dos tokens das sentenças consolidados com as estratégias de pooling das Mean ou Max\n","\n","O arquivo **meta_sentenca.tsv** pode possuir as seguintas colunas:\n","- Sentença\n","- Próxima sentença\n","- Classe\n","\n","\n","Exemplo de projeção dos arquivos gerados: \n","https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/osmarbraz/cohebertv1projecao/main/config_sentence.json\n","\n","Repositório dos arquivos no github.\n","https://github.com/osmarbraz/cohebertv1projecao\n","\n","---------------------------\n","\n","Artigos:\n","\n","- https://arxiv.org/pdf/1611.05469v1.pdf \n","\n","- https://towardsdatascience.com/visualizing-bias-in-data-using-embedding-projector-649bc65e7487\n","\n","- https://towardsdatascience.com/bert-visualization-in-embedding-projector-dfe4c9e18ca9\n","\n","- https://krishansubudhi.github.io/deeplearning/2020/08/27/bert-embeddings-visualization.html\n","\n","- https://amitness.com/interactive-sentence-embeddings/\n","\n","---------------------------\n","\n","**Utiliza o *projeto embeddings* projector para exibir os dados:**\n","https://projector.tensorflow.org/\n","\n","\n","**Link biblioteca Huggingface:**\n","https://github.com/huggingface/transformers\n","\n","\n","**Artigo original BERT Jacob Devlin:**\n","https://arxiv.org/pdf/1506.06724.pdf"]},{"cell_type":"markdown","metadata":{"id":"xyxb5Px3p1-e"},"source":["# 1 Preparação do ambiente\n","Preparação do ambiente para execução do exemplo."]},{"cell_type":"markdown","metadata":{"id":"cW_5CN8En7zl"},"source":["## 1.1 Tempo inicial de processamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcTEKloUn-VK"},"outputs":[],"source":["# Import das bibliotecas\n","import time\n","import datetime\n","\n","#marca o tempo de início do processamento.\n","inicio_processamento = time.time()"]},{"cell_type":"markdown","metadata":{"id":"GOcN8hK-scnt"},"source":["## 1.2 Funções e classes auxiliares"]},{"cell_type":"markdown","metadata":{"id":"OPRnA-mk5-c4"},"source":["Verifica se existe o diretório cohebert no diretório corrente.   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fj5TaAH_5-nB"},"outputs":[],"source":["# Import das bibliotecas.\n","import os # Biblioteca para manipular arquivos\n","\n","# ============================  \n","def verificaDiretorioCoheBERT():\n","    \"\"\"\n","      Verifica se existe o diretório cohebert no diretório corrente.    \n","    \"\"\"\n","    \n","    # Verifica se o diretório existe\n","    if not os.path.exists(DIRETORIO_COHEBERT):  \n","        # Cria o diretório\n","        os.makedirs(DIRETORIO_COHEBERT)\n","        logging.info(\"Diretório Cohebert criado: {}\".format(DIRETORIO_COHEBERT))\n","    \n","    return DIRETORIO_COHEBERT"]},{"cell_type":"markdown","metadata":{"id":"yDCOeh2y5jOH"},"source":["Realiza o download e um arquivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5B1mvfAU5jZf"},"outputs":[],"source":["# Import das bibliotecas.\n","import requests # Biblioteca de download\n","from tqdm.notebook import tqdm as tqdm_notebook # Biblioteca para barra de progresso\n","import os # Biblioteca para manipular arquivos\n","\n","def downloadArquivo(url_arquivo, nome_arquivo_destino):\n","    \"\"\"    \n","      Realiza o download de um arquivo de uma url em salva em nome_arquivo_destino.\n","    \n","      Parâmetros:\n","        `url_arquivo` - URL do arquivo a ser feito download.      \n","        `nome_arquivo_destino` - Nome do arquivo a ser salvo.      \n","    \"\"\"\n","    \n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","    \n","    # Realiza o download de um arquivo em uma url\n","    data = requests.get(url_arquivo, stream=True)\n","    \n","    # Verifica se o arquivo existe\n","    if data.status_code != 200:\n","        logging.info(\"Exceção ao tentar realizar download {}. Response {}.\".format(url_arquivo, data.status_code))\n","        data.raise_for_status()\n","        return\n","\n","    # Recupera o nome do arquivo a ser realizado o download    \n","    nome_arquivo = nome_arquivo_destino.split(\"/\")[-1]  \n","\n","    # Define o nome e caminho do arquivo temporário    \n","    nome_arquivo_temporario = DIRETORIO_COHEBERT + \"/\" + nome_arquivo + \"_part\"\n","    \n","    logging.info(\"Download do arquivo: {}.\".format(nome_arquivo_destino))\n","    \n","    # Baixa o arquivo\n","    with open(nome_arquivo_temporario, \"wb\") as arquivo_binario:        \n","        tamanho_conteudo = data.headers.get(\"Content-Length\")        \n","        total = int(tamanho_conteudo) if tamanho_conteudo is not None else None\n","        # Barra de progresso de download\n","        progresso_bar = tqdm_notebook(unit=\"B\", total=total, unit_scale=True)                \n","        # Atualiza a barra de progresso\n","        for chunk in data.iter_content(chunk_size=1024):        \n","            if chunk:                \n","                progresso_bar.update(len(chunk))\n","                arquivo_binario.write(chunk)\n","    \n","    # Renomeia o arquivo temporário para o arquivo definitivo\n","    os.rename(nome_arquivo_temporario, nome_arquivo_destino)\n","    \n","    # Fecha a barra de progresso.\n","    progresso_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"ksYnRk7zLGp0"},"source":["Remove tags de um documento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qwKjGvyLG4v"},"outputs":[],"source":["def remove_tags(documento):\n","    \"\"\"\n","      Remove tags de um documento\n","    \"\"\"\n","    \n","    import re\n","\n","    documento_limpo = re.compile(\"<.*?>\")\n","    return re.sub(documento_limpo, \"\", documento)"]},{"cell_type":"markdown","metadata":{"id":"4pduTsINLeaz"},"source":["Funções auxiliares de arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jirIzIstLea0"},"outputs":[],"source":["def carregar(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como um único parágrafo(texto).\n","    \n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.  \n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","    \n","    paragrafo = \"\"\n","    for linha in arquivo:\n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        # Remove as tags existentes no final das linhas\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          paragrafo = paragrafo + linha.strip() + \" \"\n","    \n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    # Remove os espaços em branco antes e depois do parágrafo\n","    return paragrafo.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC9Xppq-_R0w"},"outputs":[],"source":["def carregarLista(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como uma lista de sentenças(texto).\n","    \n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.   \n","        `encoding` - Codificação dos caracteres do arquivo.\n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","    \n","    sentencas = []\n","    for linha in arquivo:        \n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          sentencas.append(linha.strip())\n","    \n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    return sentencas "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkVk5LQT_G3f"},"outputs":[],"source":["def salvar(nome_arquivo, texto):                       \n","    \"\"\"\n","      Salva um texto em arquivo.\n","     \n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser salvo.\n","        `texto` - Texto a ser salvo.     \n","    \"\"\"\n","\n","    arquivo = open(nome_arquivo, \"w\")\n","    arquivo.write(str(texto))\n","    arquivo.close()"]},{"cell_type":"markdown","metadata":{"id":"603LYIYKBmq5"},"source":["Função auxiliar para formatar o tempo como `hh: mm: ss`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guy6B4whsZFR"},"outputs":[],"source":["# Import das bibliotecas.\n","import time\n","import datetime\n","\n","def formataTempo(tempo):\n","    \"\"\"\n","      Pega a tempo em segundos e retorna uma string hh:mm:ss\n","    \"\"\"\n","    # Arredonda para o segundo mais próximo.\n","    tempo_arredondado = int(round((tempo)))\n","    \n","    # Formata como hh:mm:ss\n","    return str(datetime.timedelta(seconds=tempo_arredondado))    "]},{"cell_type":"markdown","metadata":{"id":"zVKAapz7RCxk"},"source":["Classe(ModeloArgumentosMedida) de definição dos parâmetros do modelo para medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgmN6RqDRDZS"},"outputs":[],"source":["# Import das bibliotecas.\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","from typing import List\n","\n","@dataclass\n","class ModeloArgumentosMedida:\n","    max_seq_len: Optional[int] = field(\n","        default=None,\n","        metadata={'help': 'max seq len'},\n","    )    \n","    pretrained_model_name_or_path: str = field(\n","        default='neuralmind/bert-base-portuguese-cased',\n","        metadata={'help': 'nome do modelo pré-treinado do BERT.'},\n","    )\n","    modelo_spacy: str = field(\n","        default=\"pt_core_news_lg\",\n","        metadata={\"help\": \"nome do modelo do spaCy.\"},\n","    )\n","    versao_modelo_spacy: str = field(\n","        default=\"-3.2.0\",\n","        metadata={\"help\": \"versão do nome do modelo no spaCy.\"},\n","    )\n","    do_lower_case: bool = field(\n","        default=False,\n","        metadata={'help': 'define se o texto do modelo deve ser todo em minúsculo.'},\n","    )  \n","    output_attentions: bool = field(\n","        default=False,\n","        metadata={'help': 'habilita se o modelo retorna os pesos de atenção.'},\n","    )\n","    output_hidden_states: bool = field(\n","        default=False,\n","        metadata={'help': 'habilita gerar as camadas ocultas do modelo.'},\n","    )\n","    use_wandb : bool = field(\n","        default=True,\n","        metadata={'help': 'habilita o uso do wandb.'},\n","    )\n","    salvar_avaliacao : bool = field(\n","        default=True,\n","        metadata={'help': 'habilita o salvamento do resultado da avaliação.'},\n","    )     \n","    salvar_medicao : bool = field(\n","        default=False,\n","        metadata={'help': 'habilita o salvamento da medicao.'},\n","    )\n","    usar_mcl_ajustado : bool = field(\n","        default=False,\n","        metadata={'help': 'habilita o carragamento de mcl ajustado.'},\n","    )\n","    documentos_perturbados: int = field(\n","        default=\"1\",\n","        metadata={\"help\": \"Quantidade de documentos a serem perturbados a partir do original.\"},\n","    )\n","    top_k_predicao: int = field(\n","        default=\"100\",\n","        metadata={\"help\": \"Quantidade de palavras a serem recuperadas mais próximas da máscara.\"},\n","    )\n","    estrategia_medida: int = field(\n","        default=0, # 0 - MEAN estratégia média / 1 - MAX  estratégia maior\n","        metadata={'help': 'Estratégia de cálculo da médida dos embeddings.'},\n","    )\n","    equacao_medida: int = field(\n","        default=0, # 0 - ADJACENTE / 1 - COMBINAÇÃO TODAS / 2 - CONTEXTO\n","        metadata={'help': 'Equação de cálculo da coerência.'},\n","    )\n","    filtro_palavra: int = field(\n","        default=0, # 0 - Considera todas as palavras das sentenças / 1 - Desconsidera as stopwords / 2 - Considera somente as palavras substantivas\n","        metadata={'help': 'Define o filtro de palavras das sentenças para gerar os embeddings.'},\n","    )"]},{"cell_type":"markdown","metadata":{"id":"HIN413rj50EI"},"source":["Biblioteca de limpeza de tela\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxV4-3Yg50EI"},"outputs":[],"source":["# Import das bibliotecas.\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"iAPVtRXQqDim"},"source":["## 1.3 Tratamento de logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcopxbGZqDip"},"outputs":[],"source":["# Import das bibliotecas.\n","import logging # Biblioteca de logging\n","\n","# Formatando a mensagem de logging\n","logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"_GjYtXcMnSAe"},"source":["## 1.4  Identificando o ambiente Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMiH0E3OnRa1"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory.\n","import sys\n","\n","# Retorna true ou false se estiver no Google Colaboratory.\n","IN_COLAB = 'google.colab' in sys.modules"]},{"cell_type":"markdown","metadata":{"id":"RinFHFesVKis"},"source":["## 1.5 Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"MPngEboiVbfi"},"source":["Usando Colab GPU para Treinamento\n"]},{"cell_type":"markdown","metadata":{"id":"EjWE6WlvVbfj"},"source":["Uma GPU pode ser adicionada acessando o menu e selecionando:\n","\n","`Edit -> Notebook Settings -> Hardware accelerator -> (GPU)`\n","\n","Em seguida, execute a célula a seguir para confirmar que a GPU foi detectada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtaYZmc3Vbfj"},"outputs":[],"source":["# Import das bibliotecas.\n","import tensorflow as tf\n","\n","# Recupera o nome do dispositido da GPU.\n","device_name = tf.test.gpu_device_name()\n","\n","# O nome do dispositivo deve ser parecido com o seguinte:\n","if device_name == \"/device:GPU:0\":\n","    logging.info(\"Encontrei GPU em: {}\".format(device_name))\n","else:\n","    logging.info(\"Dispositivo GPU não encontrado\")\n","    #raise SystemError(\"Dispositivo GPU não encontrado\")"]},{"cell_type":"markdown","metadata":{"id":"iYRrUo2XWa8G"},"source":["Nome da GPU\n","\n","Para que a torch use a GPU, precisamos identificar e especificar a GPU como o dispositivo. Posteriormente, em nosso ciclo de treinamento, carregaremos dados no dispositivo.\n","\n","Vale a pena observar qual GPU você recebeu. A GPU Tesla P100 é muito mais rápido que as outras GPUs, abaixo uma lista ordenada:\n","- 1o Tesla P100\n","- 2o Tesla T4\n","- 3o Tesla P4 (Não tem memória para execução 4 x 8, somente 2 x 4)\n","- 4o Tesla K80 (Não tem memória para execução 4 x 8, somente 2 x 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrjqDO6nWa8J"},"outputs":[],"source":["# Import das bibliotecas.\n","import torch\n","\n","def getDeviceGPU():\n","    \"\"\"\n","      Retorna um dispositivo de GPU se disponível ou CPU.\n","    \n","      Retorno:\n","        `device` - Um device de GPU ou CPU.       \n","    \"\"\"\n","        \n","    # Se existe GPU disponível.\n","    if torch.cuda.is_available():\n","        \n","        # Diz ao PyTorch para usar GPU.    \n","        device = torch.device(\"cuda\")\n","        \n","        logging.info(\"Existem {} GPU(s) disponíveis.\".format(torch.cuda.device_count()))\n","        logging.info(\"Iremos usar a GPU: {}.\".format(torch.cuda.get_device_name(0)))\n","\n","    # Se não.\n","    else:        \n","        logging.info(\"Sem GPU disponível, usando CPU.\")\n","        device = torch.device(\"cpu\")\n","        \n","    return device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChDxmtXsKwjf"},"outputs":[],"source":["# Recupera o device com GPU ou CPU\n","device = getDeviceGPU()"]},{"cell_type":"markdown","metadata":{"id":"fGf59D0yVNx9"},"source":["Memória\n","\n","Memória disponível no ambiente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iC5-pSAVh7_"},"outputs":[],"source":["# Importando as bibliotecas.\n","from psutil import virtual_memory\n","\n","ram_gb = virtual_memory().total / 1e9\n","logging.info(\"Seu ambiente de execução tem {: .1f} gigabytes de RAM disponível\\n\".format(ram_gb))\n","\n","if ram_gb < 20:\n","  logging.info(\"Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \\\"Alterar tipo de tempo de execução\\\"\")\n","  logging.info(\"e selecione High-RAM. Então, execute novamente está célula\")\n","else:\n","  logging.info(\"Você está usando um ambiente de execução de memória RAM alta!\")"]},{"cell_type":"markdown","metadata":{"id":"wijMXooQQLcQ"},"source":["## 1.6 Monta uma pasta no google drive para carregar os arquivos de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysnDDapMQK8K"},"outputs":[],"source":["# import necessário\n","from google.colab import drive\n","\n","# Monta o drive na pasta especificada\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"u66iRrtwMrqy"},"source":["## 1.7 Instalação do wandb"]},{"cell_type":"markdown","metadata":{"id":"dQd3BrhvMzZs"},"source":["Instalação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejzpgGrFM0-j"},"outputs":[],"source":["!pip install --upgrade wandb"]},{"cell_type":"markdown","metadata":{"id":"oOd2MbBiDq93"},"source":["## 1.8 Instalação do spaCy\n","\n","https://spacy.io/\n","\n","Modelos do spaCy para português:\n","https://spacy.io/models/pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaMM4WdxgvQ7"},"outputs":[],"source":["# Instala o spacy\n","!pip install -U pip setuptools wheel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4p3Rz2qDq94"},"outputs":[],"source":["# Instala uma versão específica\n","!pip install -U spacy==3.2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RfUN_KolV-f"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["## 1.9 Instalação do BERT"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["Instala a interface pytorch para o BERT by Hugging Face. \n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCVzCmy7pIOz"},"outputs":[],"source":["!pip install -U transformers==4.5.1"]},{"cell_type":"markdown","metadata":{"id":"giOsAS5v61go"},"source":["# 2 Parametrização"]},{"cell_type":"markdown","metadata":{"id":"ifrYNTwGwKal"},"source":["## Gerais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uiH9pNpwI6g"},"outputs":[],"source":["# Definição dos parâmetros a serem avaliados\n","#Quantidade de documentos a serem perturbados a partir do original.\n","DOCUMENTOS_PERTURBADOS = 20\n","\n","#Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","TOP_K_PREDICAO = 20\n","\n","#Utiliza somente documentos originais, perturbados ou ambos.\n","CLASSE_DOCUMENTO = 2 # 0 - Somente com a classe 0 (Perturbado) / 1 - Somente com classe 1 (Original), 2 - As duas classes 0 e 1\n","\n","#Estratégia de recuperação dos embeddings: (0 - Embedding do token [CLS] da última, \n","#                                           1 - Embeddings da última camada, \n","#                                           2 - Embeddings da concatenação das 4 últimas camadas)\n","ESTRATEGIA_EMBEDDING = 2\n","\n","# Estratégias a serem avaliadas (0 - Mean / 1 - Max) para as palavras formadas por mais de um token do BERT\n","ESTRATEGIA_MEDIDA_STR = [\"MEAN\", \"MAX\"]\n","ESTRATEGIA_MEDIDA = 0\n","\n","# Habilita a criação do rótulo \"_next__\" no projetor para gerar linhas entre os pontos de documento de uma mesma origem em sequência.\n","LIGACAO_PROXIMO_DOCUMENTO = True"]},{"cell_type":"markdown","metadata":{"id":"mhByVujAwNAU"},"source":["## Específicos"]},{"cell_type":"markdown","metadata":{"id":"3V_ORR8Qyu1p"},"source":["Parâmetros do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ15-ylRRRdD"},"outputs":[],"source":["# Definição dos parâmetros do Modelo\n","model_args = ModeloArgumentosMedida(     \n","    max_seq_len = 512,    \n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\",\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\",\n","    \n","    pretrained_model_name_or_path = \"bert-large-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-cased\"\n","    #pretrained_model_name_or_path = \"neuralmind/bert-large-portuguese-cased\",\n","    #pretrained_model_name_or_path = \"neuralmind/bert-base-portuguese-cased\",    \n","    #pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-multilingual-uncased\",\n","\n","    modelo_spacy = \"en_core_web_lg\",\n","    #modelo_spacy = \"en_core_web_md\",\n","    #modelo_spacy = \"en_core_web_sm\",\n","    #modelo_spacy = \"pt_core_news_lg\",\n","    #modelo_spacy = \"pt_core_news_md\",\n","    #modelo_spacy = \"pt_core_news_sm\",\n","\n","    versao_modelo_spacy = \"3.2.0\",\n","    do_lower_case = False,  # default True\n","    output_attentions = False,  # default False\n","    output_hidden_states = True, # default False\n","    use_wandb = True,    \n","    salvar_medicao = True, #Salva o resultado da medição\n","    salvar_avaliacao = True, # Salva o resultado da avaliação das medições\n","    documentos_perturbados = DOCUMENTOS_PERTURBADOS, # Quantidade de documentos a serem perturbados a partir do original.    \n","    top_k_predicao = TOP_K_PREDICAO, # Conjunto de valores: 1, 10, 100, 500 e 1000. Quantidade de palavras a serem recuperadas mais próximas da máscara. \n","    usar_mcl_ajustado = False, # Especifica se deve ser carregado um MCL ajustado ou pré-treinado. Necessário especificar o tipo do modelo em pretrained_model_name_or_path. \n","    estrategia_medida = 0, # Atributo usado para os logs do wandb. 0 - MEAN estratégia média / 1 - MAX  estratégia maior\n","    equacao_medida = 0, # Atributo usado para os logs do wandb. 0 - Palavras adjacentes / 1 - Todas as palavras / 2 - Palavra e contexto\n","    filtro_palavra = 0 # # Atributo usado para os logs do wandb. 0 - Considera todas as palavras das sentenças / 1 - Desconsidera as stopwords / 2 - Considera somente as palavras substantivas\n",")"]},{"cell_type":"markdown","metadata":{"id":"WlF4PKP6Iopi"},"source":["## Nome do diretório dos arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55PNP2s6Iopi"},"outputs":[],"source":["# Diretório do cohebert\n","DIRETORIO_COHEBERT = \"COHQUAD_IN_EN\""]},{"cell_type":"markdown","metadata":{"id":"SUxlx7Sx4yxj"},"source":["## Define o caminho para os arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gQpxAO74yxj"},"outputs":[],"source":["# Diretório local para os arquivos pré-processados\n","DIRETORIO_LOCAL = \"/content/\" + DIRETORIO_COHEBERT + \"/\"\n","\n","# Diretório no google drive com os arquivos pré-processados\n","DIRETORIO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/\""]},{"cell_type":"markdown","metadata":{"id":"tDgJTbPOZ8SW"},"source":["## Inicialização diretórios"]},{"cell_type":"markdown","metadata":{"id":"qpSERA9TC4WU"},"source":["Diretório base local"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edg7eW2cDflg"},"outputs":[],"source":["# Importando as bibliotecas.\n","import os\n","\n","def criaDiretorioLocal():\n","\n","  # Cria o diretório para receber os arquivos Originais e Permutados\n","  # Diretório a ser criado\n","  dirbase = DIRETORIO_LOCAL[:-1]\n","\n","  if not os.path.exists(dirbase):  \n","      # Cria o diretório\n","      os.makedirs(dirbase)    \n","      logging.info(\"Diretório criado: {}.\".format(dirbase))\n","  else:    \n","      logging.info(\"Diretório já existe: {}.\".format(dirbase))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xge0ar9MJoKy"},"outputs":[],"source":["criaDiretorioLocal()"]},{"cell_type":"markdown","metadata":{"id":"4FmT9nhbaE3D"},"source":["Diretório para conter as os resultados das medidas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO76uzj_C3zQ"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioMedidacao():\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):  \n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1Ot2h_bJuxy"},"outputs":[],"source":["criaDiretorioMedidacao()"]},{"cell_type":"markdown","metadata":{"id":"vIkT6ksqaQs3"},"source":["Diretório para conter os arquivos da avaliação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIV4xj6zDnb8"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioAvaliacao():\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra/Avaliacao\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):  \n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiOVjJ5BJzE1"},"outputs":[],"source":["criaDiretorioAvaliacao()"]},{"cell_type":"markdown","metadata":{"id":"cjP6v878aWR7"},"source":["Diretório para conter os arquivos das medidas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf6UWAZYDsgm"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioMedicao():\n","\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra/Medicao\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):  \n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBBfHFuPJ3NM"},"outputs":[],"source":["criaDiretorioMedicao()"]},{"cell_type":"markdown","metadata":{"id":"L7G3-MOsQ1N_"},"source":["# 3 spaCy"]},{"cell_type":"markdown","metadata":{"id":"35GwcgkOlWi3"},"source":["## 3.1 Download arquivo modelo\n","\n","https://spacy.io/models/pt"]},{"cell_type":"markdown","metadata":{"id":"PWd_9X0nOYnF"},"source":["### Função download modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjWGu-9D5URZ"},"outputs":[],"source":["def downloadSpacy(model_args):\n","    \"\"\"\n","      Realiza o download do arquivo do modelo para o diretório corrente.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.       \n","    \"\"\"\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","        \n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Nome arquivo compactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","    \n","    # Url do arquivo\n","    URL_ARQUIVO_MODELO_COMPACTADO = \"https://github.com/explosion/spacy-models/releases/download/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO\n","\n","    # Realiza o download do arquivo do modelo\n","    logging.info(\"Download do arquivo do modelo do spaCy.\")\n","    downloadArquivo(URL_ARQUIVO_MODELO_COMPACTADO, DIRETORIO_COHEBERT + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"Uu_LkF7Nfm8_"},"source":["## 3.2 Descompacta o arquivo do modelo"]},{"cell_type":"markdown","metadata":{"id":"XAc1tSwvOc4d"},"source":["### Função descompacta modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq9PnXO77bPQ"},"outputs":[],"source":["# Import das bibliotecas.\n","import tarfile # Biblioteca de descompactação\n","\n","def descompactaSpacy(model_args):\n","    \"\"\"\n","      Descompacta o arquivo do modelo.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.       \n","    \"\"\"\n","    \n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","    \n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    \n","    # Nome do arquivo a ser descompactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","    \n","    logging.info(\"Descompactando o arquivo do modelo do spaCy.\")\n","    arquivo_tar = tarfile.open(NOME_ARQUIVO_MODELO_COMPACTADO, \"r:gz\")    \n","    arquivo_tar.extractall(DIRETORIO_COHEBERT)    \n","    arquivo_tar.close()\n","    \n","    # Apaga o arquivo compactado\n","    if os.path.isfile(NOME_ARQUIVO_MODELO_COMPACTADO):        \n","        os.remove(NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"STHT2c89qvwK"},"source":["## 3.3 Carrega o modelo"]},{"cell_type":"markdown","metadata":{"id":"3iFBoyWMOgKz"},"source":["### Função carrega modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePOccj0s8WMg"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy # Biblioteca do spaCy\n","\n","def carregaSpacy(model_args):\n","    \"\"\"\n","    Realiza o carregamento do Spacy.\n","    \n","    Parâmetros:\n","      `model_args` - Objeto com os argumentos do modelo.           \n","    \"\"\"\n","    \n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","                  \n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Caminho raoz do modelo do spaCy\n","    DIRETORIO_MODELO_SPACY =  DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY\n","\n","    # Verifica se o diretório existe\n","    if os.path.exists(DIRETORIO_MODELO_SPACY) == False:\n","        # Realiza o download do arquivo modelo do spaCy\n","        downloadSpacy(model_args)\n","        # Descompacta o spaCy\n","        descompactaSpacy(model_args)\n","\n","    # Diretório completo do spaCy\n","    DIRETORIO_MODELO_SPACY = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\"\n","\n","    # Carrega o spaCy. Necessário somente \"tagger\" para encontrar os substantivos\n","    nlp = spacy.load(DIRETORIO_MODELO_SPACY)\n","    logging.info(\"spaCy carregado.\")\n","\n","    # Retorna o spacy carregado\n","    return nlp "]},{"cell_type":"markdown","metadata":{"id":"cAk5hHx7OnHn"},"source":["### Carrega o modelo spaCy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbELnrpgA4T1"},"outputs":[],"source":["# Carrega o modelo spaCy\n","nlp = carregaSpacy(model_args)"]},{"cell_type":"markdown","metadata":{"id":"fzk8VOp7oy8n"},"source":["## 3.4 Funções auxiliares spaCy"]},{"cell_type":"markdown","metadata":{"id":"AEzytjZi5Iw2"},"source":["### getStopwords\n","\n","Recupera as stopwords do spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKg-_XyWoy8o"},"outputs":[],"source":["def getStopwords(nlp):\n","    \"\"\"\n","      Recupera as stop words do nlp(Spacy).\n","    \n","      Parâmetros:\n","        `nlp` - Um modelo spaCy carregado.           \n","    \"\"\"\n","    \n","    spacy_stopwords = nlp.Defaults.stop_words\n","\n","    return spacy_stopwords "]},{"cell_type":"markdown","metadata":{"id":"qZdNFrC3oy8p"},"source":["Lista dos stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1o8jevtoy8p"},"outputs":[],"source":["logging.info(\"Quantidade de stopwords: {}.\".format(len(getStopwords(nlp))))\n","\n","print(getStopwords(nlp))"]},{"cell_type":"markdown","metadata":{"id":"onM1ZApom-_W"},"source":["### getVerbos\n","Localiza os verbos da sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hdqVdfxm-_W"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy   \n","from spacy.util import filter_spans\n","from spacy.matcher import Matcher\n","\n","# (verbo normal como auxilar ou auxilar) + vários verbos auxiliares +verbo principal ou verbo auxiliar\n","gramaticav1 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"aux\",\"aux:pass\"]}},  #verbo auxiliar                                  \n","                {\"POS\": \"VERB\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"ROOT\",\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo normal como auxiliar\n","                {\"POS\": \"AUX\", \"OP\": \"*\", \"DEP\": {\"IN\": [\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo auxiliar   \n","                {\"POS\": \"VERB\", \"OP\": \"+\"}, #verbo principal\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"cop\",\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo auxiliar\n","               ] \n","\n","# verbo auxiliar + verbo normal como auxiliar + conjunção com preposição + verbo\n","gramaticav2 =  [               \n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"aux\",\"aux:pass\"]}},  #verbo auxiliar                   \n","                {\"POS\": \"VERB\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"ROOT\"]}},  #verbo principal       \n","                {\"POS\": \"SCONJ\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"mark\"]}}, #conjunção com preposição\n","                {\"POS\": \"VERB\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"xcomp\"]}}, #verbo normal como complementar\n","               ] \n","\n","#Somente verbos auxiliares\n","gramaticav3 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\"},  #Verbos auxiliar \n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"cop\"]}},  #Verbos auxiliar de ligação (AUX+(cop))\n","                {\"POS\": \"ADJ\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"ROOT\"]}}, \n","                {\"POS\": \"AUX\", \"OP\": \"?\"}  #Verbos auxiliar \n","               ] \n","\n","matcherv = Matcher(nlp.vocab)\n","         \n","matcherv.add(\"frase verbal\", [gramaticav1])\n","matcherv.add(\"frase verbal\", [gramaticav2])\n","matcherv.add(\"frase verbal\", [gramaticav3])\n","\n","#Retorna a Frase Verbal\n","def getVerbos(periodo):    \n","  #Processa o período\n","  doc1 = nlp(periodo.text)\n","  \n","  # Chama o mather para encontrar o padrão\n","  matches = matcherv(doc1)\n","\n","  padrao = [doc1[start:end] for _, start, end in matches]\n","\n","  #elimina as repetições e sobreposições\n","  #return filter_spans(padrao)\n","  lista1 = filter_spans(padrao)\n","\n","  # Converte os itens em string\n","  lista2 = []\n","  for x in lista1:\n","      lista2.append(str(x))\n","  \n","  return lista2"]},{"cell_type":"markdown","metadata":{"id":"6ZVwbmn3Nx2t"},"source":["### getDicPOSQtde\n","\n","Conta as POS Tagging de uma sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3j3VF4NOSPbq"},"outputs":[],"source":["def getDicPOSQtde(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy  \n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Retorna inteiros que mapeiam para classes gramaticais\n","  conta_dicionarios = doc.count_by(spacy.attrs.IDS[\"POS\"])\n","\n","  # Dicionário com as tags e quantidades\n","  novodic = dict()\n","  \n","  for pos, qtde in conta_dicionarios.items():\n","    classe_gramatical = doc.vocab[pos].text\n","    novodic[classe_gramatical] = qtde\n","\n","  return novodic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uPDYU4KBC5q"},"outputs":[],"source":["def getDicTodasPOSQtde(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy  \n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Retorna inteiros que mapeiam para classes gramaticais\n","  conta_dicionarios = doc.count_by(spacy.attrs.IDS[\"POS\"])\n","\n","  # Dicionário com as tags e quantidades    \n","  novodic = {\"PRON\":0, \"VERB\":0, \"PUNCT\":0, \"DET\":0, \"NOUN\":0, \"AUX\":0, \"CCONJ\":0, \"ADP\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"NUM\":0, \"SCONJ\":0, \"SYM\":0, \"SPACE\":0, \"INTJ\":0, \"X\": 0}\n","    \n","  for pos, qtde in conta_dicionarios.items():\n","    classe_gramatical = doc.vocab[pos].text\n","    novodic[classe_gramatical] = qtde\n","\n","  return novodic"]},{"cell_type":"markdown","metadata":{"id":"Jxe-mh-l6sJY"},"source":["### getDicTodasPOSQtde\n","\n","Conta as POS Tagging de uma sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9SA61kD6sJY"},"outputs":[],"source":["def getDicTodasPOSQtde(lista):\n","\n","  # Dicionário com as tags e quantidades\n","  conjunto = {\"PRON\":0, \"VERB\":0, \"PUNCT\":0, \"DET\":0, \"NOUN\":0, \"AUX\":0, \"CCONJ\":0, \"ADP\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"NUM\":0, \"SCONJ\":0, \"SYM\":0, \"SPACE\":0, \"INTJ\": 0}\n","\n","  for x in lista:\n","    valor = conjunto.get(x)\n","    if valor != None:\n","      conjunto[x] = valor + 1\n","    else:\n","      conjunto[x] = 1\n","\n","  return conjunto"]},{"cell_type":"markdown","metadata":{"id":"m4KV_jI-Nx2w"},"source":["### getSomaDic\n","\n","Soma os valores de dicionários com as mesmas chaves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGduPM6HNx2w"},"outputs":[],"source":["from collections import Counter\n","from functools import reduce\n","\n","def atualizaValor(a,b):\n","    a.update(b)\n","    return a\n","\n","def getSomaDic(lista):\n","    \n","  # Soma os dicionários da lista\n","  novodic = reduce(atualizaValor, (Counter(dict(x)) for x in lista))\n"," \n","  return novodic"]},{"cell_type":"markdown","metadata":{"id":"bGaf7bkpAEiX"},"source":["### getTokensSentenca\n","\n","Retorna a lista de tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWxyAo54AOHU"},"outputs":[],"source":["def getTokensSentenca(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy  \n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença adicionando os tokens\n","  for token in doc:    \n","    lista.append(token.text)\n","\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"ZB6bR42PA28c"},"source":["### getPOSTokensSentenca\n","\n","Retorna a lista das POS-Tagging dos tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awaqjNIZA3Fk"},"outputs":[],"source":["def getPOSTokensSentenca(sentenca):\n","\n","  # Verifica se o sentenca não foi processado pelo spaCy  \n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença adicionando os tokens\n","  for token in doc:    \n","    lista.append(token.pos_)\n","\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"B4Soqt3fp3Lu"},"source":["### getListaTokensPOSSentenca\n","\n","Retorna duas listas uma com os tokens e a outra com a POS-Tagging dos tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvd99wd_pwmt"},"outputs":[],"source":["def getListaTokensPOSSentenca(sentenca):\n","  # Verifica se o sentenca não foi processado pelo spaCy  \n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista_tokens = []\n","  lista_pos = []\n","\n","  # Percorre a sentença adicionando os tokens e as POS\n","  for token in doc:    \n","    lista_tokens.append(token.text)\n","    lista_pos.append(token.pos_)\n","    \n","  return lista_tokens, lista_pos"]},{"cell_type":"markdown","metadata":{"id":"ENvsIER06sJX"},"source":["### Tadução das tags"]},{"cell_type":"markdown","metadata":{"id":"kwSb3ECU6sJY"},"source":["Tags de palavras universal\n","\n","https://universaldependencies.org/u/pos/\n","\n","Detalhes das tags em português:\n","http://www.dbd.puc-rio.br/pergamum/tesesabertas/1412298_2016_completo.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpCUpOs06sJY"},"outputs":[],"source":["#dicionário que contêm pos tag universal e suas explicações\n","palavra_universal_dict = {\n","  \"X\"    : \"Outro\",\n","  \"VERB\" : \"Verbo \",\n","  \"SYM\"  : \"Símbolo\",\n","  \"CONJ\" : \"Conjunção\",\n","  \"SCONJ\": \"Conjunção subordinativa\",\n","  \"PUNCT\": \"Pontuação\",\n","  \"PROPN\": \"Nome próprio\",\n","  \"PRON\" : \"Pronome substativo\",\n","  \"PART\" : \"Partícula, morfemas livres\",\n","  \"NUM\"  : \"Numeral\",\n","  \"NOUN\" : \"Substantivo\",\n","  \"INTJ\" : \"Interjeição\",\n","  \"DET\"  : \"Determinante, Artigo e pronomes adjetivos\",\n","  \"CCONJ\": \"Conjunção coordenativa\",\n","  \"AUX\"  : \"Verbo auxiliar\",\n","  \"ADV\"  : \"Advérbio\",\n","  \"ADP\"  : \"Preposição\",\n","  \"ADJ\"  : \"Adjetivo\"\n","}\n","  \n","#Explica a POS\n","def getPOSPalavraUniversalTraduzido(palavra):\n","  if palavra in palavra_universal_dict.keys():\n","      traduzido = palavra_universal_dict[palavra]\n","  else:\n","      traduzido = \"NA\" \n","  return traduzido"]},{"cell_type":"markdown","metadata":{"id":"b01WgMSSKY_u"},"source":["### getSentencaSemStopWord\n","\n","Retorna uma lista dos tokens sem as stopwords."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMb0uDWzKZXP"},"outputs":[],"source":["def getSentencaSemStopWord(sentenca, stopwords):\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre os tokens da sentença\n","  for i, token in enumerate(sentenca):\n","\n","    # Verifica se o token é uma stopword\n","    if token.lower() not in stopwords:\n","      lista.append(token)\n","\n","  # Retorna o documento\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"TouR4GjNJZD6"},"source":["### getSentencaSalientePOS\n","\n","Retorna uma lista das palavras do tipo especificado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxTCYFzcJZD6"},"outputs":[],"source":["def getSentencaSalientePOS(sentenca, pos, tipo_saliente=\"NOUN\"):\n","  \n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença\n","  for i, token in enumerate(sentenca):\n","\n","    # Verifica se o token é do tipo especeficado\n","    if pos[i] == tipo_saliente:\n","      lista.append(token)\n","\n","  # Retorna o documento\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"_xaeX0oTVQ5t"},"source":["###removeStopWords\n","\n","Remove as stopwords de um documento ou senteça."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIaQ9bzBVQ5t"},"outputs":[],"source":["def removeStopWord(documento, stopwords):\n","  \n","  # Remoção das stopwords do documento\n","  documento_sem_stopwords = [palavra for palavra in documento.split() if palavra.lower() not in stopwords]\n","\n","  # Concatena o documento sem os stopwords\n","  documento_limpo = \" \".join(documento_sem_stopwords)\n","\n","  # Retorna o documento\n","  return documento_limpo"]},{"cell_type":"markdown","metadata":{"id":"A7NAe8ogCf1y"},"source":["### retornaRelevante\n","\n","Retorna somente os palavras do documento ou sentença do tipo especificado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNNfykypChn-"},"outputs":[],"source":["def retornaRelevante(documento, classe_relevante=\"NOUN\"):\n","\n","  # Corrigir!\n","  # Utilizar o documento já tokenizado pelo spacy!!!!\n","  # Existe uma lista com o documento e a sentença tokenizada pelo spacy\n","  \n","  # Realiza o parsing no spacy\n","  doc = nlp(documento)\n","\n","  # Retorna a lista das palavras relevantes\n","  documento_com_substantivos = []\n","  for token in doc:\n","    #print(\"token:\", token.pos_)\n","    if token.pos_ == classe_relevante:\n","      documento_com_substantivos.append(token.text)\n","\n","  # Concatena o documento com os substantivos\n","  documento_concatenado = \" \".join(documento_com_substantivos)\n","\n","  # Retorna o documento\n","  return documento_concatenado"]},{"cell_type":"markdown","metadata":{"id":"IBY7q_uH8JSE"},"source":["# 4 BERT"]},{"cell_type":"markdown","metadata":{"id":"MBGTMy8Ic7GK"},"source":["## 4.1 Modelo Pré-treinado BERT"]},{"cell_type":"markdown","metadata":{"id":"uiuxdXe9t1BX"},"source":["### Funções Auxiliares"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Huw0x5kt1Le"},"outputs":[],"source":["def getNomeModeloBERT(model_args):\n","    '''    \n","    Recupera uma string com uma descrição do modelo BERT para nomes de arquivos e diretórios.\n","    \n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.       \n","    \n","    Retorno:\n","    `MODELO_BERT` - Nome do modelo BERT.\n","    '''\n","\n","    # Verifica o nome do modelo(default SEM_MODELO_BERT)\n","    MODELO_BERT = \"SEM_MODELO_BERT\"\n","    \n","    if 'neuralmind' in model_args.pretrained_model_name_or_path:\n","        MODELO_BERT = \"_BERTimbau\"\n","        \n","    else:\n","        if 'multilingual' in model_args.pretrained_model_name_or_path:\n","            MODELO_BERT = \"_BERTmultilingual\"\n","            \n","    return MODELO_BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYJB4ik7t5xe"},"outputs":[],"source":["def getTamanhoBERT(model_args):\n","    '''    \n","    Recupera uma string com o tamanho(dimensão) do modelo BERT para nomes de arquivos e diretórios.\n","    \n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.       \n","    \n","    Retorno:\n","    `TAMANHO_BERT` - Nome do tamanho do modelo BERT.\n","    '''\n","    \n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = \"_large\"\n","    \n","    if 'base' in model_args.pretrained_model_name_or_path:\n","        TAMANHO_BERT = \"_base\"\n","        \n","    return TAMANHO_BERT  "]},{"cell_type":"markdown","metadata":{"id":"rHt4e5pAcEMd"},"source":["### Função download Modelo Pre-treinado BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peDUrV2ccEXA"},"outputs":[],"source":["# Import das bibliotecas.\n","import zipfile # Biblioteca para descompactar\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def downloadModeloPretreinado(model_args):\n","    \"\"\"\n","      Realiza o download do modelo BERT(MODELO) e retorna o diretório onde o modelo BERT(MODELO) foi descompactado.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \n","      Retorno:\n","        `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\" \n","    \n","    # Nome diretório base modelo BERT\n","    NOME_DIRETORIO_BASE_MODELO = \"modeloBERT\"\n","    \n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","    \n","    # Recupera o nome ou caminho do modelo\n","    MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in MODELO:\n","        URL_MODELO = MODELO\n","\n","    # Se a variável foi setada.\n","    if URL_MODELO:\n","\n","        # Diretório do modelo.\n","        DIRETORIO_MODELO = DIRETORIO_COHEBERT + \"/\" + NOME_DIRETORIO_BASE_MODELO\n","        \n","        # Recupera o nome do arquivo do modelo da url.\n","        NOME_ARQUIVO = URL_MODELO.split(\"/\")[-1]\n","\n","        # Nome do arquivo do vocabulário.\n","        ARQUIVO_VOCAB = \"vocab.txt\"\n","        \n","        # Caminho do arquivo na url.\n","        CAMINHO_ARQUIVO = URL_MODELO[0:len(URL_MODELO)-len(NOME_ARQUIVO)]\n","\n","        # Verifica se o diretório de descompactação existe no diretório corrente\n","        if os.path.exists(DIRETORIO_MODELO):\n","            logging.info(\"Apagando diretório existente do modelo!\")\n","            # Apaga o diretório e os arquivos existentes                     \n","            shutil.rmtree(DIRETORIO_MODELO)\n","        \n","        # Realiza o download do arquivo do modelo        \n","        downloadArquivo(URL_MODELO, NOME_ARQUIVO)\n","\n","        # Descompacta o arquivo no diretório de descompactação.                \n","        arquivo_zip = zipfile.ZipFile(NOME_ARQUIVO, \"r\")\n","        arquivo_zip.extractall(DIRETORIO_MODELO)\n","\n","        # Baixa o arquivo do vocabulário.\n","        # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente.\n","        URL_MODELO_VOCAB = CAMINHO_ARQUIVO + ARQUIVO_VOCAB\n","        # Coloca o arquivo do vocabulário no diretório do modelo.        \n","        downloadArquivo(URL_MODELO_VOCAB, DIRETORIO_MODELO + \"/\" + ARQUIVO_VOCAB)\n","        \n","        # Apaga o arquivo compactado\n","        os.remove(NOME_ARQUIVO)\n","\n","        logging.info(\"Diretório {} do modelo BERT pronta!\".format(DIRETORIO_MODELO))\n","\n","    else:\n","        DIRETORIO_MODELO = MODELO\n","        logging.info(\"Variável URL_MODELO não setada!\")\n","\n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"V74WUpHqcfoI"},"source":["### Copia o modelo do BERT ajustado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQMpf9yycf8f"},"outputs":[],"source":["# Import das bibliotecas.\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def copiaModeloAjustado(model_args):\n","    \"\"\" \n","      Copia o modelo ajustado BERT do GoogleDrive para o projeto.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \n","      Retorno:\n","        `DIRETORIO_LOCAL_MODELO_AJUSTADO` - Diretório de download ajustado do modelo.\n","    \"\"\"\n","\n","    # Verifica o nome do modelo BERT a ser utilizado\n","    MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = getTamanhoBERT(model_args)\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Diretório local de salvamento do modelo.\n","    DIRETORIO_LOCAL_MODELO_AJUSTADO = DIRETORIO_COHEBERT + \"/modelo_ajustado/\"\n","\n","    # Diretório remoto de salvamento do modelo no google drive.\n","    DIRETORIO_REMOTO_MODELO_AJUSTADO = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/validacao_classificacao_palavra/holdout/modelo/\" + MODELO_BERT + TAMANHO_BERT\n","\n","    # Copia o arquivo do modelo para o diretório no Google Drive.\n","    shutil.copytree(DIRETORIO_REMOTO_MODELO_AJUSTADO, DIRETORIO_LOCAL_MODELO_AJUSTADO) \n","   \n","    logging.info(\"Modelo BERT ajustado copiado!\")\n","\n","    return DIRETORIO_LOCAL_MODELO_AJUSTADO"]},{"cell_type":"markdown","metadata":{"id":"eaneOhAKcO-3"},"source":["### Verifica de onde utilizar o modelo do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTy1TXz3cPKS"},"outputs":[],"source":["def verificaModelo(model_args):\n","    \"\"\" \n","    Verifica de onde utilizar o modelo.\n","    \n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","    \n","    Retorno:\n","    `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\" \n","\n","    DIRETORIO_MODELO = None\n","    \n","    if model_args.usar_mcl_ajustado == True:        \n","        # Diretório do modelo\n","        DIRETORIO_MODELO = copiaModeloAjustado()\n","        \n","        logging.info(\"Usando modelo BERT ajustado.\")\n","        \n","    else:\n","        DIRETORIO_MODELO = downloadModeloPretreinado(model_args)\n","        logging.info(\"Usando modelo BERT pré-treinado.\")        \n","        \n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"6tKcaIfReqdy"},"source":["## 4.2 Tokenizador BERT"]},{"cell_type":"markdown","metadata":{"id":"e8n7Z5s-QZF8"},"source":["### Função carrega Tokenizador BERT\n","\n","O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzAuptkwQZR3"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertTokenizer # Importando as bibliotecas do tokenizador BERT.\n","\n","def carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o tokenizador do DIRETORIO_MODELO.\n","      O tokenizador utiliza WordPiece.\n","      Carregando o tokenizador do diretório \"./modelo/\" do diretório padrão se variável `DIRETORIO_MODELO` setada.\n","      Caso contrário carrega da comunidade\n","      Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n","      O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados.\n","    \n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.           \n","        `model_args` - Objeto com os argumentos do modelo.       \n","    \n","      Retorno:\n","        `tokenizer` - Tokenizador BERT.\n","    \"\"\"\n","\n","    tokenizer = None\n","    \n","    # Se a variável DIRETORIO_MODELO foi setada.\n","    if DIRETORIO_MODELO:\n","        # Carregando o Tokenizador.\n","        logging.info(\"Carregando o tokenizador BERT do diretório {}.\".format(DIRETORIO_MODELO))\n","\n","        tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, do_lower_case=model_args.do_lower_case)\n","\n","    else:\n","        # Carregando o Tokenizador da comunidade.\n","        logging.info(\"Carregando o tokenizador BERT da comunidade.\")\n","\n","        tokenizer = BertTokenizer.from_pretrained(model_args.pretrained_model_name_or_path, do_lower_case=model_args.do_lower_case)\n","\n","    return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"GYRV9KfHQE6v"},"source":["## 4.3 Carrega o modelo e tokenizador BERT\n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"markdown","metadata":{"id":"-pZZrUKRhR3e"},"source":["### Função carrega modelo BERT medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JUEyjCChUQh"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertModel # Importando as bibliotecas do Modelo BERT.\n","\n","def carregaModeloMedida(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o modelo e retorna o modelo.\n","    \n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.           \n","        `model_args` - Objeto com os argumentos do modelo.   \n","    \n","      Retorno:\n","        `model` - Um objeto do modelo BERT carregado.\n","    \"\"\"\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in model_args.pretrained_model_name_or_path:\n","        URL_MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Se a variável URL_MODELO foi setada\n","    if URL_MODELO:        \n","        # Carregando o Modelo BERT\n","        logging.info(\"Carregando o modelo BERT do diretório {} para cálculo de medidas.\".format(DIRETORIO_MODELO))\n","\n","        model = BertModel.from_pretrained(DIRETORIO_MODELO,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","        \n","    else:\n","        # Carregando o Modelo BERT da comunidade\n","        logging.info(\"Carregando o modelo BERT da comunidade {} para cálculo de medidas.\".format(model_args.pretrained_model_name_or_path))\n","\n","        model = BertModel.from_pretrained(model_args.pretrained_model_name_or_path,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"-uFDhRTZe2Js"},"source":["### Função carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVtAUbUBe2iS"},"outputs":[],"source":["def carregaBERT(model_args):\n","    \"\"\" \n","      Carrega o BERT para cálculo de medida ou classificação e retorna o modelo e o tokenizador.\n","      O tipo do model retornado pode ser BertModel ou BertForSequenceClassification, depende do tipo de model_args.\n","    \n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.       \n","          - Se model_args = ModeloArgumentosClassificacao deve ser carregado o BERT para classificação(BertForSequenceClassification).\n","          - Se model_args = ModeloArgumentosMedida deve ser carregado o BERT para cálculo de medida(BertModel).\n","\n","      Retorno:    \n","        `model` - Um objeto do modelo BERT carregado.       \n","        `tokenizer` - Um objeto tokenizador BERT carregado.       \n","    \"\"\"\n","            \n","    # Verifica a origem do modelo\n","    DIRETORIO_MODELO = verificaModelo(model_args)\n","    \n","    # Variável para conter o modelo\n","    model = None\n","    \n","    # Carrega o modelo para cálculo da medida\n","    model = carregaModeloMedida(DIRETORIO_MODELO, model_args)\n","                \n","    # Carrega o tokenizador. \n","    # O tokenizador é o mesmo para o classificador e medidor.\n","    tokenizer = carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args)\n","    \n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"x5NTxBRKfAcT"},"source":["### Carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYMLJJYSQHY3"},"outputs":[],"source":["# Carrega o modelo e tokenizador do BERT\n","model, tokenizer = carregaBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"d7KprWqyZBQZ"},"source":["### Recupera detalhes do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6sPjTQnuQV2"},"outputs":[],"source":["# Verifica o nome do modelo BERT a ser utilizado\n","MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","# Verifica o tamanho do modelo(default large)\n","TAMANHO_BERT = getTamanhoBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"khTFfBVbnsx9"},"source":["## 4.4 Funções auxiliares do BERT"]},{"cell_type":"markdown","metadata":{"id":"lCJzsw8T0I-5"},"source":["### concatenaListas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpmDZ1mI0JHR"},"outputs":[],"source":["def concatenaListas(lista, pos=1):\n","  lista_concat = []\n","\n","  for x in lista:\n","      lista_concat = lista_concat + x[pos]\n","  \n","  return lista_concat"]},{"cell_type":"markdown","metadata":{"id":"s42mgtmSZ8MR"},"source":["### getEmbeddingsCamadas\n","\n","Funções que recuperam os embeddings das camadas:\n","- Primeira camada;\n","- Penúltima camada;\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgo3EBTRZ9-3"},"outputs":[],"source":["def getEmbeddingPrimeiraCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","      \n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  resultado = output[2][0]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  \n","  return resultado\n","\n","def getEmbeddingPenultimaCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","      \n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  resultado = output[2][-2]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  \n","  return resultado\n","\n","def getEmbeddingUltimaCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","     \n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  resultado = output[2][-1]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  \n","  return resultado    \n","\n","def getEmbeddingSoma4UltimasCamadas(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","      \n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  embedding_camadas = output[2][-4:]\n","  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","\n","  # Usa o método `stack` para criar uma nova dimensão no tensor \n","  # com a concateção dos tensores dos embeddings.        \n","  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  resultado_stack = torch.stack(embedding_camadas, dim=0)\n","  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  \n","  # Realiza a soma dos embeddings de todos os tokens para as camadas\n","  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  resultado = torch.sum(resultado_stack, dim=0)\n","  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  \n","  return resultado\n","\n","def getEmbeddingConcat4UltimasCamadas(output):  \n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","      \n","  # Cria uma lista com os tensores a serem concatenados\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n","  # Lista com os tensores a serem concatenados  \n","  lista_concat = []\n","\n","  # Percorre os 4 últimos\n","  for i in [-1,-2,-3,-4]:\n","      # Concatena da lista\n","      lista_concat.append(output[2][i])\n","\n","  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)    \n","  # Realiza a concatenação dos embeddings de todos as camadas\n","  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n","  resultado = torch.cat(lista_concat, dim=-1)\n","  \n","  # Saída: Entrada: (<1(lote)> x <qtde_tokens> x <3072 ou 4096>)    \n","  return resultado   \n","\n","def getEmbeddingSomaTodasAsCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","   \n","  # Retorna todas as camadas descontando a primeira(0)\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  embedding_camadas = output[2][1:]\n","  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  \n","  # Usa o método `stack` para criar uma nova dimensão no tensor \n","  # com a concateção dos tensores dos embeddings.        \n","  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n","  resultado_stack = torch.stack(embedding_camadas, dim=0)\n","  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","    \n","  # Realiza a soma dos embeddings de todos os tokens para as camadas\n","  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  resultado = torch.sum(resultado_stack, dim=0)\n","  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","    \n","  return resultado"]},{"cell_type":"markdown","metadata":{"id":"Y8MjE0utzlZT"},"source":["### getEmbeddings\n","\n","Função para gerar os embeddings de sentenças.\n","\n","Existe uma função para os tipos de camadas utilizadas:\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QcqOuwS067Q"},"outputs":[],"source":["def getEmbeddingsUltimaCamada(documento, modelo, tokenizer):\n","    \n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário    \n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","    \n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","    \n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():        \n","        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding    \n","    camada = getEmbeddingUltimaCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n"," \n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1wDGBl067Y"},"outputs":[],"source":["def getEmbeddingsSoma4UltimasCamadas(documento, modelo, tokenizer):\n","    \n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário    \n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","    \n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","    \n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():        \n","        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding    \n","    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","   \n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hym19Hxr067Y"},"outputs":[],"source":["def getEmbeddingsConcat4UltimasCamadas(documento, modelo, tokenizer):\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário    \n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","    \n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():        \n","        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","    \n","    # Camada embedding    \n","    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-PLZiUR067Z"},"outputs":[],"source":["def getEmbeddingsSomaTodasAsCamadas(documento, modelo, tokenizer):\n","    \n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário    \n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","    \n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","    \n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():        \n","        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding    \n","    camada = getEmbeddingSomaTodasAsCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"zFd1rse11DpZ"},"source":["### getDocumentoTokenizado \n","\n","Retorna o documento tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvWIBFTLJ7z9"},"outputs":[],"source":["def getDocumentoTokenizado(documento, tokenizer):\n","    \"\"\"\n","      Retorna o documento tokenizado pelo BERT.\n","    \n","      Parâmetros:\n","      `documento` - Documento a ser tokenizado.\n","      `tokenizer` - Tokenizador do BERT.\n","    \"\"\"    \n","\n","    # Adiciona os tokens especiais.\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Documento tokenizado\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    del tokenizer\n","\n","    return documento_tokenizado    "]},{"cell_type":"markdown","metadata":{"id":"1qezcBkxnEdR"},"source":["# 5 - Exemplo de projeção\n","\n","Apresenta a projeção dos embeddings das sentenças gerados pelo BERT. "]},{"cell_type":"markdown","metadata":{"id":"oQUy9Tat2EF_"},"source":["## 5.1 Carregamento dos arquivos de dados originais e permutados"]},{"cell_type":"markdown","metadata":{"id":"bD_tNbBGPrnE"},"source":["### 5.1.1 Especifica os nomes dos arquivos de dados\n","\n"]},{"cell_type":"code","metadata":{"id":"bNgwJRC2uGJb"},"source":["# Nome do arquivo\n","NOME_ARQUIVO_ORIGINAL = \"original.csv\"\n","NOME_ARQUIVO_ORIGINAL_COMPACTADO = \"original.zip\"\n","NOME_ARQUIVO_ORIGINAL_POS = \"originalpos.csv\"\n","NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO = \"originalpos.zip\"\n","\n","NOME_ARQUIVO_PERTURBADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_COMPACTADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\"\n","NOME_ARQUIVO_PERTURBADO_POS = \"perturbadopos_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO = \"perturbadopos_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1.2 Cria o diretório local para receber os dados"],"metadata":{"id":"I0LsmsBlJeeV"}},{"cell_type":"code","metadata":{"id":"gFYIHcIHE985"},"source":["# Importando as bibliotecas.\n","import os\n","\n","# Cria o diretório para receber os arquivos Originais e Permutados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL[:-1]\n","\n","if not os.path.exists(dirbase):  \n","    # Cria o diretório\n","    os.makedirs(dirbase)    \n","    logging.info(\"Diretório criado: {}\".format(dirbase))\n","else:    \n","    logging.info(\"Diretório já existe: {}\".format(dirbase))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D8A9syejCsD2"},"source":["### 5.1.3 Copia os arquivos do Google Drive para o Colaboratory"]},{"cell_type":"code","metadata":{"id":"pviuxToMCxQw"},"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a cópia.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFCvZ6CUmt-9"},"source":["Descompacta os arquivos.\n","\n","Usa o unzip para descompactar:\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens \n","*   `-d` Diretório de destino\n"]},{"cell_type":"code","metadata":{"id":"dbHl3d88mouc"},"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a descompactação.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzhYJNWJm1z4"},"source":["### 5.1.4 Carregamento das lista com os dados dos arquivos originais, perturbados e permutados"]},{"cell_type":"markdown","metadata":{"id":"Usr1uRzQeJSb"},"source":["#### Carrega o arquivo dos dados originais e POS"]},{"cell_type":"code","metadata":{"id":"QRHlixdHEDTb"},"source":["# Import das bibliotecas.\n","import pandas as pd\n","\n","# Abre o arquivo e retorna o DataFrame\n","lista_documentos_originais = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_originais_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lista_documentos_originais = lista_documentos_originais[:5]"],"metadata":{"id":"mzl2SUe7N4QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJ5STBZPLlie"},"source":["lista_documentos_originais.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NVwIdXXDFn7"},"source":["lista_documentos_originais_pos.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Corrigir os tipos de colunas dos dados originais e POS\n","\n","Em dados originais:\n","- coluna 1 - `sentenças` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados originais pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."],"metadata":{"id":"-hfUpvKqXoqe"}},{"cell_type":"code","source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","# Verifica se o tipo da coluna não é list e converte\n","lista_documentos_originais[\"sentencas\"] = lista_documentos_originais[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","lista_documentos_originais_pos[\"pos_documento\"] = lista_documentos_originais_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","logging.info(\"TERMINADO CORREÇÃO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO CORREÇÃO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"],"metadata":{"id":"lj9sJVavMccj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Criando dados indexados originais"],"metadata":{"id":"8yyRt4jnYxsU"}},{"cell_type":"code","source":["# Expecifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_originais_indexado = lista_documentos_originais.set_index([\"id\"])\n","lista_documentos_originais_indexado.head()"],"metadata":{"id":"B9INo4nBS8aQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Expecifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_originais_pos_indexado = lista_documentos_originais_pos.set_index([\"id\"])\n","lista_documentos_originais_pos_indexado.head()"],"metadata":{"id":"j70x_r30T_bx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJXcpioo7Bhn"},"source":["#### Carrega o arquivo dos dados perturbados e POS"]},{"cell_type":"code","metadata":{"id":"gB500dmd7Bho"},"source":["# Abre o arquivo e retorna o DataFrame\n","lista_documentos_perturbados = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_perturbados_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO PERTURBADOS: {}.\".format(len(lista_documentos_perturbados)))\n","logging.info(\"TERMINADO PERTURBADOS POS: {}.\".format(len(lista_documentos_perturbados_pos)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AlgUns csv estão com os nomes das colunas errados"],"metadata":{"id":"qzTPk_BsFdzc"}},{"cell_type":"code","source":["lista_documentos_perturbados = lista_documentos_perturbados.rename(columns={'documentoPerturbado': 'documento_perturbado'})"],"metadata":{"id":"YlJ7P-kzFYmR"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQ9cgAz47Bhp"},"source":["lista_documentos_perturbados.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_perturbados_pos.sample(5)"],"metadata":{"id":"IE1xJdZWkc5I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Corrigir os tipos de colunas dos dados perturbados e POS\n","\n","Em dados perturbados:\n","- coluna 1 - `perturbado` carregadas do arquivo vem como string e não como lista.\n","- coluna 3 - `sentencas` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados perturbados pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."],"metadata":{"id":"VrfZzjjpsUOU"}},{"cell_type":"code","source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","# Verifica se o tipo da coluna não é list e converte\n","lista_documentos_perturbados[\"perturbado\"] = lista_documentos_perturbados[\"perturbado\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","lista_documentos_perturbados[\"sentencas\"] = lista_documentos_perturbados[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","lista_documentos_perturbados_pos[\"pos_documento\"] = lista_documentos_perturbados_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","logging.info(\"TERMINADO CORREÇÃO PERTURBADO: {}.\".format(len(lista_documentos_perturbados)))\n","logging.info(\"TERMINADO CORREÇÃO PERTURBADO POS: {}.\".format(len(lista_documentos_perturbados_pos)))"],"metadata":{"id":"ZHf-7dgSsUOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_perturbados.sample(5)"],"metadata":{"id":"U-w5YwpbNqu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_perturbados_pos.sample(5)"],"metadata":{"id":"1laR3W5hP6iu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Criando dados indexados perturbados"],"metadata":{"id":"Ix-Q5fZXY3HR"}},{"cell_type":"code","source":["# Expecifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_perturbados_indexado = lista_documentos_perturbados.set_index([\"id\"])\n","lista_documentos_perturbados_indexado.head()"],"metadata":{"id":"FqRQnYUtSxzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Expecifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_perturbados_pos_indexado = lista_documentos_perturbados_pos.set_index([\"id\"])\n","lista_documentos_perturbados_pos_indexado.head()"],"metadata":{"id":"s0aDUbeZT1M8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1.5 Agrupar os dados originais e perturbados"],"metadata":{"id":"m-vP_FnPWe0K"}},{"cell_type":"code","source":["# Import das bibliotecas.\n","import ast\n","from tqdm.notebook import tqdm as tqdm_notebook\n","\n","def agruparDadosOriginaisPerturbados(lista_documentos_originais, lista_documentos_perturbados_indexado):\n","\n","  print(\"Processando\",len(lista_documentos_originais),\"documentos originais\")\n","\n","  lista_documentos_agrupados = []\n","\n","  # Barra de progresso dos documentos\n","  lista_documentos_originais_bar = tqdm_notebook(lista_documentos_originais.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(lista_documentos_originais))\n","\n","  # Percorre os documentos\n","  for i, linha_documento in lista_documentos_originais_bar: \n","    #if i < 2:\n","      #print(\"linha_documento:\",linha_documento)\n","      # Recupera o id do documento\n","      id_documento = linha_documento[0]     \n","      #print(\"id_documento:\",id_documento)     \n","  \n","      # Carrega a lista das sentenças do documento\n","      lista_sentenca_documento = linha_documento[1]    \n","      #print(\"\\nlista_sentenca_documento:\",lista_sentenca_documento)\n","      #print(\"len(lista_sentenca_documento):\",len(lista_sentenca_documento)) \n","\n","      # Adiciona o original a lista dos dados agrupados, considerando como coerente(1)\n","      lista_documentos_agrupados.append([id_documento, lista_sentenca_documento, linha_documento[2], 1])\n","    \n","      # Percorre os documentos perturbados apartir do original\n","      for j in range(0, model_args.documentos_perturbados):\n","\n","        # Id do documento perturbado\n","        id_perturbado = str(id_documento) + \"_pert_\" + str(j)\n","\n","        # localiza o documento perturbado \n","        #documento_perturbado = lista_documentos_perturbados.loc[lista_documentos_perturbados['id']==id_perturbado].values[0]\n","        documento_perturbado = lista_documentos_perturbados_indexado.loc[id_perturbado]\n","        # Recupera a sentença do documento perturbado\n","        lista_perturbado = documento_perturbado[0]\n","            \n","        # Adiciona o perturbado a lista dos dados agrupados considerando como incoerente(0)\n","        lista_documentos_agrupados.append([id_perturbado, lista_perturbado, documento_perturbado[1], 0])    \n","\n","  logging.info(\"TERMINADO AGRUPAMENTO: {}.\".format(len(lista_documentos_agrupados)))\n","\n","  # Cria o dataframe da lista\n","  lista_documentos_agrupados = pd.DataFrame(lista_documentos_agrupados, columns = [\"id\",\"sentencas\",\"documento\",\"classe\"])\n","\n","  # Corrige os tipos dos dados da lista agrupada\n","  tipos = {\"id\": str, \"sentencas\": object, \"documento\": str, \"classe\": int}\n","\n","  lista_documentos_agrupados = lista_documentos_agrupados.astype(tipos)\n","\n","  return lista_documentos_agrupados"],"metadata":{"id":"NUJBBHy8We0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importa das bibliotecas\n","import pandas as pd\n","\n","print(\"Analisando documentos originais e perturbados\")\n","# Concatena as listas de documentos originais e perturbados\n","lista_documentos_agrupados = agruparDadosOriginaisPerturbados(lista_documentos_originais, lista_documentos_perturbados_indexado)\n","lista_documentos_agrupados_pos = pd.concat([lista_documentos_originais_pos, lista_documentos_perturbados_pos])\n","\n","# Corrige o tipo de dado da coluna id da lista\n","tipos = {\"id\": str}\n","lista_documentos_agrupados_pos = lista_documentos_agrupados_pos.astype(tipos)"],"metadata":{"id":"wY3dqPGVdoHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logging.info(\"TERMINADO AGRUPAMENTO: {}.\".format(len(lista_documentos_agrupados)))"],"metadata":{"id":"AFvbzDbwdrYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_agrupados.sample(5)"],"metadata":{"id":"a5ZV4jzAWe0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logging.info(\"TERMINADO AGRUPAMENTO POS: {}.\".format(len(lista_documentos_agrupados_pos)))"],"metadata":{"id":"4mCqTRecWe0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Criar dados indexados"],"metadata":{"id":"viicg1E7mXLK"}},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_agrupados_indexado = lista_documentos_agrupados.set_index([\"id\"])\n","lista_documentos_agrupados_indexado.head()"],"metadata":{"id":"0YBdkvoPm2vO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_agrupados_pos_indexado = lista_documentos_agrupados_pos.set_index([\"id\"])\n","lista_documentos_agrupados_pos_indexado.head()"],"metadata":{"id":"NQjlOJzOmbsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuM-kq1upR3M"},"source":["## 5.2 Gera os arquivos para o Embedding Projector"]},{"cell_type":"markdown","metadata":{"id":"8FRocbK4_wTk"},"source":["### 5.2.1 Cria o diretório para os arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaGALkXc_zLl"},"outputs":[],"source":["# Importando as bibliotecas.\n","import os\n","\n","# Cria o diretório para receber os arquivos Originais e Permutados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL + \"projector\"\n","\n","if not os.path.exists(dirbase):  \n","    # Cria o diretório\n","    os.makedirs(dirbase)    \n","    logging.info(\"Diretório criado: {}\".format(dirbase))\n","else:    \n","    logging.info(\"Diretório já existe: {}\".format(dirbase))"]},{"cell_type":"markdown","metadata":{"id":"eFugAWlm1VfN"},"source":["### 5.2.2 Gera os embeddings dos documentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maBbfHFj1VfN"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","\n","lista_embeddings = []\n","lista_documentos = []\n","lista_documentos_classe = []\n","lista_documentos_id = []\n","lista_documentos_origem = []\n","\n","if CLASSE_DOCUMENTO != 2:\n","  documentos = lista_documentos_agrupados.loc[lista_documentos_agrupados['classe'] == CLASSE_DOCUMENTO] \n","else:\n","  documentos = lista_documentos_agrupados\n","\n","# Barra de progresso dos documentos\n","documentos_bar = tqdm_notebook(documentos.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(documentos))  \n","\n","# Percorre os documentos\n","for i, linha_documento in documentos_bar:\n","\n","    # Recupera o id do documento\n","    id_documento = linha_documento[0]    \n","    # print(\"id_documento:\",id_documento)  \n","    # print(\"linha_documento['documento']:\", linha_documento['documento'])  \n","\n","    # Recupera a classe documento (1-original 0-perturbado)\n","    classe = linha_documento['classe']\n","    #print(\"classe:\",classe)\n","\n","    # Localiza a POSTagging do documento agrupado    \n","    lista_pos_documento = lista_documentos_agrupados_pos_indexado.loc[id_documento][0]\n","    # print(\"lista_pos_documento:\",lista_pos_documento)\n","    # print(\"len(lista_pos_documento):\",len(lista_pos_documento))\n","        \n","    # Troca o documento por uma versão da concatenação das palavras geradas pelo spaCy\n","    # Percorre a lista_pos concatenando a posição 0 dos tokens\n","    documento_concatenado = \" \".join(concatenaListas(lista_pos_documento, pos=0))\n","    # print(\"documento_concatenado:\", documento_concatenado)\n","    documento = documento_concatenado\n","\n","    if CLASSE_DOCUMENTO != 1:\n","      # Recupera a posição do traço no id do arquivo\n","      traco_ix = id_documento.find(\"_\")\n","      if traco_ix != -1:\n","        # Recupera o id da perturbacao até a posição do traço até o fim\n","        id_perturbacao = id_documento[:traco_ix]\n","      else:\n","        id_perturbacao = id_documento\n","\n","    # Recupera os embeddings do token CLS\n","    if ESTRATEGIA_EMBEDDING == 0:\n","        # print(\"Recuperando os embeddings do token CLS\")\n","        # Gera embeddings concatenando as 4 últimas camadas do BERT\n","        token_embeddings, documento_tokenizado = getEmbeddingsUltimaCamada(documento, model, tokenizer)\n","        # print(\"token_embeddings=\", token_embeddings.shape)\n"," \n","        #Recupera os embeddings do token CLS\n","        token_cls = token_embeddings[0:1]\n","\n","        # Guarda os embeddings e os os outros dados do documento\n","        # Guarda o embedding do token [CLS]\n","        lista_embeddings.append(token_cls)\n","        lista_documentos.append(documento)            \n","        lista_documentos_classe.append(classe)\n","        lista_documentos_id.append(id_documento)\n","        if CLASSE_DOCUMENTO != 1:\n","          lista_documentos_origem.append(id_perturbacao)\n","\n","    else:\n","      # Recupera os embeddings da última camada\n","      if ESTRATEGIA_EMBEDDING == 1:\n","        # Gera embeddings concatenando da última camada do BERT\n","        token_embeddings, documento_tokenizado = getEmbeddingsUltimaCamada(documento, model, tokenizer)\n","\n","        # Estratégia de medida Mean\n","        if ESTRATEGIA_MEDIDA == 0:\n","          # Entrada: <qtde_tokens> x <768 ou 1024>  \n","          # print(\"token_embeddings=\", token_embeddings.shape)\n","          # Remove o token [CLS] e [SEP]\n","          embedding_documento = torch.mean(token_embeddings[1:-1], dim=0)\n","          # Saída: <768 ou 1024>\n","          # print(\"embedding_documento=\", embedding_documento.shape)\n","        else:\n","          # Estratégia de medida Max\n","          # Entrada: <qtde_tokens> x <768 ou 1024>  \n","          # print(\"token_embeddings=\", token_embeddings.shape)\n","          embedding_documento, linha = torch.max(token_embeddings[1:-1], dim=0)\n","          # Saída: <768 ou 1024>\n","          # print(\"embedding_documento=\", embedding_documento.shape)\n","          \n","        # Guarda os embeddings e os os outros dados do documento        \n","        lista_embeddings.append(embedding_documento)\n","        lista_documentos.append(documento)            \n","        lista_documentos_classe.append(classe)\n","        lista_documentos_id.append(id_documento)\n","        if CLASSE_DOCUMENTO != 1:\n","          lista_documentos_origem.append(id_perturbacao)\n","      else:\n","        # Recupera os embeddings da concatenação das 4 últimas camadas\n","        if ESTRATEGIA_EMBEDDING == 2:\n","          # print(\"Recuperando a concatenação das 4 últimas camadas\")\n","          # Gera embeddings concatenando as 4 últimas camadas do BERT\n","          token_embeddings, documento_tokenizado = getEmbeddingsConcat4UltimasCamadas(documento, model, tokenizer)\n","\n","          # Estratégia de medida Mean\n","          if ESTRATEGIA_MEDIDA == 0:            \n","            # Entrada: <qtde_tokens> x <768 ou 1024>  \n","            # print(\"token_embeddings=\", token_embeddings.shape)\n","            # Remove o token [CLS] e [SEP]\n","            embedding_documento = torch.mean(token_embeddings[1:-1], dim=0)\n","            # Saída: <768 ou 1024>\n","            # print(\"embedding_documento=\", embedding_documento.shape)\n","          else:\n","            # Estratégia de medida Max\n","            # Entrada: <qtde_tokens> x <768 ou 1024>  \n","            # print(\"token_embeddings=\", token_embeddings.shape)\n","            embedding_documento, linha = torch.max(token_embeddings[1:-1], dim=0)\n","            # Saída: <768 ou 1024>\n","            # print(\"embedding_documento=\", embedding_documento.shape)\n","            \n","          # Guarda os embeddings e os os outros dados do documento          \n","          lista_embeddings.append(embedding_documento)\n","          lista_documentos.append(documento)            \n","          lista_documentos_classe.append(classe)\n","          lista_documentos_id.append(id_documento)\n","          if CLASSE_DOCUMENTO != 1:\n","            lista_documentos_origem.append(id_perturbacao)"]},{"cell_type":"markdown","metadata":{"id":"VOyrUs1d1VfO"},"source":["Mostra um documento processado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLp3DWq61VfO"},"outputs":[],"source":["print(len(lista_embeddings[0]))\n","print(lista_documentos_classe[0])"]},{"cell_type":"markdown","metadata":{"id":"za-9WAYO1VfQ"},"source":["### 5.2.3 Gera os arquivos para o Embedding Projector"]},{"cell_type":"markdown","source":["Gera o sufixo do nome do arquivo"],"metadata":{"id":"R9zqngHokA0-"}},{"cell_type":"code","source":["def getSufixoNomeArquivo():\n","  \n","  sufixo_arquivo = \"_\"\n","\n","  # Documento perturbados\n","  if CLASSE_DOCUMENTO == 0:\n","      sufixo_arquivo = sufixo_arquivo + \"PERTDO\" + \"_P\" + str(DOCUMENTOS_PERTURBADOS)\n","  else:\n","    # Documento originais\n","    if CLASSE_DOCUMENTO == 1:\n","      sufixo_arquivo = sufixo_arquivo + \"DO\"      \n","    else:\n","      # Documento originais e perturbados\n","      if CLASSE_DOCUMENTO == 2:\n","        sufixo_arquivo = sufixo_arquivo + \"DO_PERTDO\"  + \"_P\" + str(DOCUMENTOS_PERTURBADOS) + \"_CLASSE\"\n","\n","  # Embeddingsdo token CLS\n","  if ESTRATEGIA_EMBEDDING == 0:\n","    # Tamanho dos embeddings\n","    sufixo_arquivo = sufixo_arquivo + \"_\" + str(lista_embeddings[0].size()[1]) + TAMANHO_BERT \n","    \n","    # Adiciona a palavra CLS\n","    sufixo_arquivo = sufixo_arquivo + \"_CLS\"\n","\n","    # Não possui MEAN ou MAX\n","  else:\n","    # Embeddings da última camada ou da concatenação das 4 últimas camadas\n","    if ESTRATEGIA_EMBEDDING == 1 or ESTRATEGIA_EMBEDDING == 2:   \n","      sufixo_arquivo = sufixo_arquivo + \"_\" + str(lista_embeddings[0].size()[0]) + TAMANHO_BERT \n","\n","      # Média dos embeddings\n","      if ESTRATEGIA_MEDIDA == 0:\n","        sufixo_arquivo = sufixo_arquivo + \"_MEAN\"\n","      else:\n","        # Máximo dos embeddings\n","        sufixo_arquivo = sufixo_arquivo + \"_MAX\"\n","  return sufixo_arquivo"],"metadata":{"id":"RNiX8OU7K4-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCR6vo371VfQ"},"source":["Arquivos com os valores dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbILMYpG1VfQ"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","import csv\n","  \n","# Recupera o sufixo do nome do arquivo\n","sufixo_arquivo = getSufixoNomeArquivo()\n","#print(\"sufixo_arquivo:\", sufixo_arquivo)\n","\n","# Concatena o diretório e o nome do arquivo  \n","NOME_ARQUIVO_RECORD =  DIRETORIO_LOCAL + \"projector/\" + \"records_sentenca\" + sufixo_arquivo + \".tsv\"\n","\n","# Abre o arquivo\n","with open(NOME_ARQUIVO_RECORD, 'w', encoding='utf8') as tsvfile:\n","  # Cria um arquivo separado por tab\n","    writer = csv.writer(tsvfile, delimiter='\\t')    \n","\n","    # Barra de progresso dos embedings\n","    lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","    # Percorre os embeddings\n","    for i, documento_embedding in lista_embeddings_bar:\n","\n","      # Embedding [CLS]\n","      if ESTRATEGIA_EMBEDDING == 0:\n","    \n","        # Converte os tensores em numpy array    \n","        documento_embedding_np = documento_embedding.numpy()\n","        \n","        # Escreve no arquivo os embeddings do documento               \n","        writer.writerows(documento_embedding_np)                  \n","        \n","      # Estratégia de medida Mean e Max\n","      else:\n","      \n","        # Converte os tensores em numpy array    \n","        documento_embedding_np = [documento_embedding.numpy()]\n","\n","        # Escreve no arquivo os embeddings do documento               \n","        writer.writerows(documento_embedding_np)                \n","    "]},{"cell_type":"markdown","metadata":{"id":"mb580LQT1VfR"},"source":["Arquivo com os metadados dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XskFjQ5v1VfR"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","import csv\n","\n","# Recupera o sufixo do nome do arquivo\n","sufixo_arquivo = getSufixoNomeArquivo()\n","#print(\"sufixo_arquivo:\", sufixo_arquivo)\n","\n","# Concatena o diretório e o nome do arquivo\n","NOME_ARQUIVO_META =  DIRETORIO_LOCAL + \"projector/\" + \"meta_sentenca\" + sufixo_arquivo + \".tsv\"\n","\n","# Abre o arquivo\n","with open(NOME_ARQUIVO_META, 'w', encoding='utf8') as tsvfile:\n","    # Define o escritor do arquivo\n","    writer = csv.writer(tsvfile, delimiter='\\t')\n","                \n","    # Sem a classe do documento \n","    if CLASSE_DOCUMENTO != 2: \n","        writer.writerow([\"Sentença\", \"Id\"])  \n","\n","        # Barra de progresso dos embedings\n","        lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","        # Percorre os embeddings\n","        for i, documento_embedding in lista_embeddings_bar:\n","           #Escreva a sentença\n","           s = [lista_documentos[i],         \n","                lista_documentos_id[i]]\n","           writer.writerow(s)\n","\n","    else:\n","        # Com a classe do documento \n","        # Cria o rótulo próximo(__next__) para linkar em sequência os documentos de uma mesma origem\n","        if LIGACAO_PROXIMO_DOCUMENTO == True:   \n","          # Com classe\n","          writer.writerow([\"Sentença\", \"Id\", \"Origem\", \"__next__\", \"Classe\"])  \n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Conta o número de sequências de um documento\n","          conta_proximo = 0\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","                            \n","              # Incrementa em 1 o contador do próximo\n","              conta_proximo = conta_proximo + 1\n","              # Transforma o i+1 que é o indicador da sequência em string\n","              proximo = str((i + 1))\n","              \n","              # Se chegou no último documento da sequência reinicia o contador com 0 e Branco para o próximo\n","              if conta_proximo > DOCUMENTOS_PERTURBADOS:\n","                conta_proximo = 0\n","                proximo = \"\"\n","\n","              # Escreve a palavra e sua sentença            \n","              s = [lista_documentos[i], \n","                  lista_documentos_id[i],\n","                  lista_documentos_origem[i], \n","                  proximo,\n","                  lista_documentos_classe[i]]\n","              writer.writerow(s)   \n","        else:\n","          # Sem o rótulo \"__next__\" \n","          writer.writerow([\"Sentença\", \"Id\", \"Origem\", \"Classe\"])  \n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","                \n","              # Escreve a palavra e sua sentença            \n","              s = [lista_documentos[i], \n","                  lista_documentos_id[i],\n","                  lista_documentos_origem[i], \n","                  lista_documentos_classe[i]]\n","              writer.writerow(s)   "]},{"cell_type":"markdown","metadata":{"id":"S23zNSjM1VfR"},"source":["Faça o download dos arquivos **records_sentenca_4096.tsv** e **meta_sentenca_4096.tsv** e carregue em https://projector.tensorflow.org/ na opção load.\n","\n","Faça o download dos arquivos gerados pelo notebook clicando na lateral esquerda no ícone \"Arquivos\".\n","\n","Carrega os arquivos na ferramenta através do link \"Load\". Na opção existe um link botão para carregar o arquivo dos embeddings e um outro botão para carregar os metadados.\n","\n","Você também pode utilizar um link a um arquivo de configuração config.json com a referência aos arquivos em algum repositório publico na internet, por exemplo github ou gist\n","\n","Aqui um exemplo.\n","\n","https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/osmarbraz/cohebertv1projecao/main/config.json\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t56ovS3Kt-W0"},"source":["### 5.3.4 Compacta e copia o arquivo do projetor para uma pasta do GoogleDrive"]},{"cell_type":"markdown","metadata":{"id":"MnElyyGbcmlV"},"source":["Compacta o arquivo gerado da comparação para facilitar o envio para o GoogleDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGK7bu_P2F-m"},"outputs":[],"source":["# Nome do arquivo\n","NOME_ARQUIVO_PROJECTOR_COMPACTADO = \"projector_sentenca.zip\""]},{"cell_type":"markdown","metadata":{"id":"5vZq_1sEtsB0"},"source":["Compacta os arquivos.\n","\n","Usa o zip para compactar:\n","*   `-r` Compacta o diretório\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NunMOJWR2O8H"},"outputs":[],"source":["!zip -r -o -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PROJECTOR_COMPACTADO\" \"$DIRETORIO_LOCAL\"\"/projector/\""]},{"cell_type":"markdown","metadata":{"id":"sw3p4ydkt-W-"},"source":["Copia o arquivo compactado para o GoogleDrive\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5c8sv10t-W_"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","    # Copia o arquivo original   \n","    # !cp \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PROJECTOR_COMPACTADO\" \"$DIRETORIO_DRIVE\"\n","\n","    logging.info(\"Terminei a cópia\")"]},{"cell_type":"markdown","metadata":{"id":"NYBu_-xVdHe4"},"source":["## 5.3 Projeção dos embeddings"]},{"cell_type":"markdown","metadata":{"id":"68qT3LkGtRgg"},"source":["### Configuração"]},{"cell_type":"markdown","metadata":{"id":"PHLxY3piwAB-"},"source":["Verifica a versão do tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5A6v8akfdG6j"},"outputs":[],"source":["try:  \n","  # %tensorflow_version só existe no Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"Xh5VnBHxtUS7"},"source":["Importa a biblioteca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nABca5ukdPE_"},"outputs":[],"source":["# Importa de biblioteca\n","from tensorboard.plugins import projector"]},{"cell_type":"markdown","metadata":{"id":"ix1XJaMqtWqv"},"source":["### Configura o diretório dos logs e arquivos de configuração\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj1SzSWAfZIJ"},"outputs":[],"source":["# Configure um diretório de logs\n","log_dir =\"/content/projector/\"\n","if not os.path.exists(log_dir):\n","    os.makedirs(log_dir)"]},{"cell_type":"markdown","metadata":{"id":"bI0SITtUsZMx"},"source":["### Cria os arquivos de configuração dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U9hYPvidYls"},"outputs":[],"source":["# Configura o projetor\n","config = projector.ProjectorConfig()\n","\n","# Configuração do primeiro conjunto de embeddings sem pooling\n","embedding = config.embeddings.add()\n","# Nome do tensor\n","embedding.tensor_name = \"Sentenças BERTimbau large\"\n","# Caminho para os metadados\n","embedding.metadata_path = NOME_ARQUIVO_META\n","# Caminho para os tensores\n","embedding.tensor_path = NOME_ARQUIVO_RECORD\n","# Salva o arquivo de configuração\n","projector.visualize_embeddings(log_dir, config)"]},{"cell_type":"markdown","metadata":{"id":"SkrnWNMLvpCq"},"source":["### Mata o processo\n","\n","Se executar novamente o notebook é necessário matar o processo do tensorprojector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgTL4Ns2gINB"},"outputs":[],"source":["# Mata o processo do tensorboard\n","#!kill 407"]},{"cell_type":"markdown","metadata":{"id":"xlXJ7-Uytj3f"},"source":["### Visualizando a projeção\n","\n","Na caixa de seleção selecione \"PROJECTOR\" no lugar de \"INACTIVE\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoAAOlWXdf0t"},"outputs":[],"source":["# Agora execute o tensorboard nos dados de log que acabamos de salvar.\n","%tensorboard --logdir /content/projector"]}]}