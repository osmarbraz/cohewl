{"cells":[{"cell_type":"markdown","metadata":{"id":"78HE8FLsKN9Q"},"source":["# Gera os arquivos para o Embedding Projector das palavras do conjunto de dados CohQuAD CoIn pt-br com BERT Transformers by HuggingFace.\n","\n","Gera os arquivos para Embedding Projector(https://projector.tensorflow.org/).\n","\n","Pode ser configurado para utilizar o BERTimbau **Large** e **Base**.\n","\n","Gera arquivos **records_token.tsv** com:\n","- Com e sem pooling dos embeddings das palavras fora do vocabulário.\n","- Gera embeddings da concatenação das 4 últimas camadas do BERT ou da última camada.\n","\n","O arquivo **meta_token.tsv** possui as seguindas colunas:\n","- Token\n","- POS-Tag\n","- OOV (1 - Não existe no vocabulário do **BERT** e combina os *embeddings* dos tokens para formar a palavra e 0 - Existe no vocabulário do **BERT**)\n","- Id (Id do documento)\n","- Origem (Id do documento de origem)\n","- Classe (1 - Original e 0 - Perturbado)\n","- Perturbada (1 - Perturbada, 0 - Não perturbada)\n","- Index (Índice da palavra na sentença)\n","- Próximo token da sentença\n","- Sentença\n","\n","\n","Exemplo de projeção dos arquivos gerados:\n","https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/osmarbraz/cohebertv1projecao/main/config_token.json\n","\n","Repositório dos arquivos no github.\n","https://github.com/osmarbraz/cohebertv1projecao\n","\n","---------------------------\n","\n","Artigos:\n","\n","- https://arxiv.org/pdf/1611.05469v1.pdf\n","\n","- https://towardsdatascience.com/visualizing-bias-in-data-using-embedding-projector-649bc65e7487\n","\n","- https://towardsdatascience.com/bert-visualization-in-embedding-projector-dfe4c9e18ca9\n","\n","- https://krishansubudhi.github.io/deeplearning/2020/08/27/bert-embeddings-visualization.html\n","\n","- https://amitness.com/interactive-sentence-embeddings/\n","\n","---------------------------\n","\n","**Utiliza o *projeto embeddings* projector para exibir os dados:**\n","https://projector.tensorflow.org/\n","\n","\n","**Link biblioteca Huggingface:**\n","https://github.com/huggingface/transformers\n","\n","\n","**Artigo original BERT Jacob Devlin:**\n","https://arxiv.org/pdf/1506.06724.pdf"]},{"cell_type":"markdown","metadata":{"id":"xyxb5Px3p1-e"},"source":["# 1 Preparação do ambiente\n","Preparação do ambiente para execução do exemplo."]},{"cell_type":"markdown","metadata":{"id":"cW_5CN8En7zl"},"source":["## 1.1 Tempo inicial de processamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcTEKloUn-VK"},"outputs":[],"source":["# Import das bibliotecas\n","import time\n","import datetime\n","\n","#marca o tempo de início do processamento.\n","inicio_processamento = time.time()"]},{"cell_type":"markdown","metadata":{"id":"GOcN8hK-scnt"},"source":["## 1.2 Funções e classes auxiliares"]},{"cell_type":"markdown","metadata":{"id":"OPRnA-mk5-c4"},"source":["Verifica se existe o diretório cohebert no diretório corrente.   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fj5TaAH_5-nB"},"outputs":[],"source":["# Import das bibliotecas.\n","import os # Biblioteca para manipular arquivos\n","\n","# ============================\n","def verificaDiretorioCoheBERT():\n","    \"\"\"\n","      Verifica se existe o diretório cohebert no diretório corrente.\n","    \"\"\"\n","\n","    # Verifica se o diretório existe\n","    if not os.path.exists(DIRETORIO_COHEBERT):\n","        # Cria o diretório\n","        os.makedirs(DIRETORIO_COHEBERT)\n","        logging.info(\"Diretório Cohebert criado: {}\".format(DIRETORIO_COHEBERT))\n","\n","    return DIRETORIO_COHEBERT"]},{"cell_type":"markdown","metadata":{"id":"yDCOeh2y5jOH"},"source":["Realiza o download e um arquivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5B1mvfAU5jZf"},"outputs":[],"source":["# Import das bibliotecas.\n","import requests # Biblioteca de download\n","from tqdm.notebook import tqdm as tqdm_notebook # Biblioteca para barra de progresso\n","import os # Biblioteca para manipular arquivos\n","\n","def downloadArquivo(url_arquivo, nome_arquivo_destino):\n","    \"\"\"\n","      Realiza o download de um arquivo de uma url em salva em nome_arquivo_destino.\n","\n","      Parâmetros:\n","        `url_arquivo` - URL do arquivo a ser feito download.\n","        `nome_arquivo_destino` - Nome do arquivo a ser salvo.\n","    \"\"\"\n","\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Realiza o download de um arquivo em uma url\n","    data = requests.get(url_arquivo, stream=True)\n","\n","    # Verifica se o arquivo existe\n","    if data.status_code != 200:\n","        logging.info(\"Exceção ao tentar realizar download {}. Response {}.\".format(url_arquivo, data.status_code))\n","        data.raise_for_status()\n","        return\n","\n","    # Recupera o nome do arquivo a ser realizado o download\n","    nome_arquivo = nome_arquivo_destino.split(\"/\")[-1]\n","\n","    # Define o nome e caminho do arquivo temporário\n","    nome_arquivo_temporario = DIRETORIO_COHEBERT + \"/\" + nome_arquivo + \"_part\"\n","\n","    logging.info(\"Download do arquivo: {}.\".format(nome_arquivo_destino))\n","\n","    # Baixa o arquivo\n","    with open(nome_arquivo_temporario, \"wb\") as arquivo_binario:\n","        tamanho_conteudo = data.headers.get(\"Content-Length\")\n","        total = int(tamanho_conteudo) if tamanho_conteudo is not None else None\n","        # Barra de progresso de download\n","        progresso_bar = tqdm_notebook(unit=\"B\", total=total, unit_scale=True)\n","        # Atualiza a barra de progresso\n","        for chunk in data.iter_content(chunk_size=1024):\n","            if chunk:\n","                progresso_bar.update(len(chunk))\n","                arquivo_binario.write(chunk)\n","\n","    # Renomeia o arquivo temporário para o arquivo definitivo\n","    os.rename(nome_arquivo_temporario, nome_arquivo_destino)\n","\n","    # Fecha a barra de progresso.\n","    progresso_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"ksYnRk7zLGp0"},"source":["Remove tags de um documento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qwKjGvyLG4v"},"outputs":[],"source":["def remove_tags(documento):\n","    \"\"\"\n","      Remove tags de um documento\n","    \"\"\"\n","\n","    import re\n","\n","    documento_limpo = re.compile(\"<.*?>\")\n","    return re.sub(documento_limpo, \"\", documento)"]},{"cell_type":"markdown","metadata":{"id":"4pduTsINLeaz"},"source":["Funções auxiliares de arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jirIzIstLea0"},"outputs":[],"source":["def carregar(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como um único parágrafo(texto).\n","\n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.\n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","\n","    paragrafo = \"\"\n","    for linha in arquivo:\n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        # Remove as tags existentes no final das linhas\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          paragrafo = paragrafo + linha.strip() + \" \"\n","\n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    # Remove os espaços em branco antes e depois do parágrafo\n","    return paragrafo.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC9Xppq-_R0w"},"outputs":[],"source":["def carregarLista(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como uma lista de sentenças(texto).\n","\n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.\n","        `encoding` - Codificação dos caracteres do arquivo.\n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","\n","    sentencas = []\n","    for linha in arquivo:\n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          sentencas.append(linha.strip())\n","\n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    return sentencas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkVk5LQT_G3f"},"outputs":[],"source":["def salvar(nome_arquivo, texto):\n","    \"\"\"\n","      Salva um texto em arquivo.\n","\n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser salvo.\n","        `texto` - Texto a ser salvo.\n","    \"\"\"\n","\n","    arquivo = open(nome_arquivo, \"w\")\n","    arquivo.write(str(texto))\n","    arquivo.close()"]},{"cell_type":"markdown","metadata":{"id":"603LYIYKBmq5"},"source":["Função auxiliar para formatar o tempo como `hh: mm: ss`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guy6B4whsZFR"},"outputs":[],"source":["# Import das bibliotecas.\n","import time\n","import datetime\n","\n","def formataTempo(tempo):\n","    \"\"\"\n","      Pega a tempo em segundos e retorna uma string hh:mm:ss\n","    \"\"\"\n","    # Arredonda para o segundo mais próximo.\n","    tempo_arredondado = int(round((tempo)))\n","\n","    # Formata como hh:mm:ss\n","    return str(datetime.timedelta(seconds=tempo_arredondado))"]},{"cell_type":"markdown","metadata":{"id":"zVKAapz7RCxk"},"source":["Classe(ModeloArgumentosMedida) de definição dos parâmetros do modelo para medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgmN6RqDRDZS"},"outputs":[],"source":["# Import das bibliotecas.\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","from typing import List\n","\n","@dataclass\n","class ModeloArgumentosMedida:\n","    max_seq_len: Optional[int] = field(\n","        default=None,\n","        metadata={'help': 'max seq len'},\n","    )\n","    pretrained_model_name_or_path: str = field(\n","        default='neuralmind/bert-base-portuguese-cased',\n","        metadata={'help': 'nome do modelo pré-treinado do BERT.'},\n","    )\n","    modelo_spacy: str = field(\n","        default=\"pt_core_news_lg\",\n","        metadata={\"help\": \"nome do modelo do spaCy.\"},\n","    )\n","    versao_modelo_spacy: str = field(\n","        default=\"-3.2.0\",\n","        metadata={\"help\": \"versão do nome do modelo no spaCy.\"},\n","    )\n","    do_lower_case: bool = field(\n","        default=False,\n","        metadata={'help': 'define se o texto do modelo deve ser todo em minúsculo.'},\n","    )\n","    output_attentions: bool = field(\n","        default=False,\n","        metadata={'help': 'habilita se o modelo retorna os pesos de atenção.'},\n","    )\n","    output_hidden_states: bool = field(\n","        default=False,\n","        metadata={'help': 'habilita gerar as camadas ocultas do modelo.'},\n","    )\n","    use_wandb : bool = field(\n","        default=True,\n","        metadata={'help': 'habilita o uso do wandb.'},\n","    )\n","    salvar_avaliacao : bool = field(\n","        default=True,\n","        metadata={'help': 'habilita o salvamento do resultado da avaliação.'},\n","    )\n","    salvar_medicao : bool = field(\n","        default=False,\n","        metadata={'help': 'habilita o salvamento da medicao.'},\n","    )\n","    usar_mcl_ajustado : bool = field(\n","        default=False,\n","        metadata={'help': 'habilita o carragamento de mcl ajustado.'},\n","    )\n","    documentos_perturbados: int = field(\n","        default=\"1\",\n","        metadata={\"help\": \"Quantidade de documentos a serem perturbados a partir do original.\"},\n","    )\n","    top_k_predicao: int = field(\n","        default=\"100\",\n","        metadata={\"help\": \"Quantidade de palavras a serem recuperadas mais próximas da máscara.\"},\n","    )\n","    estrategia_medida: int = field(\n","        default=0, # 0 - MEAN estratégia média / 1 - MAX  estratégia maior\n","        metadata={'help': 'Estratégia de cálculo da médida dos embeddings.'},\n","    )\n","    equacao_medida: int = field(\n","        default=0, # 0 - ADJACENTE / 1 - COMBINAÇÃO TODAS / 2 - CONTEXTO\n","        metadata={'help': 'Equação de cálculo da coerência.'},\n","    )\n","    filtro_palavra: int = field(\n","        default=0, # 0 - Considera todas as palavras das sentenças / 1 - Desconsidera as stopwords / 2 - Considera somente as palavras substantivas\n","        metadata={'help': 'Define o filtro de palavras das sentenças para gerar os embeddings.'},\n","    )"]},{"cell_type":"markdown","metadata":{"id":"HIN413rj50EI"},"source":["Biblioteca de limpeza de tela\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxV4-3Yg50EI"},"outputs":[],"source":["# Import das bibliotecas.\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"iAPVtRXQqDim"},"source":["## 1.3 Tratamento de logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcopxbGZqDip"},"outputs":[],"source":["# Import das bibliotecas.\n","import logging # Biblioteca de logging\n","\n","# Formatando a mensagem de logging\n","logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"_GjYtXcMnSAe"},"source":["## 1.4  Identificando o ambiente Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMiH0E3OnRa1"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory.\n","import sys\n","\n","# Retorna true ou false se estiver no Google Colaboratory.\n","IN_COLAB = 'google.colab' in sys.modules"]},{"cell_type":"markdown","metadata":{"id":"RinFHFesVKis"},"source":["## 1.5 Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"MPngEboiVbfi"},"source":["Usando Colab GPU para Treinamento\n"]},{"cell_type":"markdown","metadata":{"id":"EjWE6WlvVbfj"},"source":["Uma GPU pode ser adicionada acessando o menu e selecionando:\n","\n","`Edit -> Notebook Settings -> Hardware accelerator -> (GPU)`\n","\n","Em seguida, execute a célula a seguir para confirmar que a GPU foi detectada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtaYZmc3Vbfj"},"outputs":[],"source":["# Import das bibliotecas.\n","import tensorflow as tf\n","\n","# Recupera o nome do dispositido da GPU.\n","device_name = tf.test.gpu_device_name()\n","\n","# O nome do dispositivo deve ser parecido com o seguinte:\n","if device_name == \"/device:GPU:0\":\n","    logging.info(\"Encontrei GPU em: {}\".format(device_name))\n","else:\n","    logging.info(\"Dispositivo GPU não encontrado\")\n","    #raise SystemError(\"Dispositivo GPU não encontrado\")"]},{"cell_type":"markdown","metadata":{"id":"iYRrUo2XWa8G"},"source":["Nome da GPU\n","\n","Para que a torch use a GPU, precisamos identificar e especificar a GPU como o dispositivo. Posteriormente, em nosso ciclo de treinamento, carregaremos dados no dispositivo.\n","\n","Vale a pena observar qual GPU você recebeu. A GPU Tesla P100 é muito mais rápido que as outras GPUs, abaixo uma lista ordenada:\n","- 1o Tesla P100\n","- 2o Tesla T4\n","- 3o Tesla P4 (Não tem memória para execução 4 x 8, somente 2 x 4)\n","- 4o Tesla K80 (Não tem memória para execução 4 x 8, somente 2 x 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrjqDO6nWa8J"},"outputs":[],"source":["# Import das bibliotecas.\n","import torch\n","\n","def getDeviceGPU():\n","    \"\"\"\n","      Retorna um dispositivo de GPU se disponível ou CPU.\n","\n","      Retorno:\n","        `device` - Um device de GPU ou CPU.\n","    \"\"\"\n","\n","    # Se existe GPU disponível.\n","    if torch.cuda.is_available():\n","\n","        # Diz ao PyTorch para usar GPU.\n","        device = torch.device(\"cuda\")\n","\n","        logging.info(\"Existem {} GPU(s) disponíveis.\".format(torch.cuda.device_count()))\n","        logging.info(\"Iremos usar a GPU: {}.\".format(torch.cuda.get_device_name(0)))\n","\n","    # Se não.\n","    else:\n","        logging.info(\"Sem GPU disponível, usando CPU.\")\n","        device = torch.device(\"cpu\")\n","\n","    return device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChDxmtXsKwjf"},"outputs":[],"source":["# Recupera o device com GPU ou CPU\n","device = getDeviceGPU()"]},{"cell_type":"markdown","metadata":{"id":"fGf59D0yVNx9"},"source":["Memória\n","\n","Memória disponível no ambiente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iC5-pSAVh7_"},"outputs":[],"source":["# Importando as bibliotecas.\n","from psutil import virtual_memory\n","\n","ram_gb = virtual_memory().total / 1e9\n","logging.info(\"Seu ambiente de execução tem {: .1f} gigabytes de RAM disponível\\n\".format(ram_gb))\n","\n","if ram_gb < 20:\n","  logging.info(\"Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \\\"Alterar tipo de tempo de execução\\\"\")\n","  logging.info(\"e selecione High-RAM. Então, execute novamente está célula\")\n","else:\n","  logging.info(\"Você está usando um ambiente de execução de memória RAM alta!\")"]},{"cell_type":"markdown","metadata":{"id":"wijMXooQQLcQ"},"source":["## 1.6 Monta uma pasta no google drive para carregar os arquivos de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysnDDapMQK8K"},"outputs":[],"source":["# import necessário\n","from google.colab import drive\n","\n","# Monta o drive na pasta especificada\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"u66iRrtwMrqy"},"source":["## 1.7 Instalação do wandb"]},{"cell_type":"markdown","metadata":{"id":"dQd3BrhvMzZs"},"source":["Instalação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejzpgGrFM0-j"},"outputs":[],"source":["!pip install --upgrade wandb"]},{"cell_type":"markdown","metadata":{"id":"oOd2MbBiDq93"},"source":["## 1.8 Instalação do spaCy\n","\n","https://spacy.io/\n","\n","Modelos do spaCy para português:\n","https://spacy.io/models/pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaMM4WdxgvQ7"},"outputs":[],"source":["# Instala o spacy\n","!pip install -U pip setuptools wheel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4p3Rz2qDq94"},"outputs":[],"source":["# Instala uma versão específica\n","!pip install -U spacy==3.2.0"]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["## 1.9 Instalação do BERT"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["Instala a interface pytorch para o BERT by Hugging Face.\n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCVzCmy7pIOz"},"outputs":[],"source":["!pip install -U transformers==4.5.1"]},{"cell_type":"markdown","metadata":{"id":"giOsAS5v61go"},"source":["# 2 Parametrização"]},{"cell_type":"markdown","metadata":{"id":"ifrYNTwGwKal"},"source":["## Gerais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uiH9pNpwI6g"},"outputs":[],"source":["#Definição dos parâmetros a serem avaliados\n","#Quantidade de documentos a serem perturbados a partir do original.\n","DOCUMENTOS_PERTURBADOS = 20\n","\n","#Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","TOP_K_PREDICAO = 20\n","\n","#Realiza o pooling dos tokens de palavras fora do vocabulário do BERT\n","POOLING_TOKENS = 1 # 0 - Sem pooling / 1 - Com pooling\n","\n","#Utiliza somente documentos originais, perturbados ou ambos.\n","CLASSE_DOCUMENTO = 2 # 0 - Somente com a classe 0 (Perturbado) / 1 - Somente com classe 1 (Original), 2 - As duas classes 0 e 1\n","\n","#Estratégia de recuperação dos embeddings: (1 - Embeddings da última camada,\n","#                                           2 - Embeddings da concatenação das 4 últimas camadas)\n","ESTRATEGIA_EMBEDDING = 2\n","\n","# Estratégias a serem avaliadas (0 - Mean / 1 - Max) para as palavras formadas por mais de um token do BERT\n","ESTRATEGIA_MEDIDA_STR = [\"MEAN\", \"MAX\"]\n","ESTRATEGIA_MEDIDA = [0, 1]\n","\n","# Habilita a criação do rótulo \"__next__\" no projetor para gerar linhas entre os pontos de tokens de uma mesma origem em sequência.\n","LIGACAO_PROXIMO_TOKEN = True"]},{"cell_type":"markdown","source":["Permite filtrar os documentos a serem utilizados na geração dos arquivos da projeção."],"metadata":{"id":"BkttC-OgqSDP"}},{"cell_type":"code","source":["# Filtra um determinado conjunto de documentos originais e suas versões perturbadas\n","# FILTRO_DO = [] # Filtro vazio seleciona todos os documentos\n","FILTRO_DO = ['1']"],"metadata":{"id":"D40avaBwqQfq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mhByVujAwNAU"},"source":["## Específicos"]},{"cell_type":"markdown","metadata":{"id":"3V_ORR8Qyu1p"},"source":["Parâmetros do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ15-ylRRRdD"},"outputs":[],"source":["# Definição dos parâmetros do Modelo\n","model_args = ModeloArgumentosMedida(\n","    max_seq_len = 512,\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\",\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\",\n","\n","    #pretrained_model_name_or_path = \"bert-large-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-cased\"\n","    pretrained_model_name_or_path = \"neuralmind/bert-large-portuguese-cased\",\n","    #pretrained_model_name_or_path = \"neuralmind/bert-base-portuguese-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-multilingual-uncased\",\n","\n","    #modelo_spacy = \"en_core_web_lg\",\n","    #modelo_spacy = \"en_core_web_md\",\n","    #modelo_spacy = \"en_core_web_sm\",\n","    modelo_spacy = \"pt_core_news_lg\",\n","    #modelo_spacy = \"pt_core_news_md\",\n","    #modelo_spacy = \"pt_core_news_sm\",\n","\n","    versao_modelo_spacy = \"3.2.0\",\n","    do_lower_case = False,  # default True\n","    output_attentions = False,  # default False\n","    output_hidden_states = True, # default False\n","    use_wandb = True,\n","    salvar_medicao = True, #Salva o resultado da medição\n","    salvar_avaliacao = True, # Salva o resultado da avaliação das medições\n","    documentos_perturbados = DOCUMENTOS_PERTURBADOS, # Quantidade de documentos a serem perturbados a partir do original.\n","    top_k_predicao = TOP_K_PREDICAO, # Conjunto de valores: 1, 10, 100, 500 e 1000. Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","    usar_mcl_ajustado = False, # Especifica se deve ser carregado um MCL ajustado ou pré-treinado. Necessário especificar o tipo do modelo em pretrained_model_name_or_path.\n","    estrategia_medida = 0, # Atributo usado para os logs do wandb. 0 - MEAN estratégia média / 1 - MAX  estratégia maior\n","    equacao_medida = 0, # Atributo usado para os logs do wandb. 0 - Palavras adjacentes / 1 - Todas as palavras / 2 - Palavra e contexto\n","    filtro_palavra = 0 # # Atributo usado para os logs do wandb. 0 - Considera todas as palavras das sentenças / 1 - Desconsidera as stopwords / 2 - Considera somente as palavras substantivas\n",")"]},{"cell_type":"markdown","metadata":{"id":"WlF4PKP6Iopi"},"source":["## Nome do diretório dos arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55PNP2s6Iopi"},"outputs":[],"source":["# Diretório do cohebert\n","DIRETORIO_COHEBERT = \"COHQUAD_COIN_PTBR\""]},{"cell_type":"markdown","metadata":{"id":"SUxlx7Sx4yxj"},"source":["## Define o caminho para os arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gQpxAO74yxj"},"outputs":[],"source":["# Diretório local para os arquivos pré-processados\n","DIRETORIO_LOCAL = \"/content/\" + DIRETORIO_COHEBERT + \"/\"\n","\n","# Diretório no google drive com os arquivos pré-processados\n","DIRETORIO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/\""]},{"cell_type":"markdown","metadata":{"id":"tDgJTbPOZ8SW"},"source":["## Inicialização diretórios"]},{"cell_type":"markdown","metadata":{"id":"qpSERA9TC4WU"},"source":["Diretório base local"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edg7eW2cDflg"},"outputs":[],"source":["# Importando as bibliotecas.\n","import os\n","\n","def criaDiretorioLocal():\n","\n","  # Cria o diretório para receber os arquivos Originais e Permutados\n","  # Diretório a ser criado\n","  dirbase = DIRETORIO_LOCAL[:-1]\n","\n","  if not os.path.exists(dirbase):\n","      # Cria o diretório\n","      os.makedirs(dirbase)\n","      logging.info(\"Diretório criado: {}.\".format(dirbase))\n","  else:\n","      logging.info(\"Diretório já existe: {}.\".format(dirbase))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xge0ar9MJoKy"},"outputs":[],"source":["criaDiretorioLocal()"]},{"cell_type":"markdown","metadata":{"id":"4FmT9nhbaE3D"},"source":["Diretório para conter as os resultados das medidas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO76uzj_C3zQ"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioMedidacao():\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):\n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1Ot2h_bJuxy"},"outputs":[],"source":["criaDiretorioMedidacao()"]},{"cell_type":"markdown","metadata":{"id":"vIkT6ksqaQs3"},"source":["Diretório para conter os arquivos da avaliação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIV4xj6zDnb8"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioAvaliacao():\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra/Avaliacao\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):\n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiOVjJ5BJzE1"},"outputs":[],"source":["criaDiretorioAvaliacao()"]},{"cell_type":"markdown","metadata":{"id":"cjP6v878aWR7"},"source":["Diretório para conter os arquivos das medidas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf6UWAZYDsgm"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioMedicao():\n","\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra/Medicao\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):\n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBBfHFuPJ3NM"},"outputs":[],"source":["criaDiretorioMedicao()"]},{"cell_type":"markdown","metadata":{"id":"L7G3-MOsQ1N_"},"source":["# 3 spaCy"]},{"cell_type":"markdown","metadata":{"id":"35GwcgkOlWi3"},"source":["## 3.1 Download arquivo modelo\n","\n","https://spacy.io/models/pt"]},{"cell_type":"markdown","metadata":{"id":"PWd_9X0nOYnF"},"source":["### Função download modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjWGu-9D5URZ"},"outputs":[],"source":["def downloadSpacy(model_args):\n","    \"\"\"\n","      Realiza o download do arquivo do modelo para o diretório corrente.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \"\"\"\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Nome arquivo compactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","\n","    # Url do arquivo\n","    URL_ARQUIVO_MODELO_COMPACTADO = \"https://github.com/explosion/spacy-models/releases/download/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO\n","\n","    # Realiza o download do arquivo do modelo\n","    logging.info(\"Download do arquivo do modelo do spaCy.\")\n","    downloadArquivo(URL_ARQUIVO_MODELO_COMPACTADO, DIRETORIO_COHEBERT + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"Uu_LkF7Nfm8_"},"source":["## 3.2 Descompacta o arquivo do modelo"]},{"cell_type":"markdown","metadata":{"id":"XAc1tSwvOc4d"},"source":["### Função descompacta modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq9PnXO77bPQ"},"outputs":[],"source":["# Import das bibliotecas.\n","import tarfile # Biblioteca de descompactação\n","\n","def descompactaSpacy(model_args):\n","    \"\"\"\n","      Descompacta o arquivo do modelo.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \"\"\"\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","\n","    # Nome do arquivo a ser descompactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","\n","    logging.info(\"Descompactando o arquivo do modelo do spaCy.\")\n","    arquivo_tar = tarfile.open(NOME_ARQUIVO_MODELO_COMPACTADO, \"r:gz\")\n","    arquivo_tar.extractall(DIRETORIO_COHEBERT)\n","    arquivo_tar.close()\n","\n","    # Apaga o arquivo compactado\n","    if os.path.isfile(NOME_ARQUIVO_MODELO_COMPACTADO):\n","        os.remove(NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"STHT2c89qvwK"},"source":["## 3.3 Carrega o modelo"]},{"cell_type":"markdown","metadata":{"id":"3iFBoyWMOgKz"},"source":["### Função carrega modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePOccj0s8WMg"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy # Biblioteca do spaCy\n","\n","def carregaSpacy(model_args):\n","    \"\"\"\n","    Realiza o carregamento do Spacy.\n","\n","    Parâmetros:\n","      `model_args` - Objeto com os argumentos do modelo.\n","    \"\"\"\n","\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Caminho raoz do modelo do spaCy\n","    DIRETORIO_MODELO_SPACY =  DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY\n","\n","    # Verifica se o diretório existe\n","    if os.path.exists(DIRETORIO_MODELO_SPACY) == False:\n","        # Realiza o download do arquivo modelo do spaCy\n","        downloadSpacy(model_args)\n","        # Descompacta o spaCy\n","        descompactaSpacy(model_args)\n","\n","    # Diretório completo do spaCy\n","    DIRETORIO_MODELO_SPACY = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\"\n","\n","    # Carrega o spaCy. Necessário somente \"tagger\" para encontrar os substantivos\n","    nlp = spacy.load(DIRETORIO_MODELO_SPACY)\n","    logging.info(\"spaCy carregado.\")\n","\n","    # Retorna o spacy carregado\n","    return nlp"]},{"cell_type":"markdown","metadata":{"id":"cAk5hHx7OnHn"},"source":["### Carrega o modelo spaCy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbELnrpgA4T1"},"outputs":[],"source":["# Carrega o modelo spaCy\n","nlp = carregaSpacy(model_args)"]},{"cell_type":"markdown","metadata":{"id":"fzk8VOp7oy8n"},"source":["## 3.4 Funções auxiliares spaCy"]},{"cell_type":"markdown","metadata":{"id":"AEzytjZi5Iw2"},"source":["### getStopwords\n","\n","Recupera as stopwords do spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKg-_XyWoy8o"},"outputs":[],"source":["def getStopwords(nlp):\n","    \"\"\"\n","      Recupera as stop words do nlp(Spacy).\n","\n","      Parâmetros:\n","        `nlp` - Um modelo spaCy carregado.\n","    \"\"\"\n","\n","    spacy_stopwords = nlp.Defaults.stop_words\n","\n","    return spacy_stopwords"]},{"cell_type":"markdown","metadata":{"id":"qZdNFrC3oy8p"},"source":["Lista dos stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1o8jevtoy8p"},"outputs":[],"source":["logging.info(\"Quantidade de stopwords: {}.\".format(len(getStopwords(nlp))))\n","\n","print(getStopwords(nlp))"]},{"cell_type":"markdown","metadata":{"id":"onM1ZApom-_W"},"source":["### getVerbos\n","Localiza os verbos da sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hdqVdfxm-_W"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy\n","from spacy.util import filter_spans\n","from spacy.matcher import Matcher\n","\n","# (verbo normal como auxilar ou auxilar) + vários verbos auxiliares +verbo principal ou verbo auxiliar\n","gramaticav1 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"aux\",\"aux:pass\"]}},  #verbo auxiliar\n","                {\"POS\": \"VERB\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"ROOT\",\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo normal como auxiliar\n","                {\"POS\": \"AUX\", \"OP\": \"*\", \"DEP\": {\"IN\": [\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo auxiliar\n","                {\"POS\": \"VERB\", \"OP\": \"+\"}, #verbo principal\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"cop\",\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo auxiliar\n","               ]\n","\n","# verbo auxiliar + verbo normal como auxiliar + conjunção com preposição + verbo\n","gramaticav2 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"aux\",\"aux:pass\"]}},  #verbo auxiliar\n","                {\"POS\": \"VERB\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"ROOT\"]}},  #verbo principal\n","                {\"POS\": \"SCONJ\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"mark\"]}}, #conjunção com preposição\n","                {\"POS\": \"VERB\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"xcomp\"]}}, #verbo normal como complementar\n","               ]\n","\n","#Somente verbos auxiliares\n","gramaticav3 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\"},  #Verbos auxiliar\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"cop\"]}},  #Verbos auxiliar de ligação (AUX+(cop))\n","                {\"POS\": \"ADJ\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"ROOT\"]}},\n","                {\"POS\": \"AUX\", \"OP\": \"?\"}  #Verbos auxiliar\n","               ]\n","\n","matcherv = Matcher(nlp.vocab)\n","\n","matcherv.add(\"frase verbal\", [gramaticav1])\n","matcherv.add(\"frase verbal\", [gramaticav2])\n","matcherv.add(\"frase verbal\", [gramaticav3])\n","\n","#Retorna a Frase Verbal\n","def getVerbos(periodo):\n","  #Processa o período\n","  doc1 = nlp(periodo.text)\n","\n","  # Chama o mather para encontrar o padrão\n","  matches = matcherv(doc1)\n","\n","  padrao = [doc1[start:end] for _, start, end in matches]\n","\n","  #elimina as repetições e sobreposições\n","  #return filter_spans(padrao)\n","  lista1 = filter_spans(padrao)\n","\n","  # Converte os itens em string\n","  lista2 = []\n","  for x in lista1:\n","      lista2.append(str(x))\n","\n","  return lista2"]},{"cell_type":"markdown","metadata":{"id":"6ZVwbmn3Nx2t"},"source":["### getDicPOSQtde\n","\n","Conta as POS Tagging de uma sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3j3VF4NOSPbq"},"outputs":[],"source":["def getDicPOSQtde(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Retorna inteiros que mapeiam para classes gramaticais\n","  conta_dicionarios = doc.count_by(spacy.attrs.IDS[\"POS\"])\n","\n","  # Dicionário com as tags e quantidades\n","  novodic = dict()\n","\n","  for pos, qtde in conta_dicionarios.items():\n","    classe_gramatical = doc.vocab[pos].text\n","    novodic[classe_gramatical] = qtde\n","\n","  return novodic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uPDYU4KBC5q"},"outputs":[],"source":["def getDicTodasPOSQtde(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Retorna inteiros que mapeiam para classes gramaticais\n","  conta_dicionarios = doc.count_by(spacy.attrs.IDS[\"POS\"])\n","\n","  # Dicionário com as tags e quantidades\n","  novodic = {\"PRON\":0, \"VERB\":0, \"PUNCT\":0, \"DET\":0, \"NOUN\":0, \"AUX\":0, \"CCONJ\":0, \"ADP\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"NUM\":0, \"SCONJ\":0, \"SYM\":0, \"SPACE\":0, \"INTJ\":0, \"X\": 0}\n","\n","  for pos, qtde in conta_dicionarios.items():\n","    classe_gramatical = doc.vocab[pos].text\n","    novodic[classe_gramatical] = qtde\n","\n","  return novodic"]},{"cell_type":"markdown","metadata":{"id":"Jxe-mh-l6sJY"},"source":["### getDicTodasPOSQtde\n","\n","Conta as POS Tagging de uma sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9SA61kD6sJY"},"outputs":[],"source":["def getDicTodasPOSQtde(lista):\n","\n","  # Dicionário com as tags e quantidades\n","  conjunto = {\"PRON\":0, \"VERB\":0, \"PUNCT\":0, \"DET\":0, \"NOUN\":0, \"AUX\":0, \"CCONJ\":0, \"ADP\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"NUM\":0, \"SCONJ\":0, \"SYM\":0, \"SPACE\":0, \"INTJ\": 0}\n","\n","  for x in lista:\n","    valor = conjunto.get(x)\n","    if valor != None:\n","      conjunto[x] = valor + 1\n","    else:\n","      conjunto[x] = 1\n","\n","  return conjunto"]},{"cell_type":"markdown","metadata":{"id":"m4KV_jI-Nx2w"},"source":["### getSomaDic\n","\n","Soma os valores de dicionários com as mesmas chaves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGduPM6HNx2w"},"outputs":[],"source":["from collections import Counter\n","from functools import reduce\n","\n","def atualizaValor(a,b):\n","    a.update(b)\n","    return a\n","\n","def getSomaDic(lista):\n","\n","  # Soma os dicionários da lista\n","  novodic = reduce(atualizaValor, (Counter(dict(x)) for x in lista))\n","\n","  return novodic"]},{"cell_type":"markdown","metadata":{"id":"bGaf7bkpAEiX"},"source":["### getTokensSentenca\n","\n","Retorna a lista de tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWxyAo54AOHU"},"outputs":[],"source":["def getTokensSentenca(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença adicionando os tokens\n","  for token in doc:\n","    lista.append(token.text)\n","\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"ZB6bR42PA28c"},"source":["### getPOSTokensSentenca\n","\n","Retorna a lista das POS-Tagging dos tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awaqjNIZA3Fk"},"outputs":[],"source":["def getPOSTokensSentenca(sentenca):\n","\n","  # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença adicionando os tokens\n","  for token in doc:\n","    lista.append(token.pos_)\n","\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"B4Soqt3fp3Lu"},"source":["### getListaTokensPOSSentenca\n","\n","Retorna duas listas uma com os tokens e a outra com a POS-Tagging dos tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvd99wd_pwmt"},"outputs":[],"source":["def getListaTokensPOSSentenca(sentenca):\n","  # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista_tokens = []\n","  lista_pos = []\n","\n","  # Percorre a sentença adicionando os tokens e as POS\n","  for token in doc:\n","    lista_tokens.append(token.text)\n","    lista_pos.append(token.pos_)\n","\n","  return lista_tokens, lista_pos"]},{"cell_type":"markdown","metadata":{"id":"ENvsIER06sJX"},"source":["### Tadução das tags"]},{"cell_type":"markdown","metadata":{"id":"kwSb3ECU6sJY"},"source":["Tags de palavras universal\n","\n","https://universaldependencies.org/u/pos/\n","\n","Detalhes das tags em português:\n","http://www.dbd.puc-rio.br/pergamum/tesesabertas/1412298_2016_completo.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpCUpOs06sJY"},"outputs":[],"source":["#dicionário que contêm pos tag universal e suas explicações\n","palavra_universal_dict = {\n","  \"X\"    : \"Outro\",\n","  \"VERB\" : \"Verbo \",\n","  \"SYM\"  : \"Símbolo\",\n","  \"CONJ\" : \"Conjunção\",\n","  \"SCONJ\": \"Conjunção subordinativa\",\n","  \"PUNCT\": \"Pontuação\",\n","  \"PROPN\": \"Nome próprio\",\n","  \"PRON\" : \"Pronome substativo\",\n","  \"PART\" : \"Partícula, morfemas livres\",\n","  \"NUM\"  : \"Numeral\",\n","  \"NOUN\" : \"Substantivo\",\n","  \"INTJ\" : \"Interjeição\",\n","  \"DET\"  : \"Determinante, Artigo e pronomes adjetivos\",\n","  \"CCONJ\": \"Conjunção coordenativa\",\n","  \"AUX\"  : \"Verbo auxiliar\",\n","  \"ADV\"  : \"Advérbio\",\n","  \"ADP\"  : \"Preposição\",\n","  \"ADJ\"  : \"Adjetivo\"\n","}\n","\n","#Explica a POS\n","def getPOSPalavraUniversalTraduzido(palavra):\n","  if palavra in palavra_universal_dict.keys():\n","      traduzido = palavra_universal_dict[palavra]\n","  else:\n","      traduzido = \"NA\"\n","  return traduzido"]},{"cell_type":"markdown","metadata":{"id":"b01WgMSSKY_u"},"source":["### getSentencaSemStopWord\n","\n","Retorna uma lista dos tokens sem as stopwords."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMb0uDWzKZXP"},"outputs":[],"source":["def getSentencaSemStopWord(sentenca, stopwords):\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre os tokens da sentença\n","  for i, token in enumerate(sentenca):\n","\n","    # Verifica se o token é uma stopword\n","    if token.lower() not in stopwords:\n","      lista.append(token)\n","\n","  # Retorna o documento\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"TouR4GjNJZD6"},"source":["### getSentencaSalientePOS\n","\n","Retorna uma lista das palavras do tipo especificado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxTCYFzcJZD6"},"outputs":[],"source":["def getSentencaSalientePOS(sentenca, pos, tipo_saliente=\"NOUN\"):\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença\n","  for i, token in enumerate(sentenca):\n","\n","    # Verifica se o token é do tipo especeficado\n","    if pos[i] == tipo_saliente:\n","      lista.append(token)\n","\n","  # Retorna o documento\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"_xaeX0oTVQ5t"},"source":["###removeStopWords\n","\n","Remove as stopwords de um documento ou senteça."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIaQ9bzBVQ5t"},"outputs":[],"source":["def removeStopWord(documento, stopwords):\n","\n","  # Remoção das stopwords do documento\n","  documento_sem_stopwords = [palavra for palavra in documento.split() if palavra.lower() not in stopwords]\n","\n","  # Concatena o documento sem os stopwords\n","  documento_limpo = \" \".join(documento_sem_stopwords)\n","\n","  # Retorna o documento\n","  return documento_limpo"]},{"cell_type":"markdown","metadata":{"id":"A7NAe8ogCf1y"},"source":["### retornaRelevante\n","\n","Retorna somente os palavras do documento ou sentença do tipo especificado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNNfykypChn-"},"outputs":[],"source":["def retornaRelevante(documento, classe_relevante=\"NOUN\"):\n","\n","  # Corrigir!\n","  # Utilizar o documento já tokenizado pelo spacy!!!!\n","  # Existe uma lista com o documento e a sentença tokenizada pelo spacy\n","\n","  # Realiza o parsing no spacy\n","  doc = nlp(documento)\n","\n","  # Retorna a lista das palavras relevantes\n","  documento_com_substantivos = []\n","  for token in doc:\n","    #print(\"token:\", token.pos_)\n","    if token.pos_ == classe_relevante:\n","      documento_com_substantivos.append(token.text)\n","\n","  # Concatena o documento com os substantivos\n","  documento_concatenado = \" \".join(documento_com_substantivos)\n","\n","  # Retorna o documento\n","  return documento_concatenado"]},{"cell_type":"markdown","metadata":{"id":"IBY7q_uH8JSE"},"source":["# 4 BERT"]},{"cell_type":"markdown","metadata":{"id":"MBGTMy8Ic7GK"},"source":["## 4.1 Modelo Pré-treinado BERT"]},{"cell_type":"markdown","metadata":{"id":"uiuxdXe9t1BX"},"source":["### Funções Auxiliares"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Huw0x5kt1Le"},"outputs":[],"source":["def getNomeModeloBERT(model_args):\n","    '''\n","    Recupera uma string com uma descrição do modelo BERT para nomes de arquivos e diretórios.\n","\n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","\n","    Retorno:\n","    `MODELO_BERT` - Nome do modelo BERT.\n","    '''\n","\n","    # Verifica o nome do modelo(default SEM_MODELO_BERT)\n","    MODELO_BERT = \"SEM_MODELO_BERT\"\n","\n","    if 'neuralmind' in model_args.pretrained_model_name_or_path:\n","        MODELO_BERT = \"_BERTimbau\"\n","\n","    else:\n","        if 'multilingual' in model_args.pretrained_model_name_or_path:\n","            MODELO_BERT = \"_BERTmultilingual\"\n","\n","    return MODELO_BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYJB4ik7t5xe"},"outputs":[],"source":["def getTamanhoBERT(model_args):\n","    '''\n","    Recupera uma string com o tamanho(dimensão) do modelo BERT para nomes de arquivos e diretórios.\n","\n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","\n","    Retorno:\n","    `TAMANHO_BERT` - Nome do tamanho do modelo BERT.\n","    '''\n","\n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = \"_large\"\n","\n","    if 'base' in model_args.pretrained_model_name_or_path:\n","        TAMANHO_BERT = \"_base\"\n","\n","    return TAMANHO_BERT"]},{"cell_type":"markdown","metadata":{"id":"rHt4e5pAcEMd"},"source":["### Função download Modelo Pre-treinado BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peDUrV2ccEXA"},"outputs":[],"source":["# Import das bibliotecas.\n","import zipfile # Biblioteca para descompactar\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def downloadModeloPretreinado(model_args):\n","    \"\"\"\n","      Realiza o download do modelo BERT(MODELO) e retorna o diretório onde o modelo BERT(MODELO) foi descompactado.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\"\n","\n","    # Nome diretório base modelo BERT\n","    NOME_DIRETORIO_BASE_MODELO = \"modeloBERT\"\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Recupera o nome ou caminho do modelo\n","    MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in MODELO:\n","        URL_MODELO = MODELO\n","\n","    # Se a variável foi setada.\n","    if URL_MODELO:\n","\n","        # Diretório do modelo.\n","        DIRETORIO_MODELO = DIRETORIO_COHEBERT + \"/\" + NOME_DIRETORIO_BASE_MODELO\n","\n","        # Recupera o nome do arquivo do modelo da url.\n","        NOME_ARQUIVO = URL_MODELO.split(\"/\")[-1]\n","\n","        # Nome do arquivo do vocabulário.\n","        ARQUIVO_VOCAB = \"vocab.txt\"\n","\n","        # Caminho do arquivo na url.\n","        CAMINHO_ARQUIVO = URL_MODELO[0:len(URL_MODELO)-len(NOME_ARQUIVO)]\n","\n","        # Verifica se o diretório de descompactação existe no diretório corrente\n","        if os.path.exists(DIRETORIO_MODELO):\n","            logging.info(\"Apagando diretório existente do modelo!\")\n","            # Apaga o diretório e os arquivos existentes\n","            shutil.rmtree(DIRETORIO_MODELO)\n","\n","        # Realiza o download do arquivo do modelo\n","        downloadArquivo(URL_MODELO, NOME_ARQUIVO)\n","\n","        # Descompacta o arquivo no diretório de descompactação.\n","        arquivo_zip = zipfile.ZipFile(NOME_ARQUIVO, \"r\")\n","        arquivo_zip.extractall(DIRETORIO_MODELO)\n","\n","        # Baixa o arquivo do vocabulário.\n","        # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente.\n","        URL_MODELO_VOCAB = CAMINHO_ARQUIVO + ARQUIVO_VOCAB\n","        # Coloca o arquivo do vocabulário no diretório do modelo.\n","        downloadArquivo(URL_MODELO_VOCAB, DIRETORIO_MODELO + \"/\" + ARQUIVO_VOCAB)\n","\n","        # Apaga o arquivo compactado\n","        os.remove(NOME_ARQUIVO)\n","\n","        logging.info(\"Diretório {} do modelo BERT pronta!\".format(DIRETORIO_MODELO))\n","\n","    else:\n","        DIRETORIO_MODELO = MODELO\n","        logging.info(\"Variável URL_MODELO não setada!\")\n","\n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"V74WUpHqcfoI"},"source":["### Copia o modelo do BERT ajustado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQMpf9yycf8f"},"outputs":[],"source":["# Import das bibliotecas.\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def copiaModeloAjustado(model_args):\n","    \"\"\"\n","      Copia o modelo ajustado BERT do GoogleDrive para o projeto.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `DIRETORIO_LOCAL_MODELO_AJUSTADO` - Diretório de download ajustado do modelo.\n","    \"\"\"\n","\n","    # Verifica o nome do modelo BERT a ser utilizado\n","    MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = getTamanhoBERT(model_args)\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Diretório local de salvamento do modelo.\n","    DIRETORIO_LOCAL_MODELO_AJUSTADO = DIRETORIO_COHEBERT + \"/modelo_ajustado/\"\n","\n","    # Diretório remoto de salvamento do modelo no google drive.\n","    DIRETORIO_REMOTO_MODELO_AJUSTADO = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/validacao_classificacao_palavra/holdout/modelo/\" + MODELO_BERT + TAMANHO_BERT\n","\n","    # Copia o arquivo do modelo para o diretório no Google Drive.\n","    shutil.copytree(DIRETORIO_REMOTO_MODELO_AJUSTADO, DIRETORIO_LOCAL_MODELO_AJUSTADO)\n","\n","    logging.info(\"Modelo BERT ajustado copiado!\")\n","\n","    return DIRETORIO_LOCAL_MODELO_AJUSTADO"]},{"cell_type":"markdown","metadata":{"id":"eaneOhAKcO-3"},"source":["### Verifica de onde utilizar o modelo do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTy1TXz3cPKS"},"outputs":[],"source":["def verificaModelo(model_args):\n","    \"\"\"\n","    Verifica de onde utilizar o modelo.\n","\n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","\n","    Retorno:\n","    `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\"\n","\n","    DIRETORIO_MODELO = None\n","\n","    if model_args.usar_mcl_ajustado == True:\n","        # Diretório do modelo\n","        DIRETORIO_MODELO = copiaModeloAjustado()\n","\n","        logging.info(\"Usando modelo BERT ajustado.\")\n","\n","    else:\n","        DIRETORIO_MODELO = downloadModeloPretreinado(model_args)\n","        logging.info(\"Usando modelo BERT pré-treinado.\")\n","\n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"6tKcaIfReqdy"},"source":["## 4.2 Tokenizador BERT"]},{"cell_type":"markdown","metadata":{"id":"e8n7Z5s-QZF8"},"source":["### Função carrega Tokenizador BERT\n","\n","O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzAuptkwQZR3"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertTokenizer # Importando as bibliotecas do tokenizador BERT.\n","\n","def carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o tokenizador do DIRETORIO_MODELO.\n","      O tokenizador utiliza WordPiece.\n","      Carregando o tokenizador do diretório \"./modelo/\" do diretório padrão se variável `DIRETORIO_MODELO` setada.\n","      Caso contrário carrega da comunidade\n","      Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n","      O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados.\n","\n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `tokenizer` - Tokenizador BERT.\n","    \"\"\"\n","\n","    tokenizer = None\n","\n","    # Se a variável DIRETORIO_MODELO foi setada.\n","    if DIRETORIO_MODELO:\n","        # Carregando o Tokenizador.\n","        logging.info(\"Carregando o tokenizador BERT do diretório {}.\".format(DIRETORIO_MODELO))\n","\n","        tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, do_lower_case=model_args.do_lower_case)\n","\n","    else:\n","        # Carregando o Tokenizador da comunidade.\n","        logging.info(\"Carregando o tokenizador BERT da comunidade.\")\n","\n","        tokenizer = BertTokenizer.from_pretrained(model_args.pretrained_model_name_or_path, do_lower_case=model_args.do_lower_case)\n","\n","    return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"GYRV9KfHQE6v"},"source":["## 4.3 Carrega o modelo e tokenizador BERT\n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"markdown","metadata":{"id":"-pZZrUKRhR3e"},"source":["### Função carrega modelo BERT medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JUEyjCChUQh"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertModel # Importando as bibliotecas do Modelo BERT.\n","\n","def carregaModeloMedida(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o modelo e retorna o modelo.\n","\n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `model` - Um objeto do modelo BERT carregado.\n","    \"\"\"\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in model_args.pretrained_model_name_or_path:\n","        URL_MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Se a variável URL_MODELO foi setada\n","    if URL_MODELO:\n","        # Carregando o Modelo BERT\n","        logging.info(\"Carregando o modelo BERT do diretório {} para cálculo de medidas.\".format(DIRETORIO_MODELO))\n","\n","        model = BertModel.from_pretrained(DIRETORIO_MODELO,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","\n","    else:\n","        # Carregando o Modelo BERT da comunidade\n","        logging.info(\"Carregando o modelo BERT da comunidade {} para cálculo de medidas.\".format(model_args.pretrained_model_name_or_path))\n","\n","        model = BertModel.from_pretrained(model_args.pretrained_model_name_or_path,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"-uFDhRTZe2Js"},"source":["### Função carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVtAUbUBe2iS"},"outputs":[],"source":["def carregaBERT(model_args):\n","    \"\"\"\n","      Carrega o BERT para cálculo de medida ou classificação e retorna o modelo e o tokenizador.\n","      O tipo do model retornado pode ser BertModel ou BertForSequenceClassification, depende do tipo de model_args.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","          - Se model_args = ModeloArgumentosClassificacao deve ser carregado o BERT para classificação(BertForSequenceClassification).\n","          - Se model_args = ModeloArgumentosMedida deve ser carregado o BERT para cálculo de medida(BertModel).\n","\n","      Retorno:\n","        `model` - Um objeto do modelo BERT carregado.\n","        `tokenizer` - Um objeto tokenizador BERT carregado.\n","    \"\"\"\n","\n","    # Verifica a origem do modelo\n","    DIRETORIO_MODELO = verificaModelo(model_args)\n","\n","    # Variável para conter o modelo\n","    model = None\n","\n","    # Carrega o modelo para cálculo da medida\n","    model = carregaModeloMedida(DIRETORIO_MODELO, model_args)\n","\n","    # Carrega o tokenizador.\n","    # O tokenizador é o mesmo para o classificador e medidor.\n","    tokenizer = carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args)\n","\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"x5NTxBRKfAcT"},"source":["### Carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYMLJJYSQHY3"},"outputs":[],"source":["# Carrega o modelo e tokenizador do BERT\n","model, tokenizer = carregaBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"d7KprWqyZBQZ"},"source":["### Recupera detalhes do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6sPjTQnuQV2"},"outputs":[],"source":["# Verifica o nome do modelo BERT a ser utilizado\n","MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","# Verifica o tamanho do modelo(default large)\n","TAMANHO_BERT = getTamanhoBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"khTFfBVbnsx9"},"source":["## 4.4 Funções auxiliares do BERT"]},{"cell_type":"markdown","metadata":{"id":"lCJzsw8T0I-5"},"source":["### concatenaListas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpmDZ1mI0JHR"},"outputs":[],"source":["def concatenaListas(lista, pos=1):\n","  lista_concat = []\n","\n","  for x in lista:\n","      lista_concat = lista_concat + x[pos]\n","\n","  return lista_concat"]},{"cell_type":"markdown","metadata":{"id":"s42mgtmSZ8MR"},"source":["### getEmbeddingsCamadas\n","\n","Funções que recuperam os embeddings das camadas:\n","- Primeira camada;\n","- Penúltima camada;\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgo3EBTRZ9-3"},"outputs":[],"source":["def getEmbeddingPrimeiraCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado = output[2][0]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  return resultado\n","\n","def getEmbeddingPenultimaCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado = output[2][-2]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  return resultado\n","\n","def getEmbeddingUltimaCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado = output[2][-1]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  return resultado\n","\n","def getEmbeddingSoma4UltimasCamadas(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  embedding_camadas = output[2][-4:]\n","  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  # Usa o método `stack` para criar uma nova dimensão no tensor\n","  # com a concateção dos tensores dos embeddings.\n","  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado_stack = torch.stack(embedding_camadas, dim=0)\n","  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  # Realiza a soma dos embeddings de todos os tokens para as camadas\n","  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  resultado = torch.sum(resultado_stack, dim=0)\n","  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  return resultado\n","\n","def getEmbeddingConcat4UltimasCamadas(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Cria uma lista com os tensores a serem concatenados\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)\n","  # Lista com os tensores a serem concatenados\n","  lista_concat = []\n","\n","  # Percorre os 4 últimos\n","  for i in [-1,-2,-3,-4]:\n","      # Concatena da lista\n","      lista_concat.append(output[2][i])\n","\n","  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)\n","  # Realiza a concatenação dos embeddings de todos as camadas\n","  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)\n","  resultado = torch.cat(lista_concat, dim=-1)\n","\n","  # Saída: Entrada: (<1(lote)> x <qtde_tokens> x <3072 ou 4096>)\n","  return resultado\n","\n","def getEmbeddingSomaTodasAsCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas as camadas descontando a primeira(0)\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  embedding_camadas = output[2][1:]\n","  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  # Usa o método `stack` para criar uma nova dimensão no tensor\n","  # com a concateção dos tensores dos embeddings.\n","  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado_stack = torch.stack(embedding_camadas, dim=0)\n","  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  # Realiza a soma dos embeddings de todos os tokens para as camadas\n","  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  resultado = torch.sum(resultado_stack, dim=0)\n","  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  return resultado"]},{"cell_type":"markdown","metadata":{"id":"q7nx_eZ8hSlr"},"source":["### getEmbeddingsVisual\n","\n","Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n","\n","Existe uma função para os tipos de camadas utilizadas:\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLdbOT8-g43V"},"outputs":[],"source":["def getEmbeddingsVisualUltimaCamada(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingUltimaCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAf9lJJ2hZbt"},"outputs":[],"source":["def getEmbeddingsVisualSoma4UltimasCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XpwSN1ghpnz"},"outputs":[],"source":["def getEmbeddingsVisualConcat4UltimasCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3KU1EFrnSPK"},"outputs":[],"source":["def getEmbeddingsVisualSomaTodasAsCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSomaTodasAsCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"Y8MjE0utzlZT"},"source":["### getEmbeddings\n","\n","Função para gerar os embeddings de sentenças.\n","\n","Existe uma função para os tipos de camadas utilizadas:\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QcqOuwS067Q"},"outputs":[],"source":["def getEmbeddingsUltimaCamada(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingUltimaCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1wDGBl067Y"},"outputs":[],"source":["def getEmbeddingsSoma4UltimasCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hym19Hxr067Y"},"outputs":[],"source":["def getEmbeddingsConcat4UltimasCamadas(documento, modelo, tokenizer):\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-PLZiUR067Z"},"outputs":[],"source":["def getEmbeddingsSomaTodasAsCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSomaTodasAsCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"zFd1rse11DpZ"},"source":["### getDocumentoTokenizado\n","\n","Retorna o documento tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvWIBFTLJ7z9"},"outputs":[],"source":["def getDocumentoTokenizado(documento, tokenizer):\n","    \"\"\"\n","      Retorna o documento tokenizado pelo BERT.\n","\n","      Parâmetros:\n","      `documento` - Documento a ser tokenizado.\n","      `tokenizer` - Tokenizador do BERT.\n","    \"\"\"\n","\n","    # Adiciona os tokens especiais.\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Documento tokenizado\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    del tokenizer\n","\n","    return documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"3wvgXwN81RCz"},"source":["### encontrarIndiceSubLista\n","\n","Retorna os índices de início e fim da sublista na lista"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abS44M4yvFxf"},"outputs":[],"source":["def encontrarIndiceSubLista(lista: List, sublista: List):\n","    \"\"\"\n","    Localiza os índices de início e fim de uma sublista em uma lista.\n","    Baseado no algoritmo de https://codereview.stackexchange.com/questions/19627/finding-sub-list\n","    de  https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm\n","\n","    Parâmetros:\n","      `lista` - Uma lista.\n","      `sublista` - Uma sublista a ser localizada na lista.\n","\n","    Retorno:\n","      Os índices de início e fim da sublista na lista.\n","    \"\"\"\n","    # Tamanho da lista\n","    h = len(lista)\n","    # Tamanho da sblista\n","    n = len(sublista)\n","    # Cria um dicionário com os saltos descrescentes dos elementos n-1 da sublista\n","    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n","    i = n - 1\n","    # Percorre a lista\n","    while i < h:\n","        # Percorre a sublista\n","        for j in range(n):\n","            # Se elemento da lista diferente da sublista pula a interação\n","            if lista[i - j] != sublista[-j - 1]:\n","              # Passa para o próximo elemento da lista saltando a sublista\n","              i += skip.get(lista[i], n)\n","              # Interrompe o for.\n","              break\n","        else:\n","            #Finalizando a pesquisa depois de executar todo o for(sem break)\n","            indice_inicio = i - n + 1\n","            indice_fim = indice_inicio + len(sublista)-1\n","\n","            # Retorna o início e fim da sublista na lista\n","            return indice_inicio, indice_fim\n","\n","    # Não encontrou a sublista na lista\n","    return -1, -1"]},{"cell_type":"markdown","metadata":{"id":"kGL37G6XFcwp"},"source":["### getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras\n","\n","A partir dos embeddings do documento, localiza o indíce de início e fim de uma sentença no documento e retorna os embeddings da sentença."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uI07Y_M8__HG"},"outputs":[],"source":["def getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embedding_documento,\n","                                                           token_BERT_documento,\n","                                                           sentenca,\n","                                                           tokenizer):\n","\n","  # Tokeniza a sentença\n","  sentenca_tokenizada_BERT = getDocumentoTokenizado(sentenca, tokenizer)\n","  #print(sentenca_tokenizada_BERT)\n","\n","  # Remove os tokens de início e fim da sentença\n","  sentenca_tokenizada_BERT.remove(\"[CLS]\")\n","  sentenca_tokenizada_BERT.remove(\"[SEP]\")\n","  #print(len(sentenca_tokenizada_BERT))\n","\n","  # Localiza os índices dos tokens da sentença no documento\n","  inicio, fim = encontrarIndiceSubLista(token_BERT_documento, sentenca_tokenizada_BERT)\n","  #print(inicio,fim)\n","\n","  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n","  embedding_sentenca = embedding_documento[inicio:fim+1]\n","  #print(\"embedding_sentenca=\", embedding_sentenca.shape)\n","\n","  del tokenizer\n","  del token_BERT_documento\n","  del embedding_documento\n","\n","  # Retorna o embedding da sentença no documento\n","  return embedding_sentenca, sentenca_tokenizada_BERT"]},{"cell_type":"markdown","metadata":{"id":"THFhXGGmIO_r"},"source":["### getEmbeddingDocumentoComTodasPalavrasMean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhW_OiEsIPJI"},"outputs":[],"source":["# Importa a biblioteca\n","import torch\n","\n","def getEmbeddingDocumentoComTodasPalavrasMean(embedding_documento):\n","  \"\"\"\n","    Calcula a média dos embeddings do documento excluindo os tokens\n","    especiais [CLS] do início e [SEP] do fim.\n","    Remove primeira dimensão devido ao cálculo da média.\n","\n","    Parâmetros:\n","    `embedding_documento` - Embedding do documento.\n","  \"\"\"\n","\n","  # Calcula a média dos embeddings para os tokens de embedding_documento, removendo a primeira dimensão.\n","  # Entrada: <qtde_tokens> x <768 ou 1024>\n","  #print(\"embedding_documento1=\", embedding_documento.shape)\n","  media_embedding_documento = torch.mean(embedding_documento[1:-1], dim=0)\n","  # Saída: <768 ou 1024>\n","\n","  del embedding_documento\n","\n","  return media_embedding_documento"]},{"cell_type":"markdown","metadata":{"id":"1Ko_of60YuNd"},"source":["### getEmbeddingDocumentoRelevanteMean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDokSSODY0Sf"},"outputs":[],"source":["# Importa a biblioteca\n","import torch\n","\n","def getEmbeddingDocumentoRelevanteMean(id_documento,\n","                                       index_sentenca,\n","                                       embedding_documento,\n","                                       token_BERT_documento,\n","                                       documento,\n","                                       token_documento,\n","                                       pos_documento,\n","                                       filtro):\n","  \"\"\"\n","    Calcula a média dos embeddings do documento considerando tokens do tipo\n","    especificado no filtro\n","    Remove primeira dimensão devido ao cálculo da média.\n","\n","    Parâmetros:\n","    `embedding_documento` - Embeddings do documento gerados pelo BERT.\n","    `token_BERT_documento` - Lista com os tokens do documento gerados pelo tokenizador BERT.\n","    `documento` - Texto com o documento.\n","    `tokenizer` - Tokenizador do BERT.\n","    `token_documento` - Lista com os tokens do documento.\n","    `pos_documento` - Lista com as POS-Tagging do documento.\n","    `filtro` - Filtro dos embeddings.\n","\n","  \"\"\"\n","\n","  # Recupera a lista de tokens do documento, a lista dos postagging e a lista dos seus embeddings com um mesmo tamanho\n","  lista_tokens, lista_postagging, lista_embeddings = getTokensEmbeddingsPOSSentenca(id_documento,\n","                                                                                    index_sentenca,\n","                                                                                    embedding_documento,\n","                                                                                    token_BERT_documento,\n","                                                                                    documento,\n","                                                                                    token_documento,\n","                                                                                    pos_documento)\n","\n","  #print(\"len(token_BERT_documento):\", len(token_BERT_documento))\n","  #print(\"token_BERT_documento:\", token_BERT_documento)\n","  #print(\"len(pos_documento):\", len(pos_documento))\n","  #print(\"pos_documento:\", pos_documento)\n","  #print(\"filtro:\", filtro)\n","  #print()\n","\n","  # Lista com os tensores selecionados\n","  lista_tokens_selecionados = []\n","  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word no documento\n","  for i, token_documento in enumerate(lista_tokens):\n","      if (lista_postagging[i] in filtro):\n","          #print(\"Adicionando palavra do embedding:\", lista_tokens[i])\n","          lista_tokens_selecionados.append(lista_embeddings[i])\n","\n","  if  len(lista_tokens_selecionados) != 0:\n","      # Empila os embeddings da lista pela dimensão 0\n","      embedding_relevante = torch.stack(lista_tokens_selecionados, dim=0)\n","      #print(\"embedding_relevante.shape:\",embedding_relevante.shape)\n","\n","      # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n","      # Entrada: <qtde_tokens> x <768 ou 1024>\n","      media_embedding_relevante = torch.mean(embedding_relevante, dim=0)\n","      # Saída: <768 ou 1024>\n","      #print(\"media_embedding_relevante.shape:\", media_embedding_relevante.shape)\n","  else:\n","      media_embedding_relevante = None\n","\n","  del embedding_documento\n","  del token_BERT_documento\n","  del documento\n","  del token_documento\n","  del pos_documento\n","\n","  return media_embedding_relevante"]},{"cell_type":"markdown","metadata":{"id":"L_vknrk7YSpF"},"source":["### getEmbeddingDocumentoMean\n","\n","Filtros:\n","- ALL - Sentença com todas as palavras\n","- NOUN - Sentença somente com substantivos\n","- VERB - Sentença somente com verbos\n","- VERB,NOUN - Sentença somente com verbos e substantivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pd8B76YyYS02"},"outputs":[],"source":["def getEmbeddingDocumentoMean(id_documento,\n","                              index_sentenca,\n","                              embedding_documento,\n","                              token_BERT_documento,\n","                              documento,\n","                              tokenizer,\n","                              token_documento,\n","                              pos_documento,\n","                              filtro=[\"ALL\"]):\n","  \"\"\"\n","    Rediciona o cálculo da média dos embeddings de acordo com o filtro especificado.\n","\n","    Parâmetros:\n","    `embedding_documento` - Embeddings do documento gerados pelo BERT.\n","    `token_BERT_documento` - Lista com os tokens do documento gerados pelo tokenizador BERT.\n","    `documento` - Texto com o documento.\n","    `tokenizer` - Tokenizador do BERT.\n","    `token_documento` - Lista com os tokens do documento.\n","    `pos_documento` - Lista com as POS-Tagging do documento.\n","    `filtro` - Filtro dos embeddings.\n","  \"\"\"\n","\n","  if \"ALL\" in filtro:\n","    return getEmbeddingDocumentoComTodasPalavrasMean(embedding_documento)\n","  else:\n","    return getEmbeddingDocumentoRelevanteMean(id_documento,\n","                                              index_sentenca,\n","                                              embedding_documento,\n","                                              token_BERT_documento,\n","                                              documento,\n","                                              token_documento,\n","                                              pos_documento,\n","                                              filtro)"]},{"cell_type":"markdown","metadata":{"id":"t1PgxcL01VfF"},"source":["### getTokensEmbeddingsPOSSentenca\n","Gera os tokens, POS e embeddings de cada sentença."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBkcF2ve1VfG"},"outputs":[],"source":["# Dicionário de tokens de exceções e seus deslocamentos para considerar mais tokens do BERT em relação ao spaCy\n","# A tokenização do BERT gera mais tokens que a tokenização das palavras do spaCy\n","dic_excecao_maior = {\"\":-1,\n","                    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXJk5Od51VfI"},"outputs":[],"source":["def getExcecaoDicMaior(token, dic_excecao_maior):\n","\n","  valor = dic_excecao_maior.get(token)\n","  if valor != None:\n","      return valor\n","  else:\n","      return -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6TSm62Y1VfI"},"outputs":[],"source":["# Dicionário de tokens de exceções e seus deslocamentos para considerar menos tokens do BERT em relação ao spaCy\n","# A tokenização do BERT gera menos tokens que a tokenização das palavras do spaCy\n","dic_excecao_menor = {\"1°\":1,\n","                    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OYmoFVk1VfJ"},"outputs":[],"source":["def getExcecaoDicMenor(token, dic_excecao_menor):\n","\n","  valor = dic_excecao_menor.get(token)\n","  if valor != None:\n","      return valor\n","  else:\n","      return -1"]},{"cell_type":"markdown","metadata":{"id":"JzYA5pPE1VfJ"},"source":["Função que retorna os embeddings, tokens e POS da sentença com um mesmo tamanho."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9H1JlTt1VfK"},"outputs":[],"source":["# Importa a biblioteca\n","import torch\n","\n","def getTokensEmbeddingsPOSSentenca(embedding_documento,\n","                                   token_BERT_documento,\n","                                   sentenca):\n","    \"\"\"\n","      Retorna os tokens, as postagging e os embeddings dos tokens igualando a quantidade de tokens do spaCy com a tokenização do BERT de acordo com a estratégia.\n","      Usa a estratégia MEAN para calcular a média dos embeddings dos tokens que formam uma palavra.\n","      Usa a estratégia MAX para calcular o valor máximo dos embeddings dos tokens que formam uma palavra.\n","    \"\"\"\n","\n","    #Guarda os tokens e embeddings\n","    lista_tokens = []\n","    lista_tokens_OOV = []\n","    lista_embeddings_MEAN = []\n","    lista_embeddings_MAX = []\n","\n","    # Gera a tokenização e POS-Tagging da sentença\n","    sentenca_token, sentenca_pos = getListaTokensPOSSentenca(sentenca)\n","\n","    # print(\"\\nsentenca          :\",sentenca)\n","    # print(\"sentenca_token      :\",sentenca_token)\n","    # print(\"len(sentenca_token) :\",len(sentenca_token))\n","    # print(\"sentenca_pos        :\",sentenca_pos)\n","    # print(\"len(sentenca_pos)   :\",len(sentenca_pos))\n","\n","    # Recupera os embeddings da sentença dos embeddings do documento\n","    embedding_sentenca = embedding_documento\n","    sentenca_tokenizada_BERT = token_BERT_documento\n","\n","    # embedding <qtde_tokens x 4096>\n","    # print(\"embedding_sentenca          :\",embedding_sentenca.shape)\n","    # print(\"sentenca_tokenizada_BERT     :\",sentenca_tokenizada_BERT)\n","    # print(\"len(sentenca_tokenizada_BERT):\",len(sentenca_tokenizada_BERT))\n","\n","    # Seleciona os pares de palavra a serem avaliadas\n","    pos_wi = 0 # Posição do token da palavra gerado pelo spaCy\n","    pos_wj = pos_wi # Posição do token da palavra gerado pelo BERT\n","    pos2 = -1\n","\n","    # Enquanto o indíce da palavra pos_wj(2a palavra) não chegou ao final da quantidade de tokens do BERT\n","    while pos_wj < len(sentenca_tokenizada_BERT):\n","\n","      # Seleciona os tokens da sentença\n","      wi = sentenca_token[pos_wi] # Recupera o token da palavra gerado pelo spaCy\n","      wi1 = \"\"\n","      pos2 = -1\n","      if pos_wi+1 < len(sentenca_token):\n","        wi1 = sentenca_token[pos_wi+1] # Recupera o próximo token da palavra gerado pelo spaCy\n","\n","        # Localiza o deslocamento da exceção\n","        pos2 = getExcecaoDicMenor(wi+wi1, dic_excecao_menor)\n","        #print(\"Exceção pos2:\", pos2)\n","\n","      wj = sentenca_tokenizada_BERT[pos_wj] # Recupera o token da palavra gerado pelo BERT\n","      # print(\"wi[\",pos_wi,\"]=\", wi)\n","      # print(\"wj[\",pos_wj,\"]=\", wj)\n","\n","      # Tratando exceções\n","      # Localiza o deslocamento da exceção\n","      pos = getExcecaoDicMaior(wi, dic_excecao_maior)\n","      #print(\"Exceção pos:\", pos)\n","\n","      if pos != -1 or pos2 != -1:\n","        if pos != -1:\n","          #print(\"Adiciona 1 Exceção palavra == wi or palavra = [UNK]:\",wi)\n","          lista_tokens.append(wi)\n","          # Marca como fora do vocabulário do BERT\n","          lista_tokens_OOV.append(1)\n","          # Verifica se tem mais de um token\n","          if pos != 1:\n","            indice_token = pos_wj + pos\n","            #print(\"Calcula a média de :\", pos_wj , \"até\", indice_token)\n","            embeddings_tokens_palavra = embedding_sentenca[pos_wj:indice_token]\n","            #print(\"embeddings_tokens_palavra:\",embeddings_tokens_palavra.shape)\n","            # calcular a média dos embeddings dos tokens do BERT da palavra\n","            embedding_estrategia_MEAN = torch.mean(embeddings_tokens_palavra, dim=0)\n","            #print(\"embedding_estrategia_MEAN:\",embedding_estrategia_MEAN.shape)\n","            lista_embeddings_MEAN.append(embedding_estrategia_MEAN)\n","\n","            # calcular o máximo dos embeddings dos tokens do BERT da palavra\n","            embedding_estrategia_MAX, linha = torch.max(embeddings_tokens_palavra, dim=0)\n","            #print(\"embedding_estrategia_MAX:\",embedding_estrategia_MAX.shape)\n","            lista_embeddings_MAX.append(embedding_estrategia_MAX)\n","          else:\n","            # Adiciona o embedding do token a lista de embeddings\n","            lista_embeddings_MEAN.append(embedding_sentenca[pos_wj])\n","            lista_embeddings_MAX.append(embedding_sentenca[pos_wj])\n","\n","          # Avança para a próxima palavra e token do BERT\n","          pos_wi = pos_wi + 1\n","          pos_wj = pos_wj + pos\n","          #print(\"Proxima:\")\n","          #print(\"wi[\",pos_wi,\"]=\", sentenca_token[pos_wi])\n","          #print(\"wj[\",pos_wj,\"]=\", sentenca_tokenizada_BERT[pos_wj])\n","        else:\n","          if pos2 != -1:\n","            #print(\"Adiciona 1 Exceção palavra == wi or palavra = [UNK]:\",wi)\n","            lista_tokens.append(wi+wi1)\n","            # Marca como fora do vocabulário do BERT\n","            lista_tokens_OOV.append(1)\n","            # Verifica se tem mais de um token\n","            if pos2 == 1:\n","              # Adiciona o embedding do token a lista de embeddings\n","              lista_embeddings_MEAN.append(embedding_sentenca[pos_wj])\n","              lista_embeddings_MAX.append(embedding_sentenca[pos_wj])\n","\n","            # Avança para a próxima palavra e token do BERT\n","            pos_wi = pos_wi + 2\n","            pos_wj = pos_wj + pos2\n","            #print(\"Proxima:\")\n","            #print(\"wi[\",pos_wi,\"]=\", sentenca_token[pos_wi])\n","            #print(\"wj[\",pos_wj,\"]=\", sentenca_tokenizada_BERT[pos_wj])\n","      else:\n","        # Tokens iguais adiciona a lista, o token não possui subtoken\n","        if (wi == wj or wj==\"[UNK]\"):\n","          # Adiciona o token a lista de tokens\n","          #print(\"Adiciona 2 wi==wj or wj==[UNK]:\", wi )\n","          lista_tokens.append(wi)\n","          # Marca como dentro do vocabulário do BERT\n","          lista_tokens_OOV.append(0)\n","          # Adiciona o embedding do token a lista de embeddings\n","          lista_embeddings_MEAN.append(embedding_sentenca[pos_wj])\n","          lista_embeddings_MAX.append(embedding_sentenca[pos_wj])\n","          #print(\"embedding1[pos_wj]:\", embedding_sentenca[pos_wj].shape)\n","          # Avança para a próxima palavra e token do BERT\n","          pos_wi = pos_wi + 1\n","          pos_wj = pos_wj + 1\n","\n","        else:\n","          # A palavra foi tokenizada pelo Wordpice com ## ou diferente do spaCy ou desconhecida\n","          # Inicializa a palavra a ser montada\n","          palavra_POS = wj\n","          indice_token = pos_wj + 1\n","          while  ((palavra_POS != wi) and indice_token < len(sentenca_tokenizada_BERT)):\n","              if \"##\" in sentenca_tokenizada_BERT[indice_token]:\n","                # Remove os caracteres \"##\" do token\n","                parte = sentenca_tokenizada_BERT[indice_token][2:]\n","              else:\n","                parte = sentenca_tokenizada_BERT[indice_token]\n","\n","              palavra_POS = palavra_POS + parte\n","              #print(\"palavra_POS:\",palavra_POS)\n","              # Avança para o próximo token do BERT\n","              indice_token = indice_token + 1\n","\n","          #print(\"\\nMontei palavra:\",palavra_POS)\n","          if (palavra_POS == wi or palavra_POS == \"[UNK]\"):\n","              # Adiciona o token a lista\n","              #print(\"Adiciona 3 palavra == wi or palavra_POS = [UNK]:\",wi)\n","              lista_tokens.append(wi)\n","              # Marca como fora do vocabulário do BERT\n","              lista_tokens_OOV.append(1)\n","              # Calcula a média dos tokens da palavra\n","              #print(\"Calcula o máximo :\", pos_wj , \"até\", indice_token)\n","              embeddings_tokens_palavra = embedding_sentenca[pos_wj:indice_token]\n","              #print(\"embeddings_tokens_palavra2:\",embeddings_tokens_palavra)\n","              #print(\"embeddings_tokens_palavra2:\",embeddings_tokens_palavra.shape)\n","\n","              # calcular a média dos embeddings dos tokens do BERT da palavra\n","              embedding_estrategia_MEAN = torch.mean(embeddings_tokens_palavra, dim=0)\n","              #print(\"embedding_estrategia_MEAN:\",embedding_estrategia_MEAN)\n","              #print(\"embedding_estrategia_MEAN.shape:\",embedding_estrategia_MEAN.shape)\n","              lista_embeddings_MEAN.append(embedding_estrategia_MEAN)\n","\n","              # calcular o valor máximo dos embeddings dos tokens do BERT da palavra\n","              embedding_estrategia_MAX, linha = torch.max(embeddings_tokens_palavra, dim=0)\n","              #print(\"embedding_estrategia_MAX:\",embedding_estrategia_MAX)\n","              #print(\"embedding_estrategia_MAX.shape:\",embedding_estrategia_MAX.shape)\n","              lista_embeddings_MAX.append(embedding_estrategia_MAX)\n","\n","          # Avança para o próximo token do spaCy\n","          pos_wi = pos_wi + 1\n","          # Pula para o próximo token do BERT\n","          pos_wj = indice_token\n","\n","    # Verificação se as listas estão com o mesmo tamanho\n","    #if (len(lista_tokens) != len(sentenca_token)) or (len(lista_embeddings_MEAN) != len(sentenca_token)):\n","    if (len(lista_tokens) !=  len(lista_embeddings_MEAN)):\n","       print(\"\\nsentenca                  :\",sentenca)\n","       print(\"sentenca_pos              :\",sentenca_pos)\n","       print(\"sentenca_token            :\",sentenca_token)\n","       print(\"sentenca_tokenizada_BERT  :\",sentenca_tokenizada_BERT)\n","       print(\"lista_tokens              :\",lista_tokens)\n","       print(\"len(lista_tokens)         :\",len(lista_tokens))\n","       print(\"lista_embeddings_MEAN     :\",lista_embeddings_MEAN)\n","       print(\"len(lista_embeddings_MEAN):\",len(lista_embeddings_MEAN))\n","       print(\"lista_embeddings_MAX      :\",lista_embeddings_MAX)\n","       print(\"len(lista_embeddings_MAX) :\",len(lista_embeddings_MAX))\n","\n","    del embedding_sentenca\n","    del token_BERT_documento\n","    del sentenca_tokenizada_BERT\n","    del sentenca_token\n","\n","    return lista_tokens, sentenca_pos, lista_tokens_OOV, lista_embeddings_MEAN, lista_embeddings_MAX"]},{"cell_type":"markdown","metadata":{"id":"1qezcBkxnEdR"},"source":["# 5 - Projeção de embeddings\n","\n","Apresenta os tokens gerados pelo BERT e seus embeddings."]},{"cell_type":"markdown","metadata":{"id":"oQUy9Tat2EF_"},"source":["## 5.1 Carregamento dos arquivos de dados originais e permutados"]},{"cell_type":"markdown","metadata":{"id":"bD_tNbBGPrnE"},"source":["### 5.1.1 Especifica os nomes dos arquivos de dados\n","\n"]},{"cell_type":"code","metadata":{"id":"bNgwJRC2uGJb"},"source":["# Nome do arquivo\n","NOME_ARQUIVO_ORIGINAL = \"original.csv\"\n","NOME_ARQUIVO_ORIGINAL_COMPACTADO = \"original.zip\"\n","NOME_ARQUIVO_ORIGINAL_POS = \"originalpos.csv\"\n","NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO = \"originalpos.zip\"\n","\n","NOME_ARQUIVO_PERTURBADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_COMPACTADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\"\n","NOME_ARQUIVO_PERTURBADO_POS = \"perturbadopos_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO = \"perturbadopos_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1.2 Cria o diretório local para receber os dados"],"metadata":{"id":"I0LsmsBlJeeV"}},{"cell_type":"code","metadata":{"id":"gFYIHcIHE985"},"source":["# Importando as bibliotecas.\n","import os\n","\n","# Cria o diretório para receber os arquivos Originais e Permutados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL[:-1]\n","\n","if not os.path.exists(dirbase):\n","    # Cria o diretório\n","    os.makedirs(dirbase)\n","    logging.info(\"Diretório criado: {}\".format(dirbase))\n","else:\n","    logging.info(\"Diretório já existe: {}\".format(dirbase))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D8A9syejCsD2"},"source":["### 5.1.3 Copia os arquivos do Google Drive para o Colaboratory"]},{"cell_type":"code","metadata":{"id":"pviuxToMCxQw"},"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a cópia.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFCvZ6CUmt-9"},"source":["Descompacta os arquivos.\n","\n","Usa o unzip para descompactar:\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens\n","*   `-d` Diretório de destino\n"]},{"cell_type":"code","metadata":{"id":"dbHl3d88mouc"},"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a descompactação.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzhYJNWJm1z4"},"source":["### 5.1.4 Carregamento das lista com os dados dos arquivos originais, perturbados e permutados"]},{"cell_type":"markdown","metadata":{"id":"Usr1uRzQeJSb"},"source":["#### Carrega o arquivo dos dados originais e POS"]},{"cell_type":"code","metadata":{"id":"QRHlixdHEDTb"},"source":["# Import das bibliotecas.\n","import pandas as pd\n","\n","# Abre o arquivo e retorna o DataFrame\n","lista_documentos_originais = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_originais_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lista_documentos_originais = lista_documentos_originais[:5]"],"metadata":{"id":"mzl2SUe7N4QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJ5STBZPLlie"},"source":["lista_documentos_originais.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NVwIdXXDFn7"},"source":["lista_documentos_originais_pos.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Corrigir os tipos de colunas dos dados originais e POS\n","\n","Em dados originais:\n","- coluna 1 - `sentenças` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados originais pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."],"metadata":{"id":"-hfUpvKqXoqe"}},{"cell_type":"code","source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","# Corrige os tipos dos dados\n","tipos = {\"id\": str}\n","lista_documentos_originais = lista_documentos_originais.astype(tipos)\n","lista_documentos_originais_pos = lista_documentos_originais_pos.astype(tipos)\n","\n","# Verifica se o tipo da coluna não é list e converte\n","lista_documentos_originais[\"sentencas\"] = lista_documentos_originais[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","lista_documentos_originais_pos[\"pos_documento\"] = lista_documentos_originais_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","logging.info(\"TERMINADO CORREÇÃO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO CORREÇÃO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"],"metadata":{"id":"lj9sJVavMccj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Criando dados indexados originais"],"metadata":{"id":"8yyRt4jnYxsU"}},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_originais_indexado = lista_documentos_originais.set_index([\"id\"])\n","lista_documentos_originais_indexado.head()"],"metadata":{"id":"B9INo4nBS8aQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_originais_pos_indexado = lista_documentos_originais_pos.set_index([\"id\"])\n","lista_documentos_originais_pos_indexado.head()"],"metadata":{"id":"j70x_r30T_bx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJXcpioo7Bhn"},"source":["#### Carrega o arquivo dos dados perturbados e POS"]},{"cell_type":"code","metadata":{"id":"gB500dmd7Bho"},"source":["# Abre o arquivo e retorna o DataFrame\n","lista_documentos_perturbados = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_perturbados_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO PERTURBADOS: {}.\".format(len(lista_documentos_perturbados)))\n","logging.info(\"TERMINADO PERTURBADOS POS: {}.\".format(len(lista_documentos_perturbados_pos)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AlgUns csv estão com os nomes das colunas errados"],"metadata":{"id":"qzTPk_BsFdzc"}},{"cell_type":"code","source":["lista_documentos_perturbados = lista_documentos_perturbados.rename(columns={'documentoPerturbado': 'documento_perturbado'})"],"metadata":{"id":"YlJ7P-kzFYmR"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQ9cgAz47Bhp"},"source":["lista_documentos_perturbados.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_perturbados_pos.sample(5)"],"metadata":{"id":"IE1xJdZWkc5I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Corrigir os tipos de colunas dos dados perturbados e POS\n","\n","Em dados perturbados:\n","- coluna 1 - `perturbado` carregadas do arquivo vem como string e não como lista.\n","- coluna 3 - `sentencas` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados perturbados pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."],"metadata":{"id":"VrfZzjjpsUOU"}},{"cell_type":"code","source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","# Corrige os tipos dos dados\n","tipos = {\"id\": str}\n","lista_documentos_perturbados = lista_documentos_perturbados.astype(tipos)\n","lista_documentos_perturbados_pos = lista_documentos_perturbados_pos.astype(tipos)\n","\n","# Verifica se o tipo da coluna não é list e converte\n","lista_documentos_perturbados[\"perturbado\"] = lista_documentos_perturbados[\"perturbado\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","lista_documentos_perturbados[\"sentencas\"] = lista_documentos_perturbados[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","lista_documentos_perturbados_pos[\"pos_documento\"] = lista_documentos_perturbados_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","logging.info(\"TERMINADO CORREÇÃO PERTURBADO: {}.\".format(len(lista_documentos_perturbados)))\n","logging.info(\"TERMINADO CORREÇÃO PERTURBADO POS: {}.\".format(len(lista_documentos_perturbados_pos)))"],"metadata":{"id":"ZHf-7dgSsUOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_perturbados.sample(5)"],"metadata":{"id":"U-w5YwpbNqu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_perturbados_pos.sample(5)"],"metadata":{"id":"1laR3W5hP6iu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Criando dados indexados perturbados"],"metadata":{"id":"Ix-Q5fZXY3HR"}},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_perturbados_indexado = lista_documentos_perturbados.set_index([\"id\"])\n","lista_documentos_perturbados_indexado.head()"],"metadata":{"id":"FqRQnYUtSxzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_perturbados_pos_indexado = lista_documentos_perturbados_pos.set_index([\"id\"])\n","lista_documentos_perturbados_pos_indexado.head()"],"metadata":{"id":"s0aDUbeZT1M8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1.5 Agrupar os dados originais e perturbados"],"metadata":{"id":"m-vP_FnPWe0K"}},{"cell_type":"code","source":["# Import das bibliotecas.\n","import ast\n","from tqdm.notebook import tqdm as tqdm_notebook\n","\n","def agruparDadosOriginaisPerturbados(lista_documentos_originais, lista_documentos_perturbados_indexado):\n","\n","  print(\"Processando\",len(lista_documentos_originais),\"documentos originais\")\n","\n","  lista_documentos_agrupados = []\n","\n","  # Se tem algum id no lista do filtro seleciona os documentos originais\n","  if len(FILTRO_DO) != 0:\n","    lista_filtro = lista_documentos_originais[lista_documentos_originais['id'].isin(FILTRO_DO)]\n","\n","    # Barra de progresso dos documentos\n","    lista_documentos_originais_bar = tqdm_notebook(lista_filtro.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(lista_filtro))\n","  else:\n","    # Barra de progresso dos documentos\n","    lista_documentos_originais_bar = tqdm_notebook(lista_documentos_originais.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(lista_documentos_originais))\n","\n","  # Percorre os documentos\n","  for i, linha_documento in lista_documentos_originais_bar:\n","      #if i < 2:\n","      #print(\"linha_documento:\",linha_documento)\n","      # Recupera o id do documento\n","      id_documento = linha_documento[0]\n","      #print(\"id_documento:\",id_documento)\n","\n","      # Carrega a lista das sentenças do documento\n","      lista_sentenca_documento = linha_documento[1]\n","      #print(\"\\nlista_sentenca_documento:\",lista_sentenca_documento)\n","      #print(\"len(lista_sentenca_documento):\",len(lista_sentenca_documento))\n","\n","      # Adiciona o original a lista dos dados agrupados, considerando como coerente(1)\n","      lista_documentos_agrupados.append([id_documento, lista_sentenca_documento, linha_documento[2], 1])\n","\n","      # Percorre os documentos perturbados apartir do original\n","      for j in range(0, model_args.documentos_perturbados):\n","\n","        # Id do documento perturbado\n","        id_perturbado = str(id_documento) + \"_pert_\" + str(j)\n","\n","        # localiza o documento perturbado\n","        #documento_perturbado = lista_documentos_perturbados.loc[lista_documentos_perturbados['id']==id_perturbado].values[0]\n","        documento_perturbado = lista_documentos_perturbados_indexado.loc[id_perturbado]\n","        # Recupera a sentença do documento perturbado\n","        lista_perturbado = documento_perturbado[0]\n","\n","        # Adiciona o perturbado a lista dos dados agrupados considerando como incoerente(0)\n","        lista_documentos_agrupados.append([id_perturbado, lista_perturbado, documento_perturbado[1], 0])\n","\n","  logging.info(\"TERMINADO AGRUPAMENTO: {}.\".format(len(lista_documentos_agrupados)))\n","\n","  # Cria o dataframe da lista\n","  lista_documentos_agrupados = pd.DataFrame(lista_documentos_agrupados, columns = [\"id\",\"sentencas\",\"documento\",\"classe\"])\n","\n","  # Corrige os tipos dos dados da lista agrupada\n","  tipos = {\"id\": str, \"sentencas\": object, \"documento\": str, \"classe\": int}\n","\n","  lista_documentos_agrupados = lista_documentos_agrupados.astype(tipos)\n","\n","  return lista_documentos_agrupados"],"metadata":{"id":"NUJBBHy8We0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importa das bibliotecas\n","import pandas as pd\n","\n","print(\"Analisando documentos originais e perturbados\")\n","# Concatena as listas de documentos originais e perturbados\n","lista_documentos_agrupados = agruparDadosOriginaisPerturbados(lista_documentos_originais, lista_documentos_perturbados_indexado)\n","lista_documentos_agrupados_pos = pd.concat([lista_documentos_originais_pos, lista_documentos_perturbados_pos])\n","\n","# Corrige o tipo de dado da coluna id da lista\n","tipos = {\"id\": str}\n","lista_documentos_agrupados_pos = lista_documentos_agrupados_pos.astype(tipos)"],"metadata":{"id":"wY3dqPGVdoHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logging.info(\"TERMINADO AGRUPAMENTO: {}.\".format(len(lista_documentos_agrupados)))"],"metadata":{"id":"AFvbzDbwdrYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_documentos_agrupados.sample(5)"],"metadata":{"id":"a5ZV4jzAWe0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logging.info(\"TERMINADO AGRUPAMENTO POS: {}.\".format(len(lista_documentos_agrupados_pos)))"],"metadata":{"id":"4mCqTRecWe0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Criar dados indexados"],"metadata":{"id":"viicg1E7mXLK"}},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_agrupados_indexado = lista_documentos_agrupados.set_index([\"id\"])\n","lista_documentos_agrupados_indexado.head()"],"metadata":{"id":"0YBdkvoPm2vO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_agrupados_pos_indexado = lista_documentos_agrupados_pos.set_index([\"id\"])\n","lista_documentos_agrupados_pos_indexado.head()"],"metadata":{"id":"NQjlOJzOmbsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1.6 Funções auxiliares"],"metadata":{"id":"pnY7O9zb8n8Z"}},{"cell_type":"markdown","source":["#### getIndicePalavraPerturbada\n","\n","Retorna o índice da palavra perturbada em um documento"],"metadata":{"id":"KhhvmLDQbg4z"}},{"cell_type":"code","source":["def getIndicePalavraPerturbada(_id_perturbado):\n","\n","  # print(\"_id_perturbado:\",_id_perturbado)\n","\n","  # localiza os dados do documento perturbado mascarado\n","  reg_documento_perturbado = lista_documentos_perturbados_indexado.loc[_id_perturbado]\n","\n","  # Recupera a lista das sentenças perturbadas\n","  lista_sentencas_mascarada = reg_documento_perturbado[2]\n","\n","  # Índice da sentença perturbada\n","  index_sentenca = -1\n","\n","  # Percorre as sentenças para encontrar a sentença perturbada\n","  for i, linha in enumerate(lista_sentencas_mascarada):\n","\n","    # Identifica a sentença mascarada que foi perturbada\n","    if 'MASK' in linha[0] :\n","      # Recupera a palavra mascarada sentença do documento perturbado\n","      index_sentenca = i\n","      sentenca_mascarada = linha[0]\n","      palavra_mascarada = linha[1]\n","      token_predito = linha[2]\n","      peso_predito = linha[3]\n","\n","\n","  # localiza os dados do documento perturbado pos\n","  reg_documento_perturbado_pos = lista_documentos_perturbados_pos_indexado.loc[_id_perturbado]\n","  # print(\"reg_documento_perturbado_pos:\",reg_documento_perturbado_pos)\n","\n","  # Recupera as POS Tagging do documento perturbado\n","  tokens_perturbado_index_palavra = []\n","\n","  # Recupera os pos das sentenças\n","  pos_documento_perturbado = reg_documento_perturbado_pos['pos_documento']\n","  # print(\"pos_documento_perturbado:\",pos_documento_perturbado)\n","\n","  # Percorre as sentenças do documento\n","  for i, linha1 in enumerate(pos_documento_perturbado):\n","    # print(\"linha1:\", linha1)\n","\n","    # Percorre os tokens da sentença\n","    for j, linha2 in enumerate(linha1[0]):\n","      # print(\"linha2:\", linha2)\n","      # Localiza o indice da palavra perturbada na sentença\n","      if token_predito == linha2:\n","        # Guarda o indice da palavra perturbada\n","        tokens_perturbado_index_palavra.append(j)\n","\n","  # Verifica se encontrou o índice da palavra perturbada\n","  if len(tokens_perturbado_index_palavra) != 0:\n","      # Possui somente uma palavra perturbada\n","      if len(tokens_perturbado_index_palavra) == 1:\n","        return tokens_perturbado_index_palavra[0]\n","      else:\n","        return tokens_perturbado_index_palavra\n","  else:\n","    # Não encontrou o índice da palavra perturbada\n","    return -1"],"metadata":{"id":"509NcnQNUnDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getIndicePerturbacao(_id_documento):\n","\n","  id_documento_perturbado = \"\"\n","\n","  # Verifica o tipo do documento\n","  #if int(_id_documento)/2 == 1:\n","  if \"_pert_\" in _id_documento:\n","    # Documento perturbado\n","    id_documento_perturbado = _id_documento\n","\n","  else:\n","    # Pega o primeiro documento perturbado para localizar posição\n","    id_documento_perturbado = _id_documento + \"_pert_0\"\n","    #id_documento_perturbado = str(int(_id_documento) + 1)\n","\n","  # Retorna o índice\n","  index_perturbacao = getIndicePalavraPerturbada(id_documento_perturbado)\n","\n","  return index_perturbacao"],"metadata":{"id":"20I7k_2M-rin"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuM-kq1upR3M"},"source":["## 5.2 Gera os arquivos para o Embedding Projector"]},{"cell_type":"markdown","metadata":{"id":"8FRocbK4_wTk"},"source":["### 5.2.1 Cria o diretório para os arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaGALkXc_zLl"},"outputs":[],"source":["# Importando as bibliotecas.\n","import os\n","\n","# Cria o diretório para receber os arquivos Originais e Permutados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL + \"projector\"\n","\n","if not os.path.exists(dirbase):\n","    # Cria o diretório\n","    os.makedirs(dirbase)\n","    logging.info(\"Diretório criado: {}\".format(dirbase))\n","else:\n","    logging.info(\"Diretório já existe: {}\".format(dirbase))"]},{"cell_type":"markdown","metadata":{"id":"eFugAWlm1VfN"},"source":["### 5.2.2 Gera os embeddings dos documentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maBbfHFj1VfN"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","\n","lista_embeddings = []\n","lista_documentos = []\n","lista_documentos_tokenizado = []\n","lista_documentos_tokenizado_oov = []\n","lista_documentos_pos = []\n","lista_documentos_classe = []\n","lista_documentos_id = []\n","lista_documentos_origem = []\n","\n","maior_sequencia = 0\n","\n","total_tokens = 0\n","\n","if CLASSE_DOCUMENTO != 2:\n","  documentos = lista_documentos_agrupados.loc[lista_documentos_agrupados['classe'] == CLASSE_DOCUMENTO]\n","else:\n","  documentos = lista_documentos_agrupados\n","\n","# Barra de progresso dos documentos\n","documentos_bar = tqdm_notebook(documentos.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(documentos))\n","\n","# Percorre os documentos\n","for i, linha_documento in documentos_bar:\n","\n","    # Recupera o id do documento\n","    id_documento = linha_documento[0]\n","    # print(\"id_documento:\",id_documento)\n","    # print(\"linha_documento['documento']:\", linha_documento['documento'])\n","\n","    # Recupera a classe documento (1-original 0-perturbado)\n","    classe = linha_documento['classe']\n","    #print(\"classe:\",classe)\n","\n","    # Localiza a POSTagging do documento agrupado\n","    lista_pos_documento = lista_documentos_agrupados_pos_indexado.loc[id_documento][0]\n","    # print(\"lista_pos_documento:\",lista_pos_documento)\n","    # print(\"len(lista_pos_documento):\",len(lista_pos_documento))\n","\n","    # Troca o documento por uma versão da concatenação das palavras geradas pelo spaCy\n","    # Percorre a lista_pos concatenando a posição 0 dos tokens\n","    documento_concatenado = \" \".join(concatenaListas(lista_pos_documento, pos=0))\n","    # print(\"documento_concatenado:\", documento_concatenado)\n","    documento = documento_concatenado\n","\n","    if CLASSE_DOCUMENTO != 1:\n","      # Recupera a posição do traço no id do arquivo\n","      traco_ix = id_documento.find(\"_\")\n","      if traco_ix != -1:\n","        # Recupera o id da perturbacao até a posição do traço até o fim\n","        id_perturbacao = id_documento[:traco_ix]\n","      else:\n","        id_perturbacao = id_documento\n","\n","    if POOLING_TOKENS == 0:\n","\n","        # Recupera os embeddings\n","        if ESTRATEGIA_EMBEDDING == 1:\n","          # Gera embeddings da última camada do BERT\n","          token_embeddings, documento_tokenizado =  getEmbeddingsUltimaCamada(documento, model, tokenizer)\n","        else:\n","          # Gera embeddings concatenando as 4 últimas camadas do BERT\n","          token_embeddings, documento_tokenizado = getEmbeddingsConcat4UltimasCamadas(documento, model, tokenizer)\n","\n","        # Guarda o maior tamanho de documento\n","        if len(documento_tokenizado) > maior_sequencia:\n","            maior_sequencia =  len(documento_tokenizado)\n","\n","        # Guarda o total de tokens dos documentos\n","        total_tokens = total_tokens + len(documento_tokenizado)\n","\n","        # Guarda os embeddings e o documento tokenizado\n","        lista_embeddings.append(token_embeddings)\n","        lista_documentos.append(documento)\n","        lista_documentos_tokenizado.append(documento_tokenizado)\n","        lista_documentos_classe.append(classe)\n","        lista_documentos_id.append(id_documento)\n","        if CLASSE_DOCUMENTO != 1:\n","          lista_documentos_origem.append(id_perturbacao)\n","\n","    else:\n","        # Recupera os embeddings\n","        if ESTRATEGIA_EMBEDDING == 1:\n","          # Gera embeddings da última camada do BERT\n","          token_embeddings, documento_tokenizado =  getEmbeddingsUltimaCamada(documento, model, tokenizer)\n","        else:\n","          # Gera embeddings concatenando as 4 últimas camadas do BERT\n","          token_embeddings, documento_tokenizado = getEmbeddingsConcat4UltimasCamadas(documento, model, tokenizer)\n","\n","        # Combina os embeddings de palavras fora do vocabulário do BERT\n","        listaTokens, listaPOS, lista_tokens_OOV, listaEmbeddingsMEAN, listaEmbeddingsMAX =  getTokensEmbeddingsPOSSentenca(token_embeddings[1:-1],\n","                                                                                                        documento_tokenizado[1:-1],\n","                                                                                                        documento)\n","\n","        # Guarda o maior tamanho de documento\n","        if len(listaTokens) > maior_sequencia:\n","            maior_sequencia =  len(listaTokens)\n","\n","        # Guarda o total de tokens dos documentos\n","        total_tokens = total_tokens + len(listaTokens)\n","\n","        # Guarda os embeddings e os os outros dados do documento\n","        lista_embeddings.append(listaEmbeddingsMEAN)\n","        lista_documentos.append(documento)\n","        lista_documentos_tokenizado.append(listaTokens)\n","        lista_documentos_tokenizado_oov.append(lista_tokens_OOV)\n","        lista_documentos_pos.append(listaPOS)\n","        lista_documentos_classe.append(classe)\n","        lista_documentos_id.append(id_documento)\n","        if CLASSE_DOCUMENTO != 1:\n","          lista_documentos_origem.append(id_perturbacao)"]},{"cell_type":"markdown","metadata":{"id":"VOyrUs1d1VfO"},"source":["Mostra um documento processado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLp3DWq61VfO"},"outputs":[],"source":["print(len(lista_embeddings[0]))\n","print(lista_documentos_tokenizado[0])\n","print(lista_documentos_classe[0])"]},{"cell_type":"markdown","metadata":{"id":"apK3F5W41VfO"},"source":["Quantidade de tokens nos documentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQvbl4XE1VfP"},"outputs":[],"source":["print(\"Quantidade de tokens:\", total_tokens)"]},{"cell_type":"markdown","metadata":{"id":"qvVjozxA1VfP"},"source":["Maior tamanho  de documento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gvZGxIp1VfP"},"outputs":[],"source":["print(\"max_seq_length:\", maior_sequencia)"]},{"cell_type":"markdown","metadata":{"id":"za-9WAYO1VfQ"},"source":["### 5.2.3 Gera os arquivos para o Embedding Projector"]},{"cell_type":"markdown","source":["Gera o sufixo do nome do arquivo"],"metadata":{"id":"R9zqngHokA0-"}},{"cell_type":"code","source":["def getSufixoNomeArquivo():\n","\n","  sufixo_arquivo = \"_\"\n","\n","  # Documento perturbados\n","  if CLASSE_DOCUMENTO == 0:\n","      sufixo_arquivo = sufixo_arquivo + \"PERTDO\" + \"_P\" + str(DOCUMENTOS_PERTURBADOS)\n","  else:\n","    # Documento originais\n","    if CLASSE_DOCUMENTO == 1:\n","      sufixo_arquivo = sufixo_arquivo + \"DO\"\n","    else:\n","      # Documento originais e perturbados\n","      if CLASSE_DOCUMENTO == 2:\n","        sufixo_arquivo = sufixo_arquivo + \"DO_PERTDO\"  + \"_P\" + str(DOCUMENTOS_PERTURBADOS) + \"_CLASSE\"\n","\n","  # Sem pooling dos tokens\n","  if POOLING_TOKENS == 0:\n","    # Tamanho dos embeddings\n","    sufixo_arquivo = sufixo_arquivo + \"_\" + str(lista_embeddings[0].size()[1]) + TAMANHO_BERT\n","\n","    # Não possui o prefixo pooling\n","  else:\n","    # Com pooling dos tokens\n","    if POOLING_TOKENS == 1:\n","      sufixo_arquivo = sufixo_arquivo + \"_\" + str(lista_embeddings[0][0].size()[0]) + TAMANHO_BERT\n","\n","      # Adiciona o prefixo\n","      sufixo_arquivo = sufixo_arquivo + \"_POOL\"\n","\n","  return sufixo_arquivo"],"metadata":{"id":"RNiX8OU7K4-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCR6vo371VfQ"},"source":["Arquivos com os valores dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbILMYpG1VfQ"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","import csv\n","\n","# Recupera o sufixo do nome do arquivo\n","sufixo_arquivo = getSufixoNomeArquivo()\n","#print(\"sufixo_arquivo:\", sufixo_arquivo)\n","\n","NOME_ARQUIVO_RECORD =  DIRETORIO_LOCAL + \"projector/\" + \"DO\" + FILTRO_DO[0] + \"_records_token\" + sufixo_arquivo + \".tsv\"\n","\n","# Abre o arquivo\n","with open(NOME_ARQUIVO_RECORD, 'w', encoding='utf8') as tsvfile:\n","  # Cria um arquivo separado por tab\n","    writer = csv.writer(tsvfile, delimiter='\\t')\n","\n","    # Barra de progresso dos embedings\n","    lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","    # Percorre os embeddings\n","    for i, documento_embedding in lista_embeddings_bar:\n","\n","      if POOLING_TOKENS == 0:\n","        # Converte os tensores em numpy array\n","        documento_embedding_np =  documento_embedding.numpy()\n","\n","        # Qtde de tokens do documento\n","        length = len(lista_documentos_tokenizado[i])\n","\n","        # Escreve no arquivo os embeddings do documento\n","        writer.writerows(documento_embedding_np[:length])\n","\n","      else:\n","        # Converte os tensores em numpy array\n","        documento_embedding_np = []\n","        for linha in documento_embedding:\n","            novo = linha.numpy()\n","            documento_embedding_np.append(novo)\n","\n","        # Qtde de tokens do documento\n","        length = len(lista_documentos_tokenizado[i])\n","\n","        # Escreve no arquivo os embeddings do documento\n","        writer.writerows(documento_embedding_np[:length])"]},{"cell_type":"markdown","metadata":{"id":"mb580LQT1VfR"},"source":["Arquivo com os metadados dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XskFjQ5v1VfR"},"outputs":[],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","import csv\n","\n","# Recupera o sufixo do nome do arquivo\n","sufixo_arquivo = getSufixoNomeArquivo()\n","#print(\"sufixo_arquivo:\", sufixo_arquivo)\n","\n","NOME_ARQUIVO_META =  DIRETORIO_LOCAL + \"projector/\" + \"DO\" + FILTRO_DO[0] + \"_meta_token\" + sufixo_arquivo + \".tsv\"\n","\n","# Abre o arquivo\n","with open(NOME_ARQUIVO_META, 'w', encoding='utf8') as tsvfile:\n","    # Define o escritor do arquivo\n","    writer = csv.writer(tsvfile, delimiter='\\t')\n","\n","    # Cabeçalho do arquivo\n","    # Sem pooling\n","    if POOLING_TOKENS == 0:\n","\n","      # Sem classe\n","      if CLASSE_DOCUMENTO != 2:\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Index\", \"__next__\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                  # Transforma o conta_proximo que é o indicador da sequência em string\n","                  proximo = str(conta_proximo)\n","\n","                  # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","                  if j  == length-1:\n","                    proximo = \"\"\n","\n","                  # Incrementa o contador da sequência\n","                  conta_proximo = conta_proximo + 1\n","\n","                  # Monta o registro a ser salvo\n","                  s = [lista_documentos_tokenizado[i][j],\n","                       lista_documentos_id[i],\n","                       lista_documentos_origem[i],\n","                       str(j),\n","                       proximo,\n","                       lista_documentos[i]]\n","\n","                  # Escreve o registro no arquivo\n","                  writer.writerow(s)\n","        else:\n","          # Sem o link de ligação dos tokens da sentença\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Index\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     str(j),\n","                     lista_documentos[i]]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","      else:\n","        # Com classe\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"__next__\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                  # Transforma o conta_proximo que é o indicador da sequência em string\n","                  proximo = str(conta_proximo)\n","\n","                  # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","                  if j  == length-1:\n","                    proximo = \"\"\n","\n","                  # Incrementa o contador da sequência\n","                  conta_proximo = conta_proximo + 1\n","\n","                  # Identifica a posição da palavra selecionada para perturbação\n","                  perturbada = \"0\"\n","                  if indice_palavra_perturbada == j:\n","                    perturbada = \"1\"\n","\n","                  # Monta o registro a ser salvo\n","                  s = [lista_documentos_tokenizado[i][j],\n","                       lista_documentos_id[i],\n","                       lista_documentos_origem[i],\n","                       lista_documentos_classe[i],\n","                       perturbada,\n","                       str(j),\n","                       proximo,\n","                       lista_documentos[i]\n","                      ]\n","\n","                  # Escreve o registro no arquivo\n","                  writer.writerow(s)\n","\n","        else:\n","          # Com Classe\n","          #Sem o link\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Identifica a posição da palavra selecionada para perturbação\n","                perturbada = \"0\"\n","                if indice_palavra_perturbada == j:\n","                  perturbada = \"1\"\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     lista_documentos_classe[i],\n","                     perturbada,\n","                     str(j),\n","                     lista_documentos[i]]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","    else:\n","      # Com polling\n","      # Sem classe\n","      if CLASSE_DOCUMENTO != 2:\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Index\", \"__next__\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Transforma o conta_proximo que é o indicador da sequência em string\n","                proximo = str(conta_proximo)\n","\n","                # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","                if j  == length-1:\n","                  proximo = \"\"\n","\n","                # Incrementa o contador da sequência\n","                conta_proximo = conta_proximo + 1\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_pos[i][j],\n","                     lista_documentos_tokenizado_oov[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     str(j),\n","                     proximo,\n","                     lista_documentos[i]]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","        else:\n","            # Sem link de próximo\n","\n","            # Escreve o cabeçalho do arquivo\n","            writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Index\", \"Sentença\"])\n","\n","            # Barra de progresso dos embedings\n","            lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","            # Contador da sequência\n","            conta_proximo = 1\n","\n","            # Percorre os embeddings\n","            for i, documento_embedding in lista_embeddings_bar:\n","\n","                # Qtde de tokens do documento\n","                length = len(lista_documentos_tokenizado[i])\n","\n","                # Escreve a palavra e sua sentença\n","                for j in range(length):\n","\n","                  # Monta o registro a ser salvo\n","                  s = [lista_documentos_tokenizado[i][j],\n","                       lista_documentos_pos[i][j],\n","                       lista_documentos_tokenizado_oov[i][j],\n","                       lista_documentos_id[i],\n","                       lista_documentos_origem[i],\n","                       str(j),\n","                       lista_documentos[i]\n","                       ]\n","\n","                  # Escreve o registro no arquivo\n","                  writer.writerow(s)\n","      else:\n","        # Com classe\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"__next__\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Transforma o conta_proximo que é o indicador da sequência em string\n","                proximo = str(conta_proximo)\n","\n","                # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","                if j  == length-1:\n","                  proximo = \"\"\n","\n","                # Incrementa o contador da sequência\n","                conta_proximo = conta_proximo + 1\n","\n","                # Identifica a posição da palavra selecionada para perturbação\n","                perturbada = \"0\"\n","                if indice_palavra_perturbada == j:\n","                  perturbada = \"1\"\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_pos[i][j],\n","                     lista_documentos_tokenizado_oov[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     lista_documentos_classe[i],\n","                     perturbada,\n","                     str(j),\n","                     proximo,\n","                     lista_documentos[i]\n","                    ]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","        else:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"Sentença\" ])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Identifica a posição da palavra selecionada para perturbação\n","              perturbada = \"0\"\n","              if indice_palavra_perturbada == j:\n","                perturbada = \"1\"\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_pos[i][j],\n","                     lista_documentos_tokenizado_oov[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     lista_documentos_classe[i],\n","                     perturbada,\n","                     str(j),\n","                     lista_documentos[i],\n","                    ]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)"]},{"cell_type":"markdown","metadata":{"id":"S23zNSjM1VfR"},"source":["Faça o download dos arquivos **records_token_4096.tsv** e **meta_token_4096.tsv** e carregue em https://projector.tensorflow.org/ na opção load.\n","\n","Faça o download dos arquivos gerados pelo notebook clicando na lateral esquerda no ícone \"Arquivos\".\n","\n","Carrega os arquivos na ferramenta através do link \"Load\". Na opção existe um link botão para carregar o arquivo dos embeddings e um outro botão para carregar os metadados.\n","\n","Você também pode utilizar um link a um arquivo de configuração config.json com a referência aos arquivos em algum repositório publico na internet, por exemplo github ou gist\n","\n","Aqui um exemplo.\n","\n","https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/osmarbraz/cohebertv1projecao/main/config.json\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t56ovS3Kt-W0"},"source":["### 5.3.4 Compacta e copia o arquivo do projetor para uma pasta do GoogleDrive"]},{"cell_type":"markdown","metadata":{"id":"MnElyyGbcmlV"},"source":["Compacta o arquivo gerado da comparação para facilitar o envio para o GoogleDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGK7bu_P2F-m"},"outputs":[],"source":["# Nome do arquivo\n","NOME_ARQUIVO_PROJECTOR_COMPACTADO = \"projector.zip\""]},{"cell_type":"markdown","metadata":{"id":"5vZq_1sEtsB0"},"source":["Compacta os arquivos.\n","\n","Usa o zip para compactar:\n","*   `-r` Compacta o diretório\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NunMOJWR2O8H"},"outputs":[],"source":["!zip -r -o -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PROJECTOR_COMPACTADO\" \"$DIRETORIO_LOCAL\"\"/projector/\""]},{"cell_type":"markdown","metadata":{"id":"sw3p4ydkt-W-"},"source":["Copia o arquivo compactado para o GoogleDrive\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5c8sv10t-W_"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","    # Copia o arquivo original\n","    # !cp \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PROJECTOR_COMPACTADO\" \"$DIRETORIO_DRIVE\"\n","\n","    logging.info(\"Terminei a cópia\")"]},{"cell_type":"markdown","metadata":{"id":"NYBu_-xVdHe4"},"source":["## 5.3 Projeção dos embeddings"]},{"cell_type":"markdown","metadata":{"id":"68qT3LkGtRgg"},"source":["### Configuração"]},{"cell_type":"markdown","metadata":{"id":"PHLxY3piwAB-"},"source":["Verifica a versão do tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5A6v8akfdG6j"},"outputs":[],"source":["try:\n","  # %tensorflow_version só existe no Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"Xh5VnBHxtUS7"},"source":["Importa a biblioteca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nABca5ukdPE_"},"outputs":[],"source":["# Importa de biblioteca\n","from tensorboard.plugins import projector"]},{"cell_type":"markdown","metadata":{"id":"ix1XJaMqtWqv"},"source":["### Configura o diretório dos logs e arquivos de configuração\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj1SzSWAfZIJ"},"outputs":[],"source":["# Configure um diretório de logs\n","log_dir =\"/content/projector/\"\n","if not os.path.exists(log_dir):\n","    os.makedirs(log_dir)"]},{"cell_type":"markdown","metadata":{"id":"bI0SITtUsZMx"},"source":["### Cria os arquivos de configuração dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U9hYPvidYls"},"outputs":[],"source":["# Configura o projetor\n","config = projector.ProjectorConfig()\n","\n","# Configuração do primeiro conjunto de embeddings sem pooling\n","embedding = config.embeddings.add()\n","# Nome do tensor\n","embedding.tensor_name = \"Cohebert: concat 4 últimas camadas pool BERTimbau large\"\n","# Caminho para os metadados\n","embedding.metadata_path = NOME_ARQUIVO_META\n","# Caminho para os tensores\n","embedding.tensor_path = NOME_ARQUIVO_RECORD\n","# Salva o arquivo de configuração\n","projector.visualize_embeddings(log_dir, config)"]},{"cell_type":"markdown","metadata":{"id":"SkrnWNMLvpCq"},"source":["### Mata o processo\n","\n","Se executar novamente o notebook é necessário matar o processo do tensorprojector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgTL4Ns2gINB"},"outputs":[],"source":["# Mata o processo do tensorboard\n","#!kill 407"]},{"cell_type":"markdown","metadata":{"id":"xlXJ7-Uytj3f"},"source":["### Visualizando a projeção\n","\n","Na caixa de seleção selecione \"PROJECTOR\" no lugar de \"INACTIVE\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoAAOlWXdf0t"},"outputs":[],"source":["# Agora execute o tensorboard nos dados de log que acabamos de salvar.\n","%tensorboard --logdir /content/projector"]}],"metadata":{"colab":{"collapsed_sections":["pnY7O9zb8n8Z"],"provenance":[{"file_id":"1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU","timestamp":1585340447636},{"file_id":"1FsBCkREOaDopLF3PIYUuQxLR8wRfjQY1","timestamp":1559844903389},{"file_id":"1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y","timestamp":1554843110227}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}