{"cells":[{"cell_type":"markdown","metadata":{"id":"78HE8FLsKN9Q"},"source":["#Mensurando e Classificando coerência em textos usando Llama v2.0 7B 8bit usando Langchain e Transformers by HuggingFace\n","\n","Pré-requisitos:\n","- Lhama v2.0 não está acessível abertamente e requer solicitação  de acesso. Faça o cadastro no site do https://huggingface.co/join. Depois do login, gere um token de acesso no link https://huggingface.co/settings/tokens.\n","- Configure o notebook para usar GPU- Acesse o menu 'Ambiente de Execução -> Alterar o tipo do ambiente de execução -> Acelerador de hardware -> T4 GPU\n","\n","\n","**Referências**\n","https://medium.com/the-techlife/using-huggingface-openai-and-cohere-models-with-langchain-db57af14ac5b\n","\n","\n","**Notebook de referência:**\n","\n","https://github.com/guardiaum/tutorial-sbbd2023/blob/main/Prompt_Engineering.ipynb\n","\n","\n","**Lista dos modelos:**\n","\n","https://huggingface.co/models\n","\n","\n","**Artigos referências:**\n","\n","https://dev.to/nithinibhandari1999/how-to-run-llama-2-on-your-local-computer-42g1\n","\n","\n","**Link biblioteca Huggingface:**\n","\n","https://github.com/huggingface/transformers\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xyxb5Px3p1-e"},"source":["# 1 Preparação do ambiente\n","Preparação do ambiente para execução do exemplo."]},{"cell_type":"markdown","metadata":{"id":"PKUr9Vk4BNLC"},"source":["##  1.1 Tempo inicial de processamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXclHCRQBSF2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719343959714,"user_tz":180,"elapsed":501,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"f4280b7e-fc39-4421-e71b-6c69b493d989"},"outputs":[{"output_type":"stream","name":"stdout","text":["25/06/2024 19:32:39\n"]}],"source":["# Import das bibliotecas.\n","import time\n","from datetime import datetime\n","\n","# Marca o tempo de início do processamento\n","inicio_processamento = time.time()\n","\n","data_e_hora_atuais = datetime.now()\n","data_e_hora_em_texto = data_e_hora_atuais.strftime('%d/%m/%Y %H:%M:%S')\n","\n","print(data_e_hora_em_texto)"]},{"cell_type":"markdown","metadata":{"id":"JKmhxcvIfbG2"},"source":["## 1.2 Funções auxiliares"]},{"cell_type":"markdown","metadata":{"id":"603LYIYKBmq5"},"source":["Função auxiliar para formatar o tempo como `hh: mm: ss`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guy6B4whsZFR"},"outputs":[],"source":["# Import das bibliotecas.\n","import time\n","import datetime\n","\n","def formataTempo(tempo):\n","    \"\"\"\n","      Pega a tempo em segundos e retorna uma string hh:mm:ss\n","    \"\"\"\n","    # Arredonda para o segundo mais próximo.\n","    tempo_arredondado = int(round((tempo)))\n","\n","    # Formata como hh:mm:ss\n","    return str(datetime.timedelta(seconds=tempo_arredondado))"]},{"cell_type":"markdown","metadata":{"id":"V1vu-ch8yT5R"},"source":["Imprime linhas menores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BKQZtF9yUBs"},"outputs":[],"source":["def print_linhas_menores(texto, tamanho=120):\n","  for i in range(0, len(texto), tamanho):\n","    print(texto[i:i+tamanho])"]},{"cell_type":"markdown","metadata":{"id":"iAPVtRXQqDim"},"source":["## 1.3 Tratamento de logs\n","\n","Método para tratamento dos logs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcopxbGZqDip"},"outputs":[],"source":["# Biblioteca de logging\n","import logging\n","\n","# Formatando a mensagem de logging\n","logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"_GjYtXcMnSAe"},"source":["## 1.4 Identificando o ambiente Colab\n","\n","Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMiH0E3OnRa1"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory\n","import sys\n","\n","# Retorna true ou false se estiver no Google Colaboratory\n","IN_COLAB = \"google.colab\" in sys.modules"]},{"cell_type":"markdown","metadata":{"id":"RinFHFesVKis"},"source":["## 1.5 Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"MPngEboiVbfi"},"source":["Usando Colab GPU para Treinamento\n"]},{"cell_type":"markdown","metadata":{"id":"EjWE6WlvVbfj"},"source":["Uma GPU pode ser adicionada acessando o menu e selecionando:\n","\n","`Edit -> Notebook Settings -> Hardware accelerator -> (GPU)`\n","\n","Em seguida, execute a célula a seguir para confirmar que a GPU foi detectada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtaYZmc3Vbfj"},"outputs":[],"source":["# Import das bibliotecas.\n","import tensorflow as tf\n","\n","# Recupera o nome do dispositido da GPU.\n","device_name = tf.test.gpu_device_name()\n","\n","# O nome do dispositivo deve ser parecido com o seguinte:\n","if device_name == \"/device:GPU:0\":\n","    logging.info(\"Encontrei GPU em: {}\".format(device_name))\n","else:\n","    logging.info(\"Dispositivo GPU não encontrado\")\n","    #raise SystemError(\"Dispositivo GPU não encontrado\")"]},{"cell_type":"markdown","metadata":{"id":"iYRrUo2XWa8G"},"source":["Nome da GPU\n","\n","Para que a torch use a GPU, precisamos identificar e especificar a GPU como o dispositivo. Posteriormente, em nosso ciclo de treinamento, carregaremos dados no dispositivo.\n","\n","Vale a pena observar qual GPU você recebeu. A GPU Tesla P100 é muito mais rápido que as outras GPUs, abaixo uma lista ordenada:\n","- 1o Tesla P100\n","- 2o Tesla T4\n","- 3o Tesla P4 (Não tem memória para execução 4 x 8, somente 2 x 4)\n","- 4o Tesla K80 (Não tem memória para execução 4 x 8, somente 2 x 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrjqDO6nWa8J"},"outputs":[],"source":["# Import das bibliotecas.\n","import torch\n","\n","def getDeviceGPU():\n","    \"\"\"\n","      Retorna um dispositivo de GPU se disponível ou CPU.\n","\n","      Retorno:\n","        `device` - Um device de GPU ou CPU.\n","    \"\"\"\n","\n","    # Se existe GPU disponível.\n","    if torch.cuda.is_available():\n","\n","        # Diz ao PyTorch para usar GPU.\n","        device = torch.device(\"cuda\")\n","\n","        logging.info(\"Existem {} GPU(s) disponíveis.\".format(torch.cuda.device_count()))\n","        logging.info(\"Iremos usar a GPU: {}.\".format(torch.cuda.get_device_name(0)))\n","\n","    # Se não.\n","    else:\n","        logging.info(\"Sem GPU disponível, usando CPU.\")\n","        device = torch.device(\"cpu\")\n","\n","    return device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChDxmtXsKwjf"},"outputs":[],"source":["# Recupera o device com GPU ou CPU\n","device = getDeviceGPU()"]},{"cell_type":"markdown","source":["GPU"],"metadata":{"id":"OdT-BxPocGTF"}},{"cell_type":"code","source":["# Import de biblioteca\n","import torch\n","\n","GPU_ENABLE = torch.cuda.is_available()\n","\n","if GPU_ENABLE:\n","    print(\"GPU está disponível.\")\n","else:\n","    print(\"GPU não está disponível.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7BarjtOb1Zw","executionInfo":{"status":"ok","timestamp":1719343960064,"user_tz":180,"elapsed":6,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"c4be5e68-cf3d-47f1-f416-52515b69c014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU está disponível.\n"]}]},{"cell_type":"markdown","metadata":{"id":"fGf59D0yVNx9"},"source":["Memória\n","\n","Memória disponível no ambiente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iC5-pSAVh7_"},"outputs":[],"source":["# Importando as bibliotecas.\n","from psutil import virtual_memory\n","\n","ram_gb = virtual_memory().total / 1e9\n","logging.info(\"Seu ambiente de execução tem {: .1f} gigabytes de RAM disponível\\n\".format(ram_gb))\n","\n","if ram_gb < 20:\n","  logging.info(\"Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \\\"Alterar tipo de tempo de execução\\\"\")\n","  logging.info(\"e selecione High-RAM. Então, execute novamente está célula\")\n","else:\n","  logging.info(\"Você está usando um ambiente de execução de memória RAM alta!\")"]},{"cell_type":"markdown","source":["Versão Python"],"metadata":{"id":"B-xSroBtxPL2"}},{"cell_type":"code","source":["# Biblioteca do sistema\n","import sys\n","\n","print(\"Versão Python:\", sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Xu2haQbxRTc","executionInfo":{"status":"ok","timestamp":1719343960064,"user_tz":180,"elapsed":4,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"b3b19810-d030-4d3e-ec65-c532aaf6e7be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Versão Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["## 1.6 Instalação das bibliotecas"]},{"cell_type":"markdown","metadata":{"id":"PGrlTKgSLdNj"},"source":["Instala langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppVeArJcLdVb"},"outputs":[],"source":["!pip install langchain==0.1.16"]},{"cell_type":"markdown","metadata":{"id":"zp0jVfo3QM3h"},"source":["O bitsandbytes é um wrapper leve em torno de funções personalizadas CUDA, em particular otimizadores de 8 bits, multiplicação de matrizes (LLM.int8()) e funções de quantização. É uma dependência do accelerate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12GE2W3fQM_n"},"outputs":[],"source":["!pip install bitsandbytes==0.43.1"]},{"cell_type":"markdown","metadata":{"id":"N7wU6vuyAuPd"},"source":["Accelerate é uma biblioteca que permite que o mesmo código PyTorch seja executado em qualquer configuração distribuída adicionando apenas quatro linhas de código. Otimiza as operações do PyTorch, especialmente na GPU.\n","\n","https://pypi.org/project/accelerate/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTMID1rZAvx7"},"outputs":[],"source":["!pip install accelerate==0.29.3"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["A Biblioteca A Biblioteca Transformers fornece APIs e ferramentas para baixar e treinar facilmente modelos pré-treinados de última geração para Processamento de linguagem natural, Visão computacional, Áudio, etc.\n","\n","Fornece uma maneira direta de usar modelos pré-treinados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RfUN_KolV-f"},"outputs":[],"source":["!pip install -U transformers==4.40.0"]},{"cell_type":"markdown","metadata":{"id":"OlrWrRP02tuZ"},"source":["A Biblioteca huggingface-cli fornece vários comandos para interagir com o Hugging Face Hub a partir da linha de comando. Um desses comandos é o login, que permite aos usuários se autenticarem no Hub usando suas credenciais."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQxtD3Zk14ov"},"outputs":[],"source":["#!pip install huggingface_hub==0.18.0"]},{"cell_type":"markdown","source":["# 2 Parametrização"],"metadata":{"id":"-d1LVpDyY4vT"}},{"cell_type":"markdown","metadata":{"id":"ifrYNTwGwKal"},"source":["## Gerais"]},{"cell_type":"markdown","metadata":{"id":"lBOzL86X6kjM"},"source":["Define o nome do modelo a ser carregado\n","Lista dos modelos:\n","  - https://huggingface.co/meta-llama/Llama-2-7b-hf\n","  - https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n","  - https://huggingface.co/meta-llama/Llama-2-13b-hf\n","  - https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n","  - https://huggingface.co/meta-llama/Llama-2-70b-hf\n","  - https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zOnSymM6kjM"},"outputs":[],"source":["#nome_modelo = \"meta-llama/Llama-2-7b-hf\"\n","nome_modelo = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","#nome_modelo = \"meta-llama/Llama-2-13b-hf\"\n","# nome_modelo = \"meta-llama/Llama-2-13b-chat-hf\"\n","\n","# Não roda pois exige GPU A100 e mais espaço em disco\n","#nome_modelo = \"meta-llama/Llama-2-70b-hf\"\n","# nome_modelo = \"meta-llama/Llama-2-70b-chat-hf\""]},{"cell_type":"markdown","metadata":{"id":"NFRSYoCArrQ-"},"source":["Login no huggingface\n","\n","- Lhama 2 não está acessível abertamente e requer solicitação  de acesso. Faça o cadastro no site do https://huggingface.co/join. Depois do login, gere um token de acesso no link https://huggingface.co/settings/tokens.\n","\n","Insira o token quando solicitado e depois digite Y para adicionar as credenciais."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bkqIoNU18UH"},"outputs":[],"source":["# !huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"ACJuj9wB9kjZ"},"source":["Se o seu notebook não for público e não desejar incluir o **Access Token do HuggingFace** toda vez que for executar o notebook preencha a variável '<valor_do_acess_token>'.\n","\n","Se for público crie a variável 'HF_TOKEN' com o valor do **Access Token do HuggingFace**. Abra o Google Colab e navegue até a nova seção 'Secrets' na barra lateral e adicione a variável."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRVr7uqp9Ubk"},"outputs":[],"source":["from huggingface_hub.hf_api import HfFolder\n","\n","if IN_COLAB:\n","    from google.colab import userdata\n","\n","    ACCESS_TOKEN  = userdata.get('HF_TOKEN')\n","\n","    HfFolder.save_token(ACCESS_TOKEN)\n","\n","else:\n","    ACESS_TOKEN = \"<valor_do_acess_token\"\n","\n","    HfFolder.save_token(ACCESS_TOKEN)"]},{"cell_type":"markdown","metadata":{"id":"LIzrrJLw9oQd"},"source":["Mostrando o usuário conectado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLrSstlxR_kq"},"outputs":[],"source":["# !huggingface-cli whoami"]},{"cell_type":"markdown","metadata":{"id":"mhByVujAwNAU"},"source":["## Específicos"]},{"cell_type":"markdown","metadata":{"id":"Mhkc9sW21zV7"},"source":["Parâmetros do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ15-ylRRRdD"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ecwtQDArKvZn"},"source":["## Nome do diretório dos arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtNygH9qKvmp"},"outputs":[],"source":["# Diretório do cohebert\n","DIRETORIO_COHEBERT = \"COHQUAD_INIT_PTBR\""]},{"cell_type":"markdown","metadata":{"id":"SUxlx7Sx4yxj"},"source":["## Define o caminho para os arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gQpxAO74yxj"},"outputs":[],"source":["# Diretório local para os arquivos pré-processados\n","DIRETORIO_LOCAL = \"/content/\" + DIRETORIO_COHEBERT + \"/\"\n","\n","# Diretório no google drive com os arquivos pré-processados\n","DIRETORIO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/\""]},{"cell_type":"markdown","metadata":{"id":"Bcpd9t9PpkrX"},"source":["# 3 - Carregando o LLM\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OzWcQNSORrYC"},"source":["## 3.1 - Carrega o tokenizador do LLM\n","\n","Carregando o **tokenizador** da comunidade."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlSM1VufRw5B"},"outputs":[],"source":["# Importando as bibliotecas do Tokenizador\n","from transformers import AutoTokenizer\n","\n","# Carregando o Tokenizador da comunidade\n","print('Carregando o tokenizador ' + nome_modelo + ' da comunidade...')\n","\n","tokenizer = AutoTokenizer.from_pretrained(nome_modelo)"]},{"cell_type":"markdown","metadata":{"id":"pNhZxBfM0LEe"},"source":["Tamanho do vocabulário"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzgbIOUI0LEf"},"outputs":[],"source":["print(len(tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"UhNMEhN9BHuc"},"source":["## 3.2 - Carregando o LLM\n","\n","Carregando o **LLM** da comunidade HuggingFace.\n","\n","Parametrização do from_pretrained\n","https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"]},{"cell_type":"markdown","metadata":{"id":"oj6_jlk35AUI"},"source":["Carregamento LLama 2 com 4 bits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Kx5Ed64YlV-"},"outputs":[],"source":["# # Importando as bibliotecas do Modelo\n","# from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n","# import torch\n","# import time\n","\n","# # Guarda o tempo de início do carregamento do modelo\n","# tempo_inicio = time.time()\n","\n","# # Carregando o Modelo da comunidade\n","# print('Carregando o modelo ' + nome_modelo + ' da comunidade...')\n","\n","# # BitsAndBytes é um framework com funções customizadas para\n","# # otimização com precisão 8-bit, multiplicações de matrizes e funções de quantização\n","# quantization_config = BitsAndBytesConfig(\n","#    load_in_4bit=True, # Habilita a quantização de 4 bits para comprimir o modelo\n","#    bnb_4bit_quant_type=\"nf4\", # Define o tipo de dados de quantização nas camadas (`fp4` e `nf4`).\n","#    bnb_4bit_use_double_quant=True, # Quantização aninhada, onde as constantes de quantização da primeira quantização são quantizadas novamente.\n","#    bnb_4bit_compute_dtype=torch.bfloat16 # # Os gradientes dos pesos são computados em 16-bit. Define o tipo computacional que pode ser diferente do tempo de entrada. Por exemplo, as entradas podem ser fp32, mas a computação pode ser definida como bf16 para acelerações.\n","# )\n","\n","# # Se GPU Disponível\n","# if GPU_ENABLE:\n","#   # Carrega o modelo com a otimização BitsAndBytesConfig\n","#   print (\"Carregando o LLM com GPU\")\n","#   model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n","#                                              #torch_dtype=torch.float16, #default\n","#                                              trust_remote_code=True, # Carrega de um repositório confiável\n","#                                              quantization_config=quantization_config,\n","#                                              device_map=\"auto\"\n","#                                              )\n","\n","# else:\n","#   # Carrega o modelo sem a otimização BitsAndBytesConfig\n","#   print (\"Carregando o LLM sem GPU\")\n","#   model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n","#                                              #torch_dtype=torch.float16, #default\n","#                                              trust_remote_code=True, # Carrega de um repositório confiável\n","#                                              device_map=\"auto\"\n","#                                              )\n","\n","# # Coloca o modelo e modo avaliação\n","# model.eval()\n","\n","# # Aumentar a velocidade\n","# # https://huggingface.co/docs/transformers/main/perf_torch_compile\n","# model = torch.compile(model)\n","\n","# print(\"Tempo de carregamento do modelo LLM:  {:} (h:mm:ss)\".format(formataTempo(time.time() - tempo_inicio)))"]},{"cell_type":"markdown","metadata":{"id":"doIqirI05Dos"},"source":["Carregamento LLama 2 com 8 bits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTFnuVQ3R0gp"},"outputs":[],"source":["# Importando as bibliotecas do Modelo\n","from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n","import torch\n","import time\n","\n","# Guarda o tempo de início do carregamento do modelo\n","tempo_inicio = time.time()\n","\n","# Carregando o Modelo da comunidade\n","print('Carregando o modelo ' + nome_modelo + ' da comunidade...')\n","\n","# BitsAndBytes é um framework com funções customizadas para\n","# otimização com precisão 8-bit, multiplicações de matrizes e funções de quantização\n","quantization_config = BitsAndBytesConfig(\n","   load_in_8bit=True, # Habilita a quantização de 8 bits\n",")\n","\n","# Se GPU Disponível\n","if GPU_ENABLE:\n","  # Carrega o modelo com a otimização BitsAndBytesConfig\n","  print (\"Carregando o LLM com GPU\")\n","  model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n","                                             #torch_dtype=torch.float16, #default\n","                                             trust_remote_code=True, # Carrega de um repositório confiável\n","                                             quantization_config=quantization_config,\n","                                             device_map=\"auto\"\n","                                             )\n","else:\n","  # Carrega o modelo sem a otimização BitsAndBytesConfig\n","  print (\"Carregando o LLM sem GPU\")\n","  model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n","                                             #torch_dtype=torch.float16, #default\n","                                             trust_remote_code=True, # Carrega de um repositório confiável\n","                                             device_map=\"auto\"\n","                                             )\n","\n","# Coloca o modelo e modo avaliação\n","model.eval()\n","\n","# Aumentar a velocidade\n","# https://huggingface.co/docs/transformers/main/perf_torch_compile\n","model = torch.compile(model)\n","\n","print(\"Tempo de carregamento do modelo LLM:  {:} (h:mm:ss)\".format(formataTempo(time.time() - tempo_inicio)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E11NM4T6pmpP"},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXgoG2ZvuHFI"},"outputs":[],"source":["print(model.config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysqp5fuyRWc4"},"outputs":[],"source":["print(model.config.max_position_embeddings)"]},{"cell_type":"markdown","metadata":{"id":"mpGMYgt6zWtX"},"source":["Tamanho do vocabulário"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZT7nQq3Q0ALQ"},"outputs":[],"source":["print(model.config.vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"NLdmeB6kLUUf"},"source":["## 3.3 - Configuração da geração de texto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1eEVDtDLaNF"},"outputs":[],"source":["# Import das bibliotecas\n","from transformers import GenerationConfig\n","\n","# Instância as configurações do modelo\n","generation_config = GenerationConfig.from_pretrained(nome_modelo)\n","\n","print(\"GenerationConfig antes:\\n\",generation_config)\n","\n","generation_config.max_new_tokens = 800 #Preenche até um comprimento máximo especificado com o argumento max_length ou até o comprimento de entrada máximo aceitável para o modelo se esse argumento não for fornecido.\n","generation_config.max_length = 800 # (Default 4096)\n","# Se do_sample é true setar temperature e top_p, caso contrário se do_sample é false remover temperature e top_p.\n","generation_config.do_sample = True # (Default True) Se definido como True, este parâmetro permite estratégias de decodificação como amostragem multinomial, amostragem multinomial de busca de feixe, amostragem Top-K e amostragem Top-p. Todas essas estratégias selecionam o próximo token da distribuição de probabilidade em todo o vocabulário com vários ajustes específicos da estratégia.\n","generation_config.temperature = 0.75 # (Default 0.6) A temperatura é um parâmetro que controla a aleatoriedade da saída do LLM. Uma temperatura mais alta resultará em um texto mais criativo e imaginativo, enquanto uma temperatura mais baixa resultará em um texto mais preciso e factual.\n","generation_config.top_k = 5  # Top-k diz ao modelo para escolher o próximo token entre os 'k' tokens principais de sua lista, classificados por probabilidade.\n","#generation_config.top_p = 0.1 # (Default 0.9) Top-p é mais dinâmico que top-k e é frequentemente usado para excluir resultados com probabilidades mais baixas. Portanto, se você definir p como 0,75, excluirá os 25% inferiores dos resultados menos prováveis.\n","#generation_config.repetition_penalty = 1.20 # Penaliza a repetição e visa evitar frases que se repetem sem nada de realmente interessante.\n","# generation_config.num_return_sequences=1, # Retorna uma única sentença da saída.\n","\n","print(\"GenerationConfig depois:\\n\",generation_config)"]},{"cell_type":"markdown","metadata":{"id":"ZGiVSTl1rwAe"},"source":["## 3.4 - Cria o pipeline usando Langchain\n","\n","Cria o pipeline com a classe [HuggingFacePipeline](https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html) do langchain."]},{"cell_type":"markdown","metadata":{"id":"Q7kqNDonwh49"},"source":["Passagem direta do pipeline Huggingface.\n","\n","Configura o pipeline do Huggingface usando o modelo e tokenizador previamente carregado e passa para o HuggingFacePipeline do langchain."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2WhTkmAZrNj"},"outputs":[],"source":["# Import das bibliotecas\n","from langchain.llms import HuggingFacePipeline\n","from transformers import pipeline\n","\n","# Configura o pipeline do HuggingFace\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=True,  # (Default True) Langchain espera o texto completo\n","    generation_config=generation_config, # Passa as configurações da geração de texto para o pipeline\n",")\n","\n","# Carrega o pipeline do Langchain\n","# https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n","model_llm = HuggingFacePipeline(pipeline=pipe,\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFReKYmi8bdb"},"outputs":[],"source":["print(model_llm)"]},{"cell_type":"markdown","metadata":{"id":"kcVB6qHZYgJQ"},"source":["# 4 - Classificando a coerência\n","\n"]},{"cell_type":"markdown","metadata":{"id":"riAdFoBZYgJe"},"source":["## 4.1 - Prompt\n","\n","```\n","Texto:\"{TEXTO}\"\n","Tarefa: Dado o texto acima forneça o resultado da classifação como coerente(1) ou incoerente(0) e justifique sua resposta no formato abaixo:\n","Resultado: <coerente(1)> ou <incoerente(0)>\n","Justificativa: <JUSTIFICATIVA>\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp4563RdYgJf"},"outputs":[],"source":["# Import das bibliotecas\n","from langchain.chains import LLMChain\n","from langchain import PromptTemplate\n","\n","def classificarCoerencia(texto):\n","\n","  # Cria o texto de prompt\n","  prompt_template = \"\"\"Texto:\\\"{TEXTO}\\\"\n","Tarefa: Dado o texto acima forneça o resultado da classifação como coerente(1) ou incoerente(0) e justifique sua resposta no formato abaixo:\n","Resultado: <coerente(1)> ou <incoerente(0)>\n","Justificativa: <JUSTIFICATIVA>\n","\"\"\"\n","\n","  # Cria o prompt\n","  prompt = PromptTemplate(input_variables=[\"TEXTO\"],\n","                          template = prompt_template)\n","\n","  #print(\"Prompt:\\n\", prompt.format(texto=texto))\n","\n","  # Instancia o chain\n","  chain = LLMChain(llm=model_llm, prompt=prompt)\n","\n","  # Executa o prompt no llm\n","  resultado = chain.invoke(input={\"TEXTO\": texto})\n","\n","  return resultado"]},{"cell_type":"markdown","source":["Texto coerente"],"metadata":{"id":"vwFmBBmfYgJf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT-JBRCLYgJf"},"outputs":[],"source":["texto = \"Como empilhar um elemento em uma pilha?\"\n","\n","resultado = classificarCoerencia(texto)\n","\n","print(\"Resposta: \\n\")\n","print_linhas_menores(resultado.get('text'))"]},{"cell_type":"markdown","metadata":{"id":"lzF4W0dxYgJg"},"source":["Texto incoerente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8bkYvXQYgJg"},"outputs":[],"source":["texto = \"Como enfileirar um elemento em uma pilha?\"\n","\n","resultado = classificarCoerencia(texto)\n","\n","#print(\"Resposta: \\n\" + resultado.get('text'))\n","print_linhas_menores(resultado.get('text'))"]},{"cell_type":"markdown","metadata":{"id":"_4JqQHs0ETLJ"},"source":["# 5 - Mensurando a coerência\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b295YONhETLL"},"source":["## 5.1 - Prompt\n","\n","```\n","Texto:\"{TEXTO}\"\n","Tarefa: Dado o texto acima, forneça uma pontuação de coerência do texto (5 - alta, 1 - baixa) e justifique sua resposta no formato abaixo:\n","Resultado: <PONTUAÇÃO>\n","Justificativa: <JUSTIFICATIVA>            \n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OmxemloETLL"},"outputs":[],"source":["# Import das bibliotecas\n","from langchain.chains import LLMChain\n","from langchain import PromptTemplate\n","\n","def mensurarCoerencia(texto):\n","\n","  # Cria o texto de prompt\n","  prompt_template = \"\"\"Texto:\\\"{TEXTO}\\\"\n","Tarefa: Dado o texto acima, forneça uma pontuação de coerência do texto (5 - alta, 1 - baixa) e justifique sua resposta no formato abaixo:\n","Resultado: <PONTUAÇÃO>\n","Justificativa: <JUSTIFICATIVA>\n","\"\"\"\n","\n","  # Cria o prompt\n","  prompt = PromptTemplate(input_variables=[\"TEXTO\"],\n","                          template = prompt_template)\n","\n","  #print(\"Prompt:\\n\", prompt.format(texto=texto))\n","\n","  # Instancia o chain\n","  chain = LLMChain(llm=model_llm, prompt=prompt)\n","\n","  # Executa o prompt no llm\n","  resultado = chain.invoke(input={\"TEXTO\": texto})\n","\n","  return resultado"]},{"cell_type":"markdown","source":["Texto coerente"],"metadata":{"id":"S7fM8Ez5G71H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7JnyxTunr6h"},"outputs":[],"source":["texto = \"Como empilhar um elemento em uma pilha?\"\n","\n","resultado = mensurarCoerencia(texto)\n","\n","print(\"Resposta: \\n\")\n","print_linhas_menores(resultado.get('text'))"]},{"cell_type":"markdown","metadata":{"id":"8wPwYHujtpef"},"source":["Texto incoerente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sKVfhuRETLN"},"outputs":[],"source":["texto = \"Como enfileirar um elemento em uma pilha?\"\n","\n","resultado = mensurarCoerencia(texto)\n","\n","#print(\"Resposta: \\n\" + resultado.get('text'))\n","print_linhas_menores(resultado.get('text'))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb","timestamp":1697477919370}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}