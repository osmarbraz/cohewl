{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["ifrYNTwGwKal","mhByVujAwNAU","WlF4PKP6Iopi","8yyRt4jnYxsU","Ix-Q5fZXY3HR","viicg1E7mXLK"],"authorship_tag":"ABX9TyOu4czsNxmTH/HQ2MGVtowr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"78HE8FLsKN9Q"},"source":["# Gera os arquivos para o Embedding Projector das palavras do conjunto de dados SQuAD 2 pt com BERT Transformers by HuggingFace.\n","\n","Gera os arquivos para Embedding Projector(https://projector.tensorflow.org/).\n","\n","Pode ser configurado para utilizar o BERTimbau **Large** e **Base**.\n","\n","Gera arquivos **records_token_sentenca.tsv** com:\n","- Com e sem pooling dos embeddings das palavras fora do vocabulário.\n","- Gera embeddings da concatenação das 4 últimas camadas do BERT ou da última camada.\n","\n","O último registro de records é referente é a média dos embeddings dos tokens do documento consolidados.\n","\n","O arquivo **meta_token_sentenca.tsv** possui as seguindas colunas:\n","- Token ou Sentença\n","- POS-Tag\n","- OOV (1 - Não existe no vocabulário do **BERT** e combina os *embeddings* dos tokens para formar a palavra e 0 - Existe no vocabulário do **BERT**)\n","- Id (Id do documento)\n","- Origem (Id do documento de origem)\n","- Classe (1 - Original, 0 - Perturbado)\n","- Perturbada (1 - Perturbada, 0 - Não perturbada)\n","- Index (Índice da palavra na sentença)\n","- Próximo token da sentença\n","- Granularidade (0 - Token, 1 - Sentença)\n","- Tipo Texto (0 - Palavra perturbada, 1 Palavra Original, 2 - Sentença Perturbada, 3 - Sentença Original)\n","- Sentença\n","\n","\n","Exemplo de projeção dos arquivos gerados:\n","https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/osmarbraz/cohebertv1projecao/main/config_token.json\n","\n","Repositório dos arquivos no github.\n","https://github.com/osmarbraz/cohebertv1projecao\n","\n","---------------------------\n","\n","Artigos:\n","\n","- https://arxiv.org/pdf/1611.05469v1.pdf\n","\n","- https://towardsdatascience.com/visualizing-bias-in-data-using-embedding-projector-649bc65e7487\n","\n","- https://towardsdatascience.com/bert-visualization-in-embedding-projector-dfe4c9e18ca9\n","\n","- https://krishansubudhi.github.io/deeplearning/2020/08/27/bert-embeddings-visualization.html\n","\n","- https://amitness.com/interactive-sentence-embeddings/\n","\n","---------------------------\n","\n","**Utiliza o *projeto embeddings* projector para exibir os dados:**\n","https://projector.tensorflow.org/\n","\n","\n","**Link biblioteca Huggingface:**\n","https://github.com/huggingface/transformers\n","\n","\n","**Artigo original BERT Jacob Devlin:**\n","https://arxiv.org/pdf/1506.06724.pdf"]},{"cell_type":"markdown","metadata":{"id":"xyxb5Px3p1-e"},"source":["# 1 Preparação do ambiente\n","Preparação do ambiente para execução do exemplo."]},{"cell_type":"markdown","metadata":{"id":"cW_5CN8En7zl"},"source":["## 1.1 Tempo inicial de processamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcTEKloUn-VK"},"outputs":[],"source":["# Import das bibliotecas\n","import time\n","import datetime\n","\n","#marca o tempo de início do processamento.\n","inicio_processamento = time.time()"]},{"cell_type":"markdown","metadata":{"id":"GOcN8hK-scnt"},"source":["## 1.2 Funções e classes auxiliares"]},{"cell_type":"markdown","metadata":{"id":"OPRnA-mk5-c4"},"source":["Verifica se existe o diretório cohebert no diretório corrente.   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fj5TaAH_5-nB"},"outputs":[],"source":["# Import das bibliotecas.\n","import os # Biblioteca para manipular arquivos\n","\n","# ============================\n","def verificaDiretorioCoheBERT():\n","    \"\"\"\n","      Verifica se existe o diretório cohebert no diretório corrente.\n","    \"\"\"\n","\n","    # Verifica se o diretório existe\n","    if not os.path.exists(DIRETORIO_COHEBERT):\n","        # Cria o diretório\n","        os.makedirs(DIRETORIO_COHEBERT)\n","        logging.info(\"Diretório Cohebert criado: {}\".format(DIRETORIO_COHEBERT))\n","\n","    return DIRETORIO_COHEBERT"]},{"cell_type":"markdown","metadata":{"id":"yDCOeh2y5jOH"},"source":["Realiza o download e um arquivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5B1mvfAU5jZf"},"outputs":[],"source":["# Import das bibliotecas.\n","import requests # Biblioteca de download\n","from tqdm.notebook import tqdm as tqdm_notebook # Biblioteca para barra de progresso\n","import os # Biblioteca para manipular arquivos\n","\n","def downloadArquivo(url_arquivo, nome_arquivo_destino):\n","    \"\"\"\n","      Realiza o download de um arquivo de uma url em salva em nome_arquivo_destino.\n","\n","      Parâmetros:\n","        `url_arquivo` - URL do arquivo a ser feito download.\n","        `nome_arquivo_destino` - Nome do arquivo a ser salvo.\n","    \"\"\"\n","\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Realiza o download de um arquivo em uma url\n","    data = requests.get(url_arquivo, stream=True)\n","\n","    # Verifica se o arquivo existe\n","    if data.status_code != 200:\n","        logging.info(\"Exceção ao tentar realizar download {}. Response {}.\".format(url_arquivo, data.status_code))\n","        data.raise_for_status()\n","        return\n","\n","    # Recupera o nome do arquivo a ser realizado o download\n","    nome_arquivo = nome_arquivo_destino.split(\"/\")[-1]\n","\n","    # Define o nome e caminho do arquivo temporário\n","    nome_arquivo_temporario = DIRETORIO_COHEBERT + \"/\" + nome_arquivo + \"_part\"\n","\n","    logging.info(\"Download do arquivo: {}.\".format(nome_arquivo_destino))\n","\n","    # Baixa o arquivo\n","    with open(nome_arquivo_temporario, \"wb\") as arquivo_binario:\n","        tamanho_conteudo = data.headers.get(\"Content-Length\")\n","        total = int(tamanho_conteudo) if tamanho_conteudo is not None else None\n","        # Barra de progresso de download\n","        progresso_bar = tqdm_notebook(unit=\"B\", total=total, unit_scale=True)\n","        # Atualiza a barra de progresso\n","        for chunk in data.iter_content(chunk_size=1024):\n","            if chunk:\n","                progresso_bar.update(len(chunk))\n","                arquivo_binario.write(chunk)\n","\n","    # Renomeia o arquivo temporário para o arquivo definitivo\n","    os.rename(nome_arquivo_temporario, nome_arquivo_destino)\n","\n","    # Fecha a barra de progresso.\n","    progresso_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"ksYnRk7zLGp0"},"source":["Remove tags de um documento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qwKjGvyLG4v"},"outputs":[],"source":["def remove_tags(documento):\n","    \"\"\"\n","      Remove tags de um documento\n","    \"\"\"\n","\n","    import re\n","\n","    documento_limpo = re.compile(\"<.*?>\")\n","    return re.sub(documento_limpo, \"\", documento)"]},{"cell_type":"markdown","metadata":{"id":"4pduTsINLeaz"},"source":["Funções auxiliares de arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jirIzIstLea0"},"outputs":[],"source":["def carregar(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como um único parágrafo(texto).\n","\n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.\n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","\n","    paragrafo = \"\"\n","    for linha in arquivo:\n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        # Remove as tags existentes no final das linhas\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          paragrafo = paragrafo + linha.strip() + \" \"\n","\n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    # Remove os espaços em branco antes e depois do parágrafo\n","    return paragrafo.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC9Xppq-_R0w"},"outputs":[],"source":["def carregarLista(nome_arquivo, encoding=\"Windows-1252\"):\n","    \"\"\"\n","      Carrega um arquivo texto e retorna as linhas como uma lista de sentenças(texto).\n","\n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser carregado.\n","        `encoding` - Codificação dos caracteres do arquivo.\n","    \"\"\"\n","\n","    # Abre o arquivo\n","    arquivo = open(nome_arquivo, \"r\", encoding= encoding)\n","\n","    sentencas = []\n","    for linha in arquivo:\n","        linha = linha.splitlines()\n","        linha = \" \".join(linha)\n","        linha = remove_tags(linha)\n","        if linha != \"\":\n","          sentencas.append(linha.strip())\n","\n","    # Fecha o arquivo\n","    arquivo.close()\n","\n","    return sentencas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkVk5LQT_G3f"},"outputs":[],"source":["def salvar(nome_arquivo,texto):\n","    \"\"\"\n","      Salva um texto em arquivo.\n","\n","      Parâmetros:\n","        `nome_arquivo` - Nome do arquivo a ser salvo.\n","        `texto` - Texto a ser salvo.\n","    \"\"\"\n","\n","    arquivo = open(nome_arquivo, \"w\")\n","    arquivo.write(str(texto))\n","    arquivo.close()"]},{"cell_type":"markdown","metadata":{"id":"603LYIYKBmq5"},"source":["Função auxiliar para formatar o tempo como `hh: mm: ss`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guy6B4whsZFR"},"outputs":[],"source":["# Import das bibliotecas.\n","import time\n","import datetime\n","\n","def formataTempo(tempo):\n","    \"\"\"\n","      Pega a tempo em segundos e retorna uma string hh:mm:ss\n","    \"\"\"\n","    # Arredonda para o segundo mais próximo.\n","    tempo_arredondado = int(round((tempo)))\n","\n","    # Formata como hh:mm:ss\n","    return str(datetime.timedelta(seconds=tempo_arredondado))"]},{"cell_type":"markdown","metadata":{"id":"zVKAapz7RCxk"},"source":["Classe(ModeloArgumentosMedida) de definição dos parâmetros do modelo para medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgmN6RqDRDZS"},"outputs":[],"source":["# Import das bibliotecas.\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","from typing import List\n","\n","@dataclass\n","class ModeloArgumentosMedida:\n","    max_seq_len: Optional[int] = field(\n","        default=None,\n","        metadata={'help': 'max seq len'},\n","    )\n","    pretrained_model_name_or_path: str = field(\n","        default='neuralmind/bert-base-portuguese-cased',\n","        metadata={'help': 'nome do modelo pré-treinado do BERT.'},\n","    )\n","    modelo_spacy: str = field(\n","        default=\"pt_core_news_lg\",\n","        metadata={\"help\": \"nome do modelo do spaCy.\"},\n","    )\n","    versao_modelo_spacy: str = field(\n","        default=\"-3.2.0\",\n","        metadata={\"help\": \"versão do nome do modelo no spaCy.\"},\n","    )\n","    do_lower_case: bool = field(\n","        default=False,\n","        metadata={'help': 'define se o texto do modelo deve ser todo em minúsculo.'},\n","    )\n","    output_attentions: bool = field(\n","        default=False,\n","        metadata={'help': 'habilita se o modelo retorna os pesos de atenção.'},\n","    )\n","    output_hidden_states: bool = field(\n","        default=False,\n","        metadata={'help': 'habilita gerar as camadas ocultas do modelo.'},\n","    )\n","    use_wandb : bool = field(\n","        default=True,\n","        metadata={'help': 'habilita o uso do wandb.'},\n","    )\n","    salvar_avaliacao : bool = field(\n","        default=True,\n","        metadata={'help': 'habilita o salvamento do resultado da avaliação.'},\n","    )\n","    salvar_medicao : bool = field(\n","        default=False,\n","        metadata={'help': 'habilita o salvamento da medicao.'},\n","    )\n","    usar_mcl_ajustado : bool = field(\n","        default=False,\n","        metadata={'help': 'habilita o carragamento de mcl ajustado.'},\n","    )\n","    documentos_perturbados: int = field(\n","        default=\"1\",\n","        metadata={\"help\": \"Quantidade de documentos a serem perturbados a partir do original.\"},\n","    )\n","    top_k_predicao: int = field(\n","        default=\"100\",\n","        metadata={\"help\": \"Quantidade de palavras a serem recuperadas mais próximas da máscara.\"},\n","    )\n","    estrategia_medida: int = field(\n","        default=0, # 0 - MEAN estratégia média / 1 - MAX  estratégia maior\n","        metadata={'help': 'Estratégia de cálculo da médida dos embeddings.'},\n","    )\n","    equacao_medida: int = field(\n","        default=0, # 0 - ADJACENTE / 1 - COMBINAÇÃO TODAS / 2 - CONTEXTO\n","        metadata={'help': 'Equação de cálculo da coerência.'},\n","    )\n","    filtro_palavra: int = field(\n","        default=0, # 0 - Considera todas as palavras das sentenças / 1 - Desconsidera as stopwords / 2 - Considera somente as palavras substantivas\n","        metadata={'help': 'Define o filtro de palavras das sentenças para gerar os embeddings.'},\n","    )"]},{"cell_type":"markdown","metadata":{"id":"HIN413rj50EI"},"source":["Biblioteca de limpeza de tela\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxV4-3Yg50EI"},"outputs":[],"source":["# Import das bibliotecas.\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"iAPVtRXQqDim"},"source":["## 1.3 Tratamento de logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcopxbGZqDip"},"outputs":[],"source":["# Import das bibliotecas.\n","import logging # Biblioteca de logging\n","\n","# Formatando a mensagem de logging\n","logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n","\n","logger = logging.getLogger()\n","logger.setLevel(logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"_GjYtXcMnSAe"},"source":["## 1.4  Identificando o ambiente Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMiH0E3OnRa1"},"outputs":[],"source":["# Se estiver executando no Google Colaboratory.\n","import sys\n","\n","# Retorna true ou false se estiver no Google Colaboratory.\n","IN_COLAB = 'google.colab' in sys.modules"]},{"cell_type":"markdown","metadata":{"id":"RinFHFesVKis"},"source":["## 1.5 Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"MPngEboiVbfi"},"source":["Usando Colab GPU para Treinamento\n"]},{"cell_type":"markdown","metadata":{"id":"EjWE6WlvVbfj"},"source":["Uma GPU pode ser adicionada acessando o menu e selecionando:\n","\n","`Edit -> Notebook Settings -> Hardware accelerator -> (GPU)`\n","\n","Em seguida, execute a célula a seguir para confirmar que a GPU foi detectada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtaYZmc3Vbfj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726226593,"user_tz":180,"elapsed":6274,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"91e3a5e3-3a1a-4aa7-f235-49c4de02a451"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","INFO:root:Dispositivo GPU não encontrado\n"]}],"source":["# Import das bibliotecas.\n","import tensorflow as tf\n","\n","# Recupera o nome do dispositido da GPU.\n","device_name = tf.test.gpu_device_name()\n","\n","# O nome do dispositivo deve ser parecido com o seguinte:\n","if device_name == \"/device:GPU:0\":\n","    logging.info(\"Encontrei GPU em: {}\".format(device_name))\n","else:\n","    logging.info(\"Dispositivo GPU não encontrado\")\n","    #raise SystemError(\"Dispositivo GPU não encontrado\")"]},{"cell_type":"markdown","metadata":{"id":"iYRrUo2XWa8G"},"source":["Nome da GPU\n","\n","Para que a torch use a GPU, precisamos identificar e especificar a GPU como o dispositivo. Posteriormente, em nosso ciclo de treinamento, carregaremos dados no dispositivo.\n","\n","Vale a pena observar qual GPU você recebeu. A GPU Tesla P100 é muito mais rápido que as outras GPUs, abaixo uma lista ordenada:\n","- 1o Tesla P100\n","- 2o Tesla T4\n","- 3o Tesla P4 (Não tem memória para execução 4 x 8, somente 2 x 4)\n","- 4o Tesla K80 (Não tem memória para execução 4 x 8, somente 2 x 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrjqDO6nWa8J"},"outputs":[],"source":["# Import das bibliotecas.\n","import torch\n","\n","def getDeviceGPU():\n","    \"\"\"\n","      Retorna um dispositivo de GPU se disponível ou CPU.\n","\n","      Retorno:\n","        `device` - Um device de GPU ou CPU.\n","    \"\"\"\n","\n","    # Se existe GPU disponível.\n","    if torch.cuda.is_available():\n","\n","        # Diz ao PyTorch para usar GPU.\n","        device = torch.device(\"cuda\")\n","\n","        logging.info(\"Existem {} GPU(s) disponíveis.\".format(torch.cuda.device_count()))\n","        logging.info(\"Iremos usar a GPU: {}.\".format(torch.cuda.get_device_name(0)))\n","\n","    # Se não.\n","    else:\n","        logging.info(\"Sem GPU disponível, usando CPU.\")\n","        device = torch.device(\"cpu\")\n","\n","    return device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChDxmtXsKwjf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726229883,"user_tz":180,"elapsed":16,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"30cf4c9d-9392-48bd-b3a8-048c4ff79642"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Sem GPU disponível, usando CPU.\n"]}],"source":["# Recupera o device com GPU ou CPU\n","device = getDeviceGPU()"]},{"cell_type":"markdown","metadata":{"id":"fGf59D0yVNx9"},"source":["Memória\n","\n","Memória disponível no ambiente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iC5-pSAVh7_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726229883,"user_tz":180,"elapsed":11,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"2dadc38a-07f9-4b97-af30-5c2191256c2c"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Seu ambiente de execução tem  13.6 gigabytes de RAM disponível\n","\n","INFO:root:Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \"Alterar tipo de tempo de execução\"\n","INFO:root:e selecione High-RAM. Então, execute novamente está célula\n"]}],"source":["# Importando as bibliotecas.\n","from psutil import virtual_memory\n","\n","ram_gb = virtual_memory().total / 1e9\n","logging.info(\"Seu ambiente de execução tem {: .1f} gigabytes de RAM disponível\\n\".format(ram_gb))\n","\n","if ram_gb < 20:\n","  logging.info(\"Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \\\"Alterar tipo de tempo de execução\\\"\")\n","  logging.info(\"e selecione High-RAM. Então, execute novamente está célula\")\n","else:\n","  logging.info(\"Você está usando um ambiente de execução de memória RAM alta!\")"]},{"cell_type":"markdown","metadata":{"id":"wijMXooQQLcQ"},"source":["## 1.6 Monta uma pasta no google drive para carregar os arquivos de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysnDDapMQK8K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726250722,"user_tz":180,"elapsed":20845,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"ee58f75f-e425-41ea-d609-3e9fc16146ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# import necessário\n","from google.colab import drive\n","\n","# Monta o drive na pasta especificada\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"u66iRrtwMrqy"},"source":["## 1.7 Instalação do wandb"]},{"cell_type":"markdown","metadata":{"id":"dQd3BrhvMzZs"},"source":["Instalação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejzpgGrFM0-j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726261498,"user_tz":180,"elapsed":10785,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"6e997b5c-fc75-413e-f0a7-031bd3e2b8e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 5.0 MB/s \n","\u001b[?25hCollecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 67.0 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 52.8 MB/s \n","\u001b[?25hCollecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 39.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 45.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 42.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 60.5 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 43.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 34.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 48.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 51.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=75aedf48e1545137eeaf32185782ce9ea425df49adae85ad9c0d7949a6551bbc\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.3\n"]}],"source":["!pip install --upgrade wandb"]},{"cell_type":"markdown","metadata":{"id":"oOd2MbBiDq93"},"source":["## 1.8 Instalação do spaCy\n","\n","https://spacy.io/\n","\n","Modelos do spaCy para português:\n","https://spacy.io/models/pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaMM4WdxgvQ7","colab":{"base_uri":"https://localhost:8080/","height":524},"executionInfo":{"status":"ok","timestamp":1663726285680,"user_tz":180,"elapsed":24191,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"161b768e-a2c1-42a2-cb78-e937821e2dc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-65.3.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 51.5 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","Installing collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","numba 0.56.2 requires setuptools<60, but you have setuptools 65.3.0 which is incompatible.\u001b[0m\n","Successfully installed pip-22.2.2 setuptools-65.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{}}],"source":["# Instala o spacy\n","!pip install -U pip setuptools wheel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4p3Rz2qDq94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726297456,"user_tz":180,"elapsed":12197,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"1b897df0-6a1d-4ecb-d9cd-8d6cbd5f5977"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spacy==3.2.0\n","  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.21.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.7.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (4.64.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.11.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.3)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (65.3.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.3.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.4.2)\n","Collecting typing-extensions<4.0.0.0,>=3.7.4\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.10)\n","Collecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.6/660.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.10.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (21.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.6.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.23.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2022.6.15)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.0) (2.0.1)\n","Installing collected packages: typing-extensions, pydantic, thinc, spacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.1.1\n","    Uninstalling typing_extensions-4.1.1:\n","      Successfully uninstalled typing_extensions-4.1.1\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.9.2\n","    Uninstalling pydantic-1.9.2:\n","      Successfully uninstalled pydantic-1.9.2\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.0\n","    Uninstalling thinc-8.1.0:\n","      Successfully uninstalled thinc-8.1.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.1\n","    Uninstalling spacy-3.4.1:\n","      Successfully uninstalled spacy-3.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pydantic-1.8.2 spacy-3.2.0 thinc-8.0.17 typing-extensions-3.10.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# Instala uma versão específica\n","!pip install -U spacy==3.2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RfUN_KolV-f"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["## 1.9 Instalação do BERT"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["Instala a interface pytorch para o BERT by Hugging Face.\n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCVzCmy7pIOz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663726313853,"user_tz":180,"elapsed":16414,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"30c25ae2-44d0-4f4e-8c4b-7999a63d8ac9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.5.1\n","  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.21.6)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.12.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2022.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=7851ec3ef69bbae48ee2a073b88cbe87fe1e8ae388d864a260b6ea3c1a5c99e1\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -U transformers==4.5.1"]},{"cell_type":"markdown","metadata":{"id":"giOsAS5v61go"},"source":["# 2 Parametrização"]},{"cell_type":"markdown","metadata":{"id":"ifrYNTwGwKal"},"source":["## Gerais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uiH9pNpwI6g"},"outputs":[],"source":["# Definição dos parâmetros a serem avaliados\n","#Quantidade de documentos a serem perturbados a partir do original.\n","DOCUMENTOS_PERTURBADOS = 20\n","\n","#Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","TOP_K_PREDICAO = 20\n","\n","#Realiza o pooling dos tokens de palavras fora do vocabulário do BERT\n","POOLING_TOKENS = 1 # 0 - Sem pooling / 1 - Com pooling\n","\n","#Utiliza somente documentos originais, perturbados ou ambos.\n","CLASSE_DOCUMENTO = 2 # 0 - Somente com a classe 0 (Perturbado) / 1 - Somente com classe 1 (Original), 2 - As duas classes 0 e 1\n","\n","#Estratégia de recuperação dos embeddings: (1 - Embeddings da última camada,\n","#                                           2 - Embeddings da concatenação das 4 últimas camadas)\n","ESTRATEGIA_EMBEDDING = 2\n","\n","# Estratégias a serem avaliadas (0 - Mean / 1 - Max) para as palavras formadas por mais de um token do BERT\n","ESTRATEGIA_MEDIDA_STR = [\"MEAN\", \"MAX\"]\n","ESTRATEGIA_MEDIDA = [0, 1]\n","\n","# Habilita a criação do rótulo \"__next__\" no projetor para gerar linhas entre os pontos de tokens de uma mesma origem em sequência.\n","LIGACAO_PROXIMO_TOKEN = True"]},{"cell_type":"markdown","source":["Permite filtrar os documentos a serem utilizados na geração dos arquivos da projeção."],"metadata":{"id":"BkttC-OgqSDP"}},{"cell_type":"code","source":["IDDO = 19\n","\n","FILTRO_IDO = ['56cdd21562d2951400fa68b2',   #DO1 Qual foi a intensidade escalada?\n","             '5a611ac3e9e1cc001a33cf1c',    #DO2 O que substituiu objetos de metal?\n","             '572679a5f1498d1400e8e100',    #DO3 O que usa um estado de memória interna?\n","             '5ad4a8225b96ef001a109d20',    #DO4 O que terminou o início da diferenciação de classe?\n","             '57101890b654c5140001f7d6',    #DO5 Que perguntas correspondem às homossexuais?\n","             '56d370aa59d6e414001463c2',    #DO6 Onde estava Christina Christian quando foi eliminada do show ?\n","             '5a8c3316fd22b3001a8d8643',    #DO7 Qual era o nome do padrão incompleto ?\n","             '56ce34c7aab44d1400b88596',    #DO8 Qual a nacionalidade de Estêvão Gomes ?\n","             '572ed039c246551400ce46d6',    #DO9 Onde os tornados são mais comuns na Terra ?\n","             '5a848a6d7cf838001a46a8eb',    #DO10 Qual é o fator chave para a ESA ?\n","             '5acd38ac07355d001abf3981',    #DO11 O que são neurônios dinâmicos de ampla faixa ?\n","             '59fc36eaa9fb160018f10dce',    #DO12 Que tipo de mensagem é enviada em um modelo complexo ?\n","             '56f799d2a6d7ea1400e17260',    #DO13 Quais navios foram atacados em 1852 ?\n","             '57111fd3b654c5140001fb81',    #DO14 De que cor os cartuchos foram produzidos pela Nintendo para uso interno ?\n","             '57277da6dd62a815002e9e8b',    #DO15 Qual foi o ato de mediação responsável pela restauração ?\n","             '5ad261cfd7d075001a429064',    #DO16 Qual é o exemplo mais antigo de linhas de falha financeira ?\n","             '573121fca5e9cc1400cdbc66',    #DO17 Que tratado trata de armas nucleares ?\n","             '5ad35760604f3c001a3fddf6',    #DO18 Quem convenceu Michael Dell a usar varejistas para vender PCs ?\n","             '5acd4d2c07355d001abf3c73',    #DO19 Em que ano Nero foi feito imperador ?\n","             '5728046cff5b5019007d9b05']    #DO20 Como os bytes de bit são transmitidos ?\n","\n","FILTRO_STR= ['DO1_','DO2_','DO3_','DO4_','DO5_','DO6_','DO7_','DO8_','DO9_','DO10_','DO11_','DO12_','DO13_','DO14_','DO15_','DO16_','DO17_','DO18_','DO19_','DO20_',]\n","# FILTRO_STR= ['']"],"metadata":{"id":"W6NunCYp5auL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filtra um determinado conjunto de documentos originais e suas versões perturbadas\n","# FILTRO_DO = [] # Filtro vazio seleciona todos os documentos\n","FILTRO_DO = [FILTRO_IDO[IDDO]] # Seleciona um documento da lista"],"metadata":{"id":"D40avaBwqQfq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mhByVujAwNAU"},"source":["## Específicos"]},{"cell_type":"markdown","metadata":{"id":"3V_ORR8Qyu1p"},"source":["Parâmetros do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ15-ylRRRdD"},"outputs":[],"source":["# Definição dos parâmetros do Modelo\n","model_args = ModeloArgumentosMedida(\n","    max_seq_len = 512,\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\",\n","    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\",\n","    pretrained_model_name_or_path = \"neuralmind/bert-large-portuguese-cased\",\n","    # pretrained_model_name_or_path = \"neuralmind/bert-base-portuguese-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n","    #pretrained_model_name_or_path = \"bert-base-multilingual-uncased\",\n","    modelo_spacy = \"pt_core_news_lg\",\n","    #modelo_spacy = \"pt_core_news_md\",\n","    #modelo_spacy = \"pt_core_news_sm\",\n","    versao_modelo_spacy = \"3.2.0\",\n","    do_lower_case = False,  # default True\n","    output_attentions = False,  # default False\n","    output_hidden_states = True, # default False\n","    use_wandb = True,\n","    salvar_medicao = True, #Salva o resultado da medição\n","    salvar_avaliacao = True, # Salva o resultado da avaliação das medições\n","    documentos_perturbados = DOCUMENTOS_PERTURBADOS, # Quantidade de documentos a serem perturbados a partir do original.\n","    top_k_predicao = TOP_K_PREDICAO, # Conjunto de valores: 1, 10, 100, 500 e 1000. Quantidade de palavras a serem recuperadas mais próximas da máscara.\n","    usar_mcl_ajustado = False, # Especifica se deve ser carregado um MCL ajustado ou pré-treinado. Necessário especificar o tipo do modelo em pretrained_model_name_or_path.\n","    estrategia_medida = 0, # Atributo usado para os logs do wandb. 0 - MEAN estratégia média / 1 - MAX  estratégia maior\n","    equacao_medida = 0, # Atributo usado para os logs do wandb. 0 - Palavras adjacentes / 1 - Todas as palavras / 2 - Palavra e contexto\n","    filtro_palavra = 0 # # Atributo usado para os logs do wandb. 0 - Considera todas as palavras das sentenças / 1 - Desconsidera as stopwords / 2 - Considera somente as palavras substantivas\n",")"]},{"cell_type":"markdown","metadata":{"id":"WlF4PKP6Iopi"},"source":["## Nome do diretório dos arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55PNP2s6Iopi"},"outputs":[],"source":["# Diretório do cohebert\n","DIRETORIO_COHEBERT = \"SQUAD2_P\""]},{"cell_type":"markdown","metadata":{"id":"SUxlx7Sx4yxj"},"source":["## Define o caminho para os arquivos de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gQpxAO74yxj"},"outputs":[],"source":["# Diretório local para os arquivos pré-processados\n","DIRETORIO_LOCAL = \"/content/\" + DIRETORIO_COHEBERT + \"/\"\n","\n","# Diretório no google drive com os arquivos pré-processados\n","DIRETORIO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/\""]},{"cell_type":"markdown","metadata":{"id":"tDgJTbPOZ8SW"},"source":["## Inicialização diretórios"]},{"cell_type":"markdown","metadata":{"id":"qpSERA9TC4WU"},"source":["Diretório base local"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edg7eW2cDflg"},"outputs":[],"source":["# Importando as bibliotecas.\n","import os\n","\n","def criaDiretorioLocal():\n","\n","  # Cria o diretório para receber os arquivos Originais e Permutados\n","  # Diretório a ser criado\n","  dirbase = DIRETORIO_LOCAL[:-1]\n","\n","  if not os.path.exists(dirbase):\n","      # Cria o diretório\n","      os.makedirs(dirbase)\n","      logging.info(\"Diretório criado: {}.\".format(dirbase))\n","  else:\n","      logging.info(\"Diretório já existe: {}.\".format(dirbase))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xge0ar9MJoKy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728332601,"user_tz":180,"elapsed":109,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"2aacd5f6-77d7-4e40-9d63-b3b456d13123"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Diretório já existe: /content/SQUAD2_P.\n"]}],"source":["criaDiretorioLocal()"]},{"cell_type":"markdown","metadata":{"id":"4FmT9nhbaE3D"},"source":["Diretório para conter as os resultados das medidas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO76uzj_C3zQ"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioMedidacao():\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):\n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1Ot2h_bJuxy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728332605,"user_tz":180,"elapsed":98,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"1c3e4afd-17f4-43c2-eab9-3802b7f5a24b"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Diretório já existe: /content/drive/MyDrive/Colab Notebooks/Data/SQUAD2_P/validacao_medicao_palavra.\n"]}],"source":["criaDiretorioMedidacao()"]},{"cell_type":"markdown","metadata":{"id":"vIkT6ksqaQs3"},"source":["Diretório para conter os arquivos da avaliação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIV4xj6zDnb8"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioAvaliacao():\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra/Avaliacao\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):\n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiOVjJ5BJzE1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728332607,"user_tz":180,"elapsed":92,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"7bd60ee0-80fa-434f-f766-37bb22208a17"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Diretório já existe: /content/drive/MyDrive/Colab Notebooks/Data/SQUAD2_P/validacao_medicao_palavra/Avaliacao.\n"]}],"source":["criaDiretorioAvaliacao()"]},{"cell_type":"markdown","metadata":{"id":"cjP6v878aWR7"},"source":["Diretório para conter os arquivos das medidas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf6UWAZYDsgm"},"outputs":[],"source":["# Import de bibliotecas.\n","import os\n","\n","def criaDiretorioMedicao():\n","\n","  DIRETORIO_BASE = DIRETORIO_DRIVE + \"validacao_medicao_palavra/Medicao\"\n","\n","  # Verifica se o diretório existe\n","  if not os.path.exists(DIRETORIO_BASE):\n","    # Cria o diretório\n","    os.makedirs(DIRETORIO_BASE)\n","    logging.info(\"Diretório criado: {}.\".format(DIRETORIO_BASE))\n","  else:\n","    logging.info(\"Diretório já existe: {}.\".format(DIRETORIO_BASE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBBfHFuPJ3NM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728332607,"user_tz":180,"elapsed":86,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"b81bbe99-e19e-43c8-f406-44cf00ca9830"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Diretório já existe: /content/drive/MyDrive/Colab Notebooks/Data/SQUAD2_P/validacao_medicao_palavra/Medicao.\n"]}],"source":["criaDiretorioMedicao()"]},{"cell_type":"markdown","metadata":{"id":"L7G3-MOsQ1N_"},"source":["# 3 spaCy"]},{"cell_type":"markdown","metadata":{"id":"35GwcgkOlWi3"},"source":["## 3.1 Download arquivo modelo\n","\n","https://spacy.io/models/pt"]},{"cell_type":"markdown","metadata":{"id":"PWd_9X0nOYnF"},"source":["### Função download modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjWGu-9D5URZ"},"outputs":[],"source":["def downloadSpacy(model_args):\n","    \"\"\"\n","      Realiza o download do arquivo do modelo para o diretório corrente.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \"\"\"\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Nome arquivo compactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","\n","    # Url do arquivo\n","    URL_ARQUIVO_MODELO_COMPACTADO = \"https://github.com/explosion/spacy-models/releases/download/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO\n","\n","    # Realiza o download do arquivo do modelo\n","    logging.info(\"Download do arquivo do modelo do spaCy.\")\n","    downloadArquivo(URL_ARQUIVO_MODELO_COMPACTADO, DIRETORIO_COHEBERT + \"/\" + NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"Uu_LkF7Nfm8_"},"source":["## 3.2 Descompacta o arquivo do modelo"]},{"cell_type":"markdown","metadata":{"id":"XAc1tSwvOc4d"},"source":["### Função descompacta modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq9PnXO77bPQ"},"outputs":[],"source":["# Import das bibliotecas.\n","import tarfile # Biblioteca de descompactação\n","\n","def descompactaSpacy(model_args):\n","    \"\"\"\n","      Descompacta o arquivo do modelo.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","    \"\"\"\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","\n","    # Nome do arquivo a ser descompactado\n","    NOME_ARQUIVO_MODELO_COMPACTADO = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \".tar.gz\"\n","\n","    logging.info(\"Descompactando o arquivo do modelo do spaCy.\")\n","    arquivo_tar = tarfile.open(NOME_ARQUIVO_MODELO_COMPACTADO, \"r:gz\")\n","    arquivo_tar.extractall(DIRETORIO_COHEBERT)\n","    arquivo_tar.close()\n","\n","    # Apaga o arquivo compactado\n","    if os.path.isfile(NOME_ARQUIVO_MODELO_COMPACTADO):\n","        os.remove(NOME_ARQUIVO_MODELO_COMPACTADO)"]},{"cell_type":"markdown","metadata":{"id":"STHT2c89qvwK"},"source":["## 3.3 Carrega o modelo"]},{"cell_type":"markdown","metadata":{"id":"3iFBoyWMOgKz"},"source":["### Função carrega modelo spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePOccj0s8WMg"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy # Biblioteca do spaCy\n","\n","def carregaSpacy(model_args):\n","    \"\"\"\n","    Realiza o carregamento do Spacy.\n","\n","    Parâmetros:\n","      `model_args` - Objeto com os argumentos do modelo.\n","    \"\"\"\n","\n","    # Verifica se existe o diretório base\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Nome arquivo spacy\n","    ARQUIVO_MODELO_SPACY = model_args.modelo_spacy\n","    # Versão spaCy\n","    VERSAO_SPACY = \"-\" + model_args.versao_modelo_spacy\n","    # Caminho raoz do modelo do spaCy\n","    DIRETORIO_MODELO_SPACY =  DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY\n","\n","    # Verifica se o diretório existe\n","    if os.path.exists(DIRETORIO_MODELO_SPACY) == False:\n","        # Realiza o download do arquivo modelo do spaCy\n","        downloadSpacy(model_args)\n","        # Descompacta o spaCy\n","        descompactaSpacy(model_args)\n","\n","    # Diretório completo do spaCy\n","    DIRETORIO_MODELO_SPACY = DIRETORIO_COHEBERT + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + \"/\" + ARQUIVO_MODELO_SPACY + VERSAO_SPACY + \"/\"\n","\n","    # Carrega o spaCy. Necessário somente \"tagger\" para encontrar os substantivos\n","    nlp = spacy.load(DIRETORIO_MODELO_SPACY)\n","    logging.info(\"spaCy carregado.\")\n","\n","    # Retorna o spacy carregado\n","    return nlp"]},{"cell_type":"markdown","metadata":{"id":"cAk5hHx7OnHn"},"source":["### Carrega o modelo spaCy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbELnrpgA4T1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728344276,"user_tz":180,"elapsed":11310,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"1128ae9f-ed6c-4ed2-9e24-2d7b7804f7c2"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:spaCy carregado.\n"]}],"source":["# Carrega o modelo spaCy\n","nlp = carregaSpacy(model_args)"]},{"cell_type":"markdown","metadata":{"id":"fzk8VOp7oy8n"},"source":["## 3.4 Funções auxiliares spaCy"]},{"cell_type":"markdown","metadata":{"id":"AEzytjZi5Iw2"},"source":["### getStopwords\n","\n","Recupera as stopwords do spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKg-_XyWoy8o"},"outputs":[],"source":["def getStopwords(nlp):\n","    \"\"\"\n","      Recupera as stop words do nlp(Spacy).\n","\n","      Parâmetros:\n","        `nlp` - Um modelo spaCy carregado.\n","    \"\"\"\n","\n","    spacy_stopwords = nlp.Defaults.stop_words\n","\n","    return spacy_stopwords"]},{"cell_type":"markdown","metadata":{"id":"qZdNFrC3oy8p"},"source":["Lista dos stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1o8jevtoy8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728344277,"user_tz":180,"elapsed":29,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"d58cf0a7-b8b3-4acf-a3b6-ab8434ab21ca"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Quantidade de stopwords: 416.\n"]},{"output_type":"stream","name":"stdout","text":["{'tanta', 'vossa', 'apoio', 'nada', 'seria', 'pelo', 'elas', 'têm', 'quando', 'até', 'bem', 'teus', 'ser', 'porquê', 'além', 'após', 'se', 'sua', 'pode', 'à', 'num', 'questão', 'está', 'baixo', 'primeiro', 'somente', 'nunca', 'isso', 'sétimo', 'primeira', 'certeza', 'aquelas', 'porque', 'teu', 'aí', 'tu', 'depois', 'tentei', 'de', 'só', 'meses', 'favor', 'dezassete', 'pegar', 'na', 'dar', 'ponto', 'vezes', 'pelos', 'como', 'novos', 'for', 'nossos', 'vos', 'desse', 'custa', 'tarde', 'põe', 'nós', 'fazemos', 'local', 'és', 'deverá', 'podia', 'número', 'te', 'ontem', 'estás', 'quê', 'sou', 'naquele', 'ora', 'seus', 'estivemos', 'tive', 'grande', 'põem', 'tal', 'cento', 'partir', 'inclusive', 'nos', 'forma', 'tens', 'aqueles', 'sim', 'boa', 'poderá', 'toda', 'minhas', 'contudo', 'lhe', 'nesse', 'perto', 'menor', 'mil', 'foi', 'dez', 'treze', 'nova', 'parece', 'deve', 'já', 'vocês', 'sobre', 'próxima', 'cá', 'posso', 'segundo', 'faz', 'pelas', 'apenas', 'quero', 'poder', 'dizem', 'sabe', 'possível', 'saber', 'parte', 'conhecida', 'algumas', 'desde', 'conselho', 'umas', 'ligado', 'terceiro', 'quinze', 'estou', 'vens', 'nosso', 'numa', 'oitava', 'daquela', 'mais', 'aquela', 'duas', 'aos', 'vosso', 'ele', 'dessa', 'comprido', 'nas', 'tivestes', 'geral', 'vem', 'posição', 'todo', 'ao', 'inicio', 'lado', 'suas', 'tua', 'maioria', 'veja', 'terceira', 'tem', 'quieta', 'nuns', 'cima', 'também', 'momento', 'uma', 'próximo', 'da', 'daquele', 'fazes', 'pontos', 'nossas', 'vai', 'esta', 'diz', 'em', 'essa', 'relação', 'portanto', 'qualquer', 'ambos', 'deste', 'cuja', 'nem', 'fazem', 'acerca', 'quarto', 'quem', 'quinta', 'coisa', 'das', 'tiveram', 'tuas', 'muito', 'estas', 'mal', 'aqui', 'um', 'nove', 'as', 'grandes', 'fazia', 'nível', 'dá', 'sob', 'vários', 'des', 'lugar', 'quieto', 'números', 'nesta', 'vão', 'vinte', 'sexto', 'somos', 'pouca', 'me', 'nenhuma', 'irá', 'exemplo', 'diante', 'estava', 'usa', 'estive', 'outra', 'dois', 'sempre', 'dizer', 'pôde', 'entre', 'fora', 'isto', 'quer', 'enquanto', 'estiveram', 'comprida', 'dentro', 'mas', 'através', 'certamente', 'meus', 'lá', 'por', 'obrigada', 'tiveste', 'que', 'ademais', 'puderam', 'seis', 'este', 'dezoito', 'meu', 'três', 'onde', 'o', 'vossas', 'faço', 'apontar', 'devem', 'fomos', 'sei', 'estiveste', 'doze', 'eles', 'foram', 'apoia', 'obrigado', 'porquanto', 'fostes', 'estivestes', 'então', 'querem', 'dos', 'podem', 'cada', 'qual', 'conhecido', 'ver', 'menos', 'é', 'ou', 'povo', 'essas', 'para', 'eu', 'breve', 'aquele', 'talvez', 'estes', 'grupo', 'embora', 'próprio', 'sem', 'cujo', 'pela', 'antes', 'ir', 'meio', 'eventual', 'vez', 'outras', 'sétima', 'nossa', 'longe', 'direita', 'fazer', 'sistema', 'atrás', 'fim', 'tentaram', 'área', 'dão', 'debaixo', 'catorze', 'são', 'tanto', 'a', 'esteve', 'tipo', 'possivelmente', 'ela', 'aquilo', 'quatro', 'contra', 'estar', 'logo', 'foste', 'estado', 'fazeis', 'minha', 'todas', 'bastante', 'pouco', 'você', 'final', 'sois', 'vossos', 'tão', 'vindo', 'maior', 'nessa', 'os', 'todos', 'máximo', 'temos', 'tudo', 'tais', 'estão', 'quarta', 'era', 'tempo', 'ambas', 'segunda', 'maiorias', 'valor', 'algo', 'cinco', 'ali', 'caminho', 'com', 'alguns', 'iniciar', 'tentar', 'novo', 'porém', 'dezanove', 'teve', 'cedo', 'oito', 'onze', 'falta', 'ter', 'oitavo', 'último', 'no', 'adeus', 'quinto', 'quais', 'mesmo', 'disso', 'e', 'tente', 'novas', 'vêm', 'vós', 'demais', 'desta', 'pois', 'agora', 'uns', 'vais', 'assim', 'estará', 'mês', 'fará', 'outros', 'sete', 'esses', 'dezasseis', 'quanto', 'fez', 'tendes', 'neste', 'fui', 'naquela', 'sexta', 'às', 'corrente', 'esse', 'bom', 'seu', 'muitos', 'zero', 'vinda', 'do', 'ainda', 'não', 'tivemos', 'usar', 'tenho'}\n"]}],"source":["logging.info(\"Quantidade de stopwords: {}.\".format(len(getStopwords(nlp))))\n","\n","print(getStopwords(nlp))"]},{"cell_type":"markdown","metadata":{"id":"onM1ZApom-_W"},"source":["### getVerbos\n","Localiza os verbos da sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hdqVdfxm-_W"},"outputs":[],"source":["# Import das bibliotecas.\n","import spacy\n","from spacy.util import filter_spans\n","from spacy.matcher import Matcher\n","\n","# (verbo normal como auxilar ou auxilar) + vários verbos auxiliares +verbo principal ou verbo auxiliar\n","gramaticav1 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"aux\",\"aux:pass\"]}},  #verbo auxiliar\n","                {\"POS\": \"VERB\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"ROOT\",\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo normal como auxiliar\n","                {\"POS\": \"AUX\", \"OP\": \"*\", \"DEP\": {\"IN\": [\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo auxiliar\n","                {\"POS\": \"VERB\", \"OP\": \"+\"}, #verbo principal\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"cop\",\"aux\",\"xcomp\",\"aux:pass\"]}},  #verbo auxiliar\n","               ]\n","\n","# verbo auxiliar + verbo normal como auxiliar + conjunção com preposição + verbo\n","gramaticav2 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"aux\",\"aux:pass\"]}},  #verbo auxiliar\n","                {\"POS\": \"VERB\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"ROOT\"]}},  #verbo principal\n","                {\"POS\": \"SCONJ\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"mark\"]}}, #conjunção com preposição\n","                {\"POS\": \"VERB\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"xcomp\"]}}, #verbo normal como complementar\n","               ]\n","\n","#Somente verbos auxiliares\n","gramaticav3 =  [\n","                {\"POS\": \"AUX\", \"OP\": \"?\"},  #Verbos auxiliar\n","                {\"POS\": \"AUX\", \"OP\": \"?\", \"DEP\": {\"IN\": [\"cop\"]}},  #Verbos auxiliar de ligação (AUX+(cop))\n","                {\"POS\": \"ADJ\", \"OP\": \"+\", \"DEP\": {\"IN\": [\"ROOT\"]}},\n","                {\"POS\": \"AUX\", \"OP\": \"?\"}  #Verbos auxiliar\n","               ]\n","\n","matcherv = Matcher(nlp.vocab)\n","\n","matcherv.add(\"frase verbal\", [gramaticav1])\n","matcherv.add(\"frase verbal\", [gramaticav2])\n","matcherv.add(\"frase verbal\", [gramaticav3])\n","\n","#Retorna a Frase Verbal\n","def getVerbos(periodo):\n","  #Processa o período\n","  doc1 = nlp(periodo.text)\n","\n","  # Chama o mather para encontrar o padrão\n","  matches = matcherv(doc1)\n","\n","  padrao = [doc1[start:end] for _, start, end in matches]\n","\n","  #elimina as repetições e sobreposições\n","  #return filter_spans(padrao)\n","  lista1 = filter_spans(padrao)\n","\n","  # Converte os itens em string\n","  lista2 = []\n","  for x in lista1:\n","      lista2.append(str(x))\n","\n","  return lista2"]},{"cell_type":"markdown","metadata":{"id":"6ZVwbmn3Nx2t"},"source":["### getDicPOSQtde\n","\n","Conta as POS Tagging de uma sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3j3VF4NOSPbq"},"outputs":[],"source":["def getDicPOSQtde(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Retorna inteiros que mapeiam para classes gramaticais\n","  conta_dicionarios = doc.count_by(spacy.attrs.IDS[\"POS\"])\n","\n","  # Dicionário com as tags e quantidades\n","  novodic = dict()\n","\n","  for pos, qtde in conta_dicionarios.items():\n","    classe_gramatical = doc.vocab[pos].text\n","    novodic[classe_gramatical] = qtde\n","\n","  return novodic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uPDYU4KBC5q"},"outputs":[],"source":["def getDicTodasPOSQtde(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Retorna inteiros que mapeiam para classes gramaticais\n","  conta_dicionarios = doc.count_by(spacy.attrs.IDS[\"POS\"])\n","\n","  # Dicionário com as tags e quantidades\n","  novodic = {\"PRON\":0, \"VERB\":0, \"PUNCT\":0, \"DET\":0, \"NOUN\":0, \"AUX\":0, \"CCONJ\":0, \"ADP\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"NUM\":0, \"SCONJ\":0, \"SYM\":0, \"SPACE\":0, \"INTJ\":0, \"X\": 0}\n","\n","  for pos, qtde in conta_dicionarios.items():\n","    classe_gramatical = doc.vocab[pos].text\n","    novodic[classe_gramatical] = qtde\n","\n","  return novodic"]},{"cell_type":"markdown","metadata":{"id":"Jxe-mh-l6sJY"},"source":["### getDicTodasPOSQtde\n","\n","Conta as POS Tagging de uma sentença"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9SA61kD6sJY"},"outputs":[],"source":["def getDicTodasPOSQtde(lista):\n","\n","  # Dicionário com as tags e quantidades\n","  conjunto = {\"PRON\":0, \"VERB\":0, \"PUNCT\":0, \"DET\":0, \"NOUN\":0, \"AUX\":0, \"CCONJ\":0, \"ADP\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"NUM\":0, \"SCONJ\":0, \"SYM\":0, \"SPACE\":0, \"INTJ\": 0}\n","\n","  for x in lista:\n","    valor = conjunto.get(x)\n","    if valor != None:\n","      conjunto[x] = valor + 1\n","    else:\n","      conjunto[x] = 1\n","\n","  return conjunto"]},{"cell_type":"markdown","metadata":{"id":"m4KV_jI-Nx2w"},"source":["### getSomaDic\n","\n","Soma os valores de dicionários com as mesmas chaves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGduPM6HNx2w"},"outputs":[],"source":["from collections import Counter\n","from functools import reduce\n","\n","def atualizaValor(a,b):\n","    a.update(b)\n","    return a\n","\n","def getSomaDic(lista):\n","\n","  # Soma os dicionários da lista\n","  novodic = reduce(atualizaValor, (Counter(dict(x)) for x in lista))\n","\n","  return novodic"]},{"cell_type":"markdown","metadata":{"id":"bGaf7bkpAEiX"},"source":["### getTokensSentenca\n","\n","Retorna a lista de tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWxyAo54AOHU"},"outputs":[],"source":["def getTokensSentenca(sentenca):\n","\n","    # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença adicionando os tokens\n","  for token in doc:\n","    lista.append(token.text)\n","\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"ZB6bR42PA28c"},"source":["### getPOSTokensSentenca\n","\n","Retorna a lista das POS-Tagging dos tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awaqjNIZA3Fk"},"outputs":[],"source":["def getPOSTokensSentenca(sentenca):\n","\n","  # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença adicionando os tokens\n","  for token in doc:\n","    lista.append(token.pos_)\n","\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"B4Soqt3fp3Lu"},"source":["### getListaTokensPOSSentenca\n","\n","Retorna duas listas uma com os tokens e a outra com a POS-Tagging dos tokens da sentenca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvd99wd_pwmt"},"outputs":[],"source":["def getListaTokensPOSSentenca(sentenca):\n","  # Verifica se o sentenca não foi processado pelo spaCy\n","  if type(sentenca) is not spacy.tokens.doc.Doc:\n","      # Realiza o parsing no spacy\n","      doc = nlp(sentenca)\n","  else:\n","      doc = sentenca\n","\n","  # Lista dos tokens\n","  lista_tokens = []\n","  lista_pos = []\n","\n","  # Percorre a sentença adicionando os tokens e as POS\n","  for token in doc:\n","    lista_tokens.append(token.text)\n","    lista_pos.append(token.pos_)\n","\n","  return lista_tokens, lista_pos"]},{"cell_type":"markdown","metadata":{"id":"ENvsIER06sJX"},"source":["### Tadução das tags"]},{"cell_type":"markdown","metadata":{"id":"kwSb3ECU6sJY"},"source":["Tags de palavras universal\n","\n","https://universaldependencies.org/u/pos/\n","\n","Detalhes das tags em português:\n","http://www.dbd.puc-rio.br/pergamum/tesesabertas/1412298_2016_completo.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpCUpOs06sJY"},"outputs":[],"source":["#dicionário que contêm pos tag universal e suas explicações\n","palavra_universal_dict = {\n","  \"X\"    : \"Outro\",\n","  \"VERB\" : \"Verbo \",\n","  \"SYM\"  : \"Símbolo\",\n","  \"CONJ\" : \"Conjunção\",\n","  \"SCONJ\": \"Conjunção subordinativa\",\n","  \"PUNCT\": \"Pontuação\",\n","  \"PROPN\": \"Nome próprio\",\n","  \"PRON\" : \"Pronome substativo\",\n","  \"PART\" : \"Partícula, morfemas livres\",\n","  \"NUM\"  : \"Numeral\",\n","  \"NOUN\" : \"Substantivo\",\n","  \"INTJ\" : \"Interjeição\",\n","  \"DET\"  : \"Determinante, Artigo e pronomes adjetivos\",\n","  \"CCONJ\": \"Conjunção coordenativa\",\n","  \"AUX\"  : \"Verbo auxiliar\",\n","  \"ADV\"  : \"Advérbio\",\n","  \"ADP\"  : \"Preposição\",\n","  \"ADJ\"  : \"Adjetivo\"\n","}\n","\n","#Explica a POS\n","def getPOSPalavraUniversalTraduzido(palavra):\n","  if palavra in palavra_universal_dict.keys():\n","      traduzido = palavra_universal_dict[palavra]\n","  else:\n","      traduzido = \"NA\"\n","  return traduzido"]},{"cell_type":"markdown","metadata":{"id":"b01WgMSSKY_u"},"source":["### getSentencaSemStopWord\n","\n","Retorna uma lista dos tokens sem as stopwords."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMb0uDWzKZXP"},"outputs":[],"source":["def getSentencaSemStopWord(sentenca, stopwords):\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre os tokens da sentença\n","  for i, token in enumerate(sentenca):\n","\n","    # Verifica se o token é uma stopword\n","    if token.lower() not in stopwords:\n","      lista.append(token)\n","\n","  # Retorna o documento\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"TouR4GjNJZD6"},"source":["### getSentencaSalientePOS\n","\n","Retorna uma lista das palavras do tipo especificado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxTCYFzcJZD6"},"outputs":[],"source":["def getSentencaSalientePOS(sentenca, pos, tipo_saliente=\"NOUN\"):\n","\n","  # Lista dos tokens\n","  lista = []\n","\n","  # Percorre a sentença\n","  for i, token in enumerate(sentenca):\n","\n","    # Verifica se o token é do tipo especeficado\n","    if pos[i] == tipo_saliente:\n","      lista.append(token)\n","\n","  # Retorna o documento\n","  return lista"]},{"cell_type":"markdown","metadata":{"id":"_xaeX0oTVQ5t"},"source":["###removeStopWords\n","\n","Remove as stopwords de um documento ou senteça."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIaQ9bzBVQ5t"},"outputs":[],"source":["def removeStopWord(documento, stopwords):\n","\n","  # Remoção das stopwords do documento\n","  documento_sem_stopwords = [palavra for palavra in documento.split() if palavra.lower() not in stopwords]\n","\n","  # Concatena o documento sem os stopwords\n","  documento_limpo = \" \".join(documento_sem_stopwords)\n","\n","  # Retorna o documento\n","  return documento_limpo"]},{"cell_type":"markdown","metadata":{"id":"A7NAe8ogCf1y"},"source":["### retornaRelevante\n","\n","Retorna somente os palavras do documento ou sentença do tipo especificado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNNfykypChn-"},"outputs":[],"source":["def retornaRelevante(documento, classe_relevante=\"NOUN\"):\n","\n","  # Corrigir!\n","  # Utilizar o documento já tokenizado pelo spacy!!!!\n","  # Existe uma lista com o documento e a sentença tokenizada pelo spacy\n","\n","  # Realiza o parsing no spacy\n","  doc = nlp(documento)\n","\n","  # Retorna a lista das palavras relevantes\n","  documento_com_substantivos = []\n","  for token in doc:\n","    #print(\"token:\", token.pos_)\n","    if token.pos_ == classe_relevante:\n","      documento_com_substantivos.append(token.text)\n","\n","  # Concatena o documento com os substantivos\n","  documento_concatenado = \" \".join(documento_com_substantivos)\n","\n","  # Retorna o documento\n","  return documento_concatenado"]},{"cell_type":"markdown","metadata":{"id":"IBY7q_uH8JSE"},"source":["# 4 BERT"]},{"cell_type":"markdown","metadata":{"id":"MBGTMy8Ic7GK"},"source":["## 4.1 Modelo Pré-treinado BERT"]},{"cell_type":"markdown","metadata":{"id":"uiuxdXe9t1BX"},"source":["### Funções Auxiliares"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Huw0x5kt1Le"},"outputs":[],"source":["def getNomeModeloBERT(model_args):\n","    '''\n","    Recupera uma string com uma descrição do modelo BERT para nomes de arquivos e diretórios.\n","\n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","\n","    Retorno:\n","    `MODELO_BERT` - Nome do modelo BERT.\n","    '''\n","\n","    # Verifica o nome do modelo(default SEM_MODELO_BERT)\n","    MODELO_BERT = \"SEM_MODELO_BERT\"\n","\n","    if 'neuralmind' in model_args.pretrained_model_name_or_path:\n","        MODELO_BERT = \"_BERTimbau\"\n","\n","    else:\n","        if 'multilingual' in model_args.pretrained_model_name_or_path:\n","            MODELO_BERT = \"_BERTmultilingual\"\n","\n","    return MODELO_BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYJB4ik7t5xe"},"outputs":[],"source":["def getTamanhoBERT(model_args):\n","    '''\n","    Recupera uma string com o tamanho(dimensão) do modelo BERT para nomes de arquivos e diretórios.\n","\n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","\n","    Retorno:\n","    `TAMANHO_BERT` - Nome do tamanho do modelo BERT.\n","    '''\n","\n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = \"_large\"\n","\n","    if 'base' in model_args.pretrained_model_name_or_path:\n","        TAMANHO_BERT = \"_base\"\n","\n","    return TAMANHO_BERT"]},{"cell_type":"markdown","metadata":{"id":"rHt4e5pAcEMd"},"source":["### Função download Modelo Pre-treinado BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peDUrV2ccEXA"},"outputs":[],"source":["# Import das bibliotecas.\n","import zipfile # Biblioteca para descompactar\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def downloadModeloPretreinado(model_args):\n","    \"\"\"\n","      Realiza o download do modelo BERT(MODELO) e retorna o diretório onde o modelo BERT(MODELO) foi descompactado.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\"\n","\n","    # Nome diretório base modelo BERT\n","    NOME_DIRETORIO_BASE_MODELO = \"modeloBERT\"\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Recupera o nome ou caminho do modelo\n","    MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in MODELO:\n","        URL_MODELO = MODELO\n","\n","    # Se a variável foi setada.\n","    if URL_MODELO:\n","\n","        # Diretório do modelo.\n","        DIRETORIO_MODELO = DIRETORIO_COHEBERT + \"/\" + NOME_DIRETORIO_BASE_MODELO\n","\n","        # Recupera o nome do arquivo do modelo da url.\n","        NOME_ARQUIVO = URL_MODELO.split(\"/\")[-1]\n","\n","        # Nome do arquivo do vocabulário.\n","        ARQUIVO_VOCAB = \"vocab.txt\"\n","\n","        # Caminho do arquivo na url.\n","        CAMINHO_ARQUIVO = URL_MODELO[0:len(URL_MODELO)-len(NOME_ARQUIVO)]\n","\n","        # Verifica se o diretório de descompactação existe no diretório corrente\n","        if os.path.exists(DIRETORIO_MODELO):\n","            logging.info(\"Apagando diretório existente do modelo!\")\n","            # Apaga o diretório e os arquivos existentes\n","            shutil.rmtree(DIRETORIO_MODELO)\n","\n","        # Realiza o download do arquivo do modelo\n","        downloadArquivo(URL_MODELO, NOME_ARQUIVO)\n","\n","        # Descompacta o arquivo no diretório de descompactação.\n","        arquivo_zip = zipfile.ZipFile(NOME_ARQUIVO, \"r\")\n","        arquivo_zip.extractall(DIRETORIO_MODELO)\n","\n","        # Baixa o arquivo do vocabulário.\n","        # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente.\n","        URL_MODELO_VOCAB = CAMINHO_ARQUIVO + ARQUIVO_VOCAB\n","        # Coloca o arquivo do vocabulário no diretório do modelo.\n","        downloadArquivo(URL_MODELO_VOCAB, DIRETORIO_MODELO + \"/\" + ARQUIVO_VOCAB)\n","\n","        # Apaga o arquivo compactado\n","        os.remove(NOME_ARQUIVO)\n","\n","        logging.info(\"Diretório {} do modelo BERT pronta!\".format(DIRETORIO_MODELO))\n","\n","    else:\n","        DIRETORIO_MODELO = MODELO\n","        logging.info(\"Variável URL_MODELO não setada!\")\n","\n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"V74WUpHqcfoI"},"source":["### Copia o modelo do BERT ajustado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQMpf9yycf8f"},"outputs":[],"source":["# Import das bibliotecas.\n","import shutil # iblioteca de manipulação arquivos de alto nível\n","\n","def copiaModeloAjustado(model_args):\n","    \"\"\"\n","      Copia o modelo ajustado BERT do GoogleDrive para o projeto.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `DIRETORIO_LOCAL_MODELO_AJUSTADO` - Diretório de download ajustado do modelo.\n","    \"\"\"\n","\n","    # Verifica o nome do modelo BERT a ser utilizado\n","    MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","    # Verifica o tamanho do modelo(default large)\n","    TAMANHO_BERT = getTamanhoBERT(model_args)\n","\n","    # Verifica se existe o diretório base do cohebert e retorna o nome do diretório\n","    DIRETORIO_COHEBERT = verificaDiretorioCoheBERT()\n","\n","    # Diretório local de salvamento do modelo.\n","    DIRETORIO_LOCAL_MODELO_AJUSTADO = DIRETORIO_COHEBERT + \"/modelo_ajustado/\"\n","\n","    # Diretório remoto de salvamento do modelo no google drive.\n","    DIRETORIO_REMOTO_MODELO_AJUSTADO = \"/content/drive/MyDrive/Colab Notebooks/Data/\" + DIRETORIO_COHEBERT + \"/validacao_classificacao_palavra/holdout/modelo/\" + MODELO_BERT + TAMANHO_BERT\n","\n","    # Copia o arquivo do modelo para o diretório no Google Drive.\n","    shutil.copytree(DIRETORIO_REMOTO_MODELO_AJUSTADO, DIRETORIO_LOCAL_MODELO_AJUSTADO)\n","\n","    logging.info(\"Modelo BERT ajustado copiado!\")\n","\n","    return DIRETORIO_LOCAL_MODELO_AJUSTADO"]},{"cell_type":"markdown","metadata":{"id":"eaneOhAKcO-3"},"source":["### Verifica de onde utilizar o modelo do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTy1TXz3cPKS"},"outputs":[],"source":["def verificaModelo(model_args):\n","    \"\"\"\n","    Verifica de onde utilizar o modelo.\n","\n","    Parâmetros:\n","    `model_args` - Objeto com os argumentos do modelo.\n","\n","    Retorno:\n","    `DIRETORIO_MODELO` - Diretório de download do modelo.\n","    \"\"\"\n","\n","    DIRETORIO_MODELO = None\n","\n","    if model_args.usar_mcl_ajustado == True:\n","        # Diretório do modelo\n","        DIRETORIO_MODELO = copiaModeloAjustado()\n","\n","        logging.info(\"Usando modelo BERT ajustado.\")\n","\n","    else:\n","        DIRETORIO_MODELO = downloadModeloPretreinado(model_args)\n","        logging.info(\"Usando modelo BERT pré-treinado.\")\n","\n","    return DIRETORIO_MODELO"]},{"cell_type":"markdown","metadata":{"id":"6tKcaIfReqdy"},"source":["## 4.2 Tokenizador BERT"]},{"cell_type":"markdown","metadata":{"id":"e8n7Z5s-QZF8"},"source":["### Função carrega Tokenizador BERT\n","\n","O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzAuptkwQZR3"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertTokenizer # Importando as bibliotecas do tokenizador BERT.\n","\n","def carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o tokenizador do DIRETORIO_MODELO.\n","      O tokenizador utiliza WordPiece.\n","      Carregando o tokenizador do diretório \"./modelo/\" do diretório padrão se variável `DIRETORIO_MODELO` setada.\n","      Caso contrário carrega da comunidade\n","      Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n","      O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados.\n","\n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `tokenizer` - Tokenizador BERT.\n","    \"\"\"\n","\n","    tokenizer = None\n","\n","    # Se a variável DIRETORIO_MODELO foi setada.\n","    if DIRETORIO_MODELO:\n","        # Carregando o Tokenizador.\n","        logging.info(\"Carregando o tokenizador BERT do diretório {}.\".format(DIRETORIO_MODELO))\n","\n","        tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, do_lower_case=model_args.do_lower_case)\n","\n","    else:\n","        # Carregando o Tokenizador da comunidade.\n","        logging.info(\"Carregando o tokenizador BERT da comunidade.\")\n","\n","        tokenizer = BertTokenizer.from_pretrained(model_args.pretrained_model_name_or_path, do_lower_case=model_args.do_lower_case)\n","\n","    return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"GYRV9KfHQE6v"},"source":["## 4.3 Carrega o modelo e tokenizador BERT\n","\n","Lista de modelos da comunidade:\n","* https://huggingface.co/models\n","\n","Português(https://github.com/neuralmind-ai/portuguese-bert):  \n","* **\"neuralmind/bert-base-portuguese-cased\"**\n","* **\"neuralmind/bert-large-portuguese-cased\"**"]},{"cell_type":"markdown","metadata":{"id":"-pZZrUKRhR3e"},"source":["### Função carrega modelo BERT medida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JUEyjCChUQh"},"outputs":[],"source":["# Import das bibliotecas.\n","from transformers import BertModel # Importando as bibliotecas do Modelo BERT.\n","\n","def carregaModeloMedida(DIRETORIO_MODELO, model_args):\n","    \"\"\"\n","      Carrega o modelo e retorna o modelo.\n","\n","      Parâmetros:\n","        `DIRETORIO_MODELO` - Diretório a ser utilizado pelo modelo BERT.\n","        `model_args` - Objeto com os argumentos do modelo.\n","\n","      Retorno:\n","        `model` - Um objeto do modelo BERT carregado.\n","    \"\"\"\n","\n","    # Variável para setar o arquivo.\n","    URL_MODELO = None\n","\n","    if \"http\" in model_args.pretrained_model_name_or_path:\n","        URL_MODELO = model_args.pretrained_model_name_or_path\n","\n","    # Se a variável URL_MODELO foi setada\n","    if URL_MODELO:\n","        # Carregando o Modelo BERT\n","        logging.info(\"Carregando o modelo BERT do diretório {} para cálculo de medidas.\".format(DIRETORIO_MODELO))\n","\n","        model = BertModel.from_pretrained(DIRETORIO_MODELO,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","\n","    else:\n","        # Carregando o Modelo BERT da comunidade\n","        logging.info(\"Carregando o modelo BERT da comunidade {} para cálculo de medidas.\".format(model_args.pretrained_model_name_or_path))\n","\n","        model = BertModel.from_pretrained(model_args.pretrained_model_name_or_path,\n","                                          output_attentions=model_args.output_attentions,\n","                                          output_hidden_states=model_args.output_hidden_states)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"-uFDhRTZe2Js"},"source":["### Função carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVtAUbUBe2iS"},"outputs":[],"source":["def carregaBERT(model_args):\n","    \"\"\"\n","      Carrega o BERT para cálculo de medida ou classificação e retorna o modelo e o tokenizador.\n","      O tipo do model retornado pode ser BertModel ou BertForSequenceClassification, depende do tipo de model_args.\n","\n","      Parâmetros:\n","        `model_args` - Objeto com os argumentos do modelo.\n","          - Se model_args = ModeloArgumentosClassificacao deve ser carregado o BERT para classificação(BertForSequenceClassification).\n","          - Se model_args = ModeloArgumentosMedida deve ser carregado o BERT para cálculo de medida(BertModel).\n","\n","      Retorno:\n","        `model` - Um objeto do modelo BERT carregado.\n","        `tokenizer` - Um objeto tokenizador BERT carregado.\n","    \"\"\"\n","\n","    # Verifica a origem do modelo\n","    DIRETORIO_MODELO = verificaModelo(model_args)\n","\n","    # Variável para conter o modelo\n","    model = None\n","\n","    # Carrega o modelo para cálculo da medida\n","    model = carregaModeloMedida(DIRETORIO_MODELO, model_args)\n","\n","    # Carrega o tokenizador.\n","    # O tokenizador é o mesmo para o classificador e medidor.\n","    tokenizer = carregaTokenizadorModeloPretreinado(DIRETORIO_MODELO, model_args)\n","\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"x5NTxBRKfAcT"},"source":["### Carrega o BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYMLJJYSQHY3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728358266,"user_tz":180,"elapsed":13632,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"4793ed83-fe12-4083-fff1-f3138ac5ad20"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Variável URL_MODELO não setada!\n","INFO:root:Usando modelo BERT pré-treinado.\n","INFO:root:Carregando o modelo BERT da comunidade neuralmind/bert-large-portuguese-cased para cálculo de medidas.\n","INFO:root:Carregando o tokenizador BERT do diretório neuralmind/bert-large-portuguese-cased.\n"]}],"source":["# Carrega o modelo e tokenizador do BERT\n","model, tokenizer = carregaBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"d7KprWqyZBQZ"},"source":["### Recupera detalhes do BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6sPjTQnuQV2"},"outputs":[],"source":["# Verifica o nome do modelo BERT a ser utilizado\n","MODELO_BERT = getNomeModeloBERT(model_args)\n","\n","# Verifica o tamanho do modelo(default large)\n","TAMANHO_BERT = getTamanhoBERT(model_args)"]},{"cell_type":"markdown","metadata":{"id":"khTFfBVbnsx9"},"source":["## 4.4 Funções auxiliares do BERT"]},{"cell_type":"markdown","metadata":{"id":"lCJzsw8T0I-5"},"source":["### concatenaListas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpmDZ1mI0JHR"},"outputs":[],"source":["def concatenaListas(lista, pos=1):\n","  lista_concat = []\n","\n","  for x in lista:\n","      lista_concat = lista_concat + x[pos]\n","\n","  return lista_concat"]},{"cell_type":"markdown","metadata":{"id":"s42mgtmSZ8MR"},"source":["### getEmbeddingsCamadas\n","\n","Funções que recuperam os embeddings das camadas:\n","- Primeira camada;\n","- Penúltima camada;\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgo3EBTRZ9-3"},"outputs":[],"source":["def getEmbeddingPrimeiraCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado = output[2][0]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  return resultado\n","\n","def getEmbeddingPenultimaCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado = output[2][-2]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  return resultado\n","\n","def getEmbeddingUltimaCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado = output[2][-1]\n","  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  return resultado\n","\n","def getEmbeddingSoma4UltimasCamadas(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas a primeira(-1) camada\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  embedding_camadas = output[2][-4:]\n","  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  # Usa o método `stack` para criar uma nova dimensão no tensor\n","  # com a concateção dos tensores dos embeddings.\n","  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado_stack = torch.stack(embedding_camadas, dim=0)\n","  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  # Realiza a soma dos embeddings de todos os tokens para as camadas\n","  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  resultado = torch.sum(resultado_stack, dim=0)\n","  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  return resultado\n","\n","def getEmbeddingConcat4UltimasCamadas(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Cria uma lista com os tensores a serem concatenados\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)\n","  # Lista com os tensores a serem concatenados\n","  lista_concat = []\n","\n","  # Percorre os 4 últimos\n","  for i in [-1,-2,-3,-4]:\n","      # Concatena da lista\n","      lista_concat.append(output[2][i])\n","\n","  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)\n","  # Realiza a concatenação dos embeddings de todos as camadas\n","  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)\n","  resultado = torch.cat(lista_concat, dim=-1)\n","\n","  # Saída: Entrada: (<1(lote)> x <qtde_tokens> x <3072 ou 4096>)\n","  return resultado\n","\n","def getEmbeddingSomaTodasAsCamada(output):\n","  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n","\n","  # Retorna todas as camadas descontando a primeira(0)\n","  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  embedding_camadas = output[2][1:]\n","  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","\n","  # Usa o método `stack` para criar uma nova dimensão no tensor\n","  # com a concateção dos tensores dos embeddings.\n","  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)\n","  resultado_stack = torch.stack(embedding_camadas, dim=0)\n","  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  # Realiza a soma dos embeddings de todos os tokens para as camadas\n","  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","  resultado = torch.sum(resultado_stack, dim=0)\n","  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n","\n","  return resultado"]},{"cell_type":"markdown","metadata":{"id":"q7nx_eZ8hSlr"},"source":["### getEmbeddingsVisual\n","\n","Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n","\n","Existe uma função para os tipos de camadas utilizadas:\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLdbOT8-g43V"},"outputs":[],"source":["def getEmbeddingsVisualUltimaCamada(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingUltimaCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAf9lJJ2hZbt"},"outputs":[],"source":["def getEmbeddingsVisualSoma4UltimasCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XpwSN1ghpnz"},"outputs":[],"source":["def getEmbeddingsVisualConcat4UltimasCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3KU1EFrnSPK"},"outputs":[],"source":["def getEmbeddingsVisualSomaTodasAsCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSomaTodasAsCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    # Recupera os embeddings dos tokens como um vetor\n","    embeddings = token_embeddings.numpy()\n","\n","    # Converte para um array\n","    W = np.array(embeddings)\n","    # Transforma em um array\n","    B = np.array([embeddings[0], embeddings[-1]])\n","    # Invertee B.T\n","    Bi = np.linalg.pinv(B.T)\n","\n","    #Projeta a palavra no espaço\n","    Wp = np.matmul(Bi,W.T)\n","\n","    return Wp, documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"Y8MjE0utzlZT"},"source":["### getEmbeddings\n","\n","Função para gerar os embeddings de sentenças.\n","\n","Existe uma função para os tipos de camadas utilizadas:\n","- Ùltima camada;\n","- Soma das 4 últimas camadas;\n","- Concatenação das 4 últimas camadas;\n","- Soma de todas as camadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QcqOuwS067Q"},"outputs":[],"source":["def getEmbeddingsUltimaCamada(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingUltimaCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1wDGBl067Y"},"outputs":[],"source":["def getEmbeddingsSoma4UltimasCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hym19Hxr067Y"},"outputs":[],"source":["def getEmbeddingsConcat4UltimasCamadas(documento, modelo, tokenizer):\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-PLZiUR067Z"},"outputs":[],"source":["def getEmbeddingsSomaTodasAsCamadas(documento, modelo, tokenizer):\n","\n","    # Adiciona os tokens especiais\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Divide a sentença em tokens\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    # Mapeia as strings dos tokens em seus índices do vocabuário\n","    tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n","\n","    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n","    mascara_atencao = [1] * len(documento_tokenizado)\n","\n","    # Converte a entrada em tensores\n","    tokens_tensores = torch.as_tensor([tokens_indexados])\n","    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n","\n","    # Prediz os atributos dos estados ocultos para cada camada\n","    with torch.no_grad():\n","        # Retorno de model quando ´output_hidden_states=True´ é setado:\n","        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n","        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n","\n","    # Camada embedding\n","    camada = getEmbeddingSomaTodasAsCamada(outputs)\n","\n","    # Remove a dimensão 1, o lote \"batches\".\n","    token_embeddings = torch.squeeze(camada, dim=0)\n","\n","    return token_embeddings, documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"zFd1rse11DpZ"},"source":["### getDocumentoTokenizado\n","\n","Retorna o documento tokenizado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvWIBFTLJ7z9"},"outputs":[],"source":["def getDocumentoTokenizado(documento, tokenizer):\n","    \"\"\"\n","      Retorna o documento tokenizado pelo BERT.\n","\n","      Parâmetros:\n","      `documento` - Documento a ser tokenizado.\n","      `tokenizer` - Tokenizador do BERT.\n","    \"\"\"\n","\n","    # Adiciona os tokens especiais.\n","    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n","\n","    # Documento tokenizado\n","    documento_tokenizado = tokenizer.tokenize(documento_marcado)\n","\n","    del tokenizer\n","\n","    return documento_tokenizado"]},{"cell_type":"markdown","metadata":{"id":"3wvgXwN81RCz"},"source":["### encontrarIndiceSubLista\n","\n","Retorna os índices de início e fim da sublista na lista"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abS44M4yvFxf"},"outputs":[],"source":["def encontrarIndiceSubLista(lista: List, sublista: List):\n","    \"\"\"\n","    Localiza os índices de início e fim de uma sublista em uma lista.\n","    Baseado no algoritmo de https://codereview.stackexchange.com/questions/19627/finding-sub-list\n","    de  https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm\n","\n","    Parâmetros:\n","      `lista` - Uma lista.\n","      `sublista` - Uma sublista a ser localizada na lista.\n","\n","    Retorno:\n","      Os índices de início e fim da sublista na lista.\n","    \"\"\"\n","    # Tamanho da lista\n","    h = len(lista)\n","    # Tamanho da sblista\n","    n = len(sublista)\n","    # Cria um dicionário com os saltos descrescentes dos elementos n-1 da sublista\n","    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n","    i = n - 1\n","    # Percorre a lista\n","    while i < h:\n","        # Percorre a sublista\n","        for j in range(n):\n","            # Se elemento da lista diferente da sublista pula a interação\n","            if lista[i - j] != sublista[-j - 1]:\n","              # Passa para o próximo elemento da lista saltando a sublista\n","              i += skip.get(lista[i], n)\n","              # Interrompe o for.\n","              break\n","        else:\n","            #Finalizando a pesquisa depois de executar todo o for(sem break)\n","            indice_inicio = i - n + 1\n","            indice_fim = indice_inicio + len(sublista)-1\n","\n","            # Retorna o início e fim da sublista na lista\n","            return indice_inicio, indice_fim\n","\n","    # Não encontrou a sublista na lista\n","    return -1, -1"]},{"cell_type":"markdown","metadata":{"id":"kGL37G6XFcwp"},"source":["### getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras\n","\n","A partir dos embeddings do documento, localiza o indíce de início e fim de uma sentença no documento e retorna os embeddings da sentença."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uI07Y_M8__HG"},"outputs":[],"source":["def getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embedding_documento,\n","                                                           token_BERT_documento,\n","                                                           sentenca,\n","                                                           tokenizer):\n","\n","  # Tokeniza a sentença\n","  sentenca_tokenizada_BERT = getDocumentoTokenizado(sentenca, tokenizer)\n","  #print(sentenca_tokenizada_BERT)\n","\n","  # Remove os tokens de início e fim da sentença\n","  sentenca_tokenizada_BERT.remove(\"[CLS]\")\n","  sentenca_tokenizada_BERT.remove(\"[SEP]\")\n","  #print(len(sentenca_tokenizada_BERT))\n","\n","  # Localiza os índices dos tokens da sentença no documento\n","  inicio, fim = encontrarIndiceSubLista(token_BERT_documento, sentenca_tokenizada_BERT)\n","  #print(inicio,fim)\n","\n","  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n","  embedding_sentenca = embedding_documento[inicio:fim+1]\n","  #print(\"embedding_sentenca=\", embedding_sentenca.shape)\n","\n","  del tokenizer\n","  del token_BERT_documento\n","  del embedding_documento\n","\n","  # Retorna o embedding da sentença no documento\n","  return embedding_sentenca, sentenca_tokenizada_BERT"]},{"cell_type":"markdown","metadata":{"id":"THFhXGGmIO_r"},"source":["### getEmbeddingDocumentoComTodasPalavrasMean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhW_OiEsIPJI"},"outputs":[],"source":["# Importa a biblioteca\n","import torch\n","\n","def getEmbeddingDocumentoComTodasPalavrasMean(embedding_documento):\n","  \"\"\"\n","    Calcula a média dos embeddings do documento excluindo os tokens\n","    especiais [CLS] do início e [SEP] do fim.\n","    Remove primeira dimensão devido ao cálculo da média.\n","\n","    Parâmetros:\n","    `embedding_documento` - Embedding do documento.\n","  \"\"\"\n","\n","  # Calcula a média dos embeddings para os tokens de embedding_documento, removendo a primeira dimensão.\n","  # Entrada: <qtde_tokens> x <768 ou 1024>\n","  #print(\"embedding_documento1=\", embedding_documento.shape)\n","  media_embedding_documento = torch.mean(embedding_documento[1:-1], dim=0)\n","  # Saída: <768 ou 1024>\n","\n","  del embedding_documento\n","\n","  return media_embedding_documento"]},{"cell_type":"markdown","metadata":{"id":"1Ko_of60YuNd"},"source":["### getEmbeddingDocumentoRelevanteMean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDokSSODY0Sf"},"outputs":[],"source":["# Importa a biblioteca\n","import torch\n","\n","def getEmbeddingDocumentoRelevanteMean(id_documento,\n","                                       index_sentenca,\n","                                       embedding_documento,\n","                                       token_BERT_documento,\n","                                       documento,\n","                                       token_documento,\n","                                       pos_documento,\n","                                       filtro):\n","  \"\"\"\n","    Calcula a média dos embeddings do documento considerando tokens do tipo\n","    especificado no filtro\n","    Remove primeira dimensão devido ao cálculo da média.\n","\n","    Parâmetros:\n","    `embedding_documento` - Embeddings do documento gerados pelo BERT.\n","    `token_BERT_documento` - Lista com os tokens do documento gerados pelo tokenizador BERT.\n","    `documento` - Texto com o documento.\n","    `tokenizer` - Tokenizador do BERT.\n","    `token_documento` - Lista com os tokens do documento.\n","    `pos_documento` - Lista com as POS-Tagging do documento.\n","    `filtro` - Filtro dos embeddings.\n","\n","  \"\"\"\n","\n","  # Recupera a lista de tokens do documento, a lista dos postagging e a lista dos seus embeddings com um mesmo tamanho\n","  lista_tokens, lista_postagging, lista_embeddings = getTokensEmbeddingsPOSSentenca(id_documento,\n","                                                                                    index_sentenca,\n","                                                                                    embedding_documento,\n","                                                                                    token_BERT_documento,\n","                                                                                    documento,\n","                                                                                    token_documento,\n","                                                                                    pos_documento)\n","\n","  #print(\"len(token_BERT_documento):\", len(token_BERT_documento))\n","  #print(\"token_BERT_documento:\", token_BERT_documento)\n","  #print(\"len(pos_documento):\", len(pos_documento))\n","  #print(\"pos_documento:\", pos_documento)\n","  #print(\"filtro:\", filtro)\n","  #print()\n","\n","  # Lista com os tensores selecionados\n","  lista_tokens_selecionados = []\n","  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word no documento\n","  for i, token_documento in enumerate(lista_tokens):\n","      if (lista_postagging[i] in filtro):\n","          #print(\"Adicionando palavra do embedding:\", lista_tokens[i])\n","          lista_tokens_selecionados.append(lista_embeddings[i])\n","\n","  if  len(lista_tokens_selecionados) != 0:\n","      # Empila os embeddings da lista pela dimensão 0\n","      embedding_relevante = torch.stack(lista_tokens_selecionados, dim=0)\n","      #print(\"embedding_relevante.shape:\",embedding_relevante.shape)\n","\n","      # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n","      # Entrada: <qtde_tokens> x <768 ou 1024>\n","      media_embedding_relevante = torch.mean(embedding_relevante, dim=0)\n","      # Saída: <768 ou 1024>\n","      #print(\"media_embedding_relevante.shape:\", media_embedding_relevante.shape)\n","  else:\n","      media_embedding_relevante = None\n","\n","  del embedding_documento\n","  del token_BERT_documento\n","  del documento\n","  del token_documento\n","  del pos_documento\n","\n","  return media_embedding_relevante"]},{"cell_type":"markdown","metadata":{"id":"L_vknrk7YSpF"},"source":["### getEmbeddingDocumentoMean\n","\n","Filtros:\n","- ALL - Sentença com todas as palavras\n","- NOUN - Sentença somente com substantivos\n","- VERB - Sentença somente com verbos\n","- VERB,NOUN - Sentença somente com verbos e substantivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pd8B76YyYS02"},"outputs":[],"source":["def getEmbeddingDocumentoMean(id_documento,\n","                              index_sentenca,\n","                              embedding_documento,\n","                              token_BERT_documento,\n","                              documento,\n","                              tokenizer,\n","                              token_documento,\n","                              pos_documento,\n","                              filtro=[\"ALL\"]):\n","  \"\"\"\n","    Rediciona o cálculo da média dos embeddings de acordo com o filtro especificado.\n","\n","    Parâmetros:\n","    `embedding_documento` - Embeddings do documento gerados pelo BERT.\n","    `token_BERT_documento` - Lista com os tokens do documento gerados pelo tokenizador BERT.\n","    `documento` - Texto com o documento.\n","    `tokenizer` - Tokenizador do BERT.\n","    `token_documento` - Lista com os tokens do documento.\n","    `pos_documento` - Lista com as POS-Tagging do documento.\n","    `filtro` - Filtro dos embeddings.\n","  \"\"\"\n","\n","  if \"ALL\" in filtro:\n","    return getEmbeddingDocumentoComTodasPalavrasMean(embedding_documento)\n","  else:\n","    return getEmbeddingDocumentoRelevanteMean(id_documento,\n","                                              index_sentenca,\n","                                              embedding_documento,\n","                                              token_BERT_documento,\n","                                              documento,\n","                                              token_documento,\n","                                              pos_documento,\n","                                              filtro)"]},{"cell_type":"markdown","metadata":{"id":"t1PgxcL01VfF"},"source":["### getTokensEmbeddingsPOSSentenca\n","Gera os tokens, POS e embeddings de cada sentença."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBkcF2ve1VfG"},"outputs":[],"source":["# Dicionário de tokens de exceções e seus deslocamentos para considerar mais tokens do BERT em relação ao spaCy\n","# A tokenização do BERT gera mais tokens que a tokenização das palavras do spaCy\n","dic_excecao_maior = {\"al-Ḥarrānī\": 3,\n","               \"al-Battānī\": 7,\n","               \"mi.²\":3,\n","               \"nm\":2,\n","               \"550\":2,\n","               \"mg\":1,\n","               \"550nm\":4,\n","               \"q-glass\":5,\n","               \"ômega-6\":3,\n","               str(chr(804)+chr(10217)):1,\n","               \"mm\":1,\n","               \"K\":1,\n","               \"al-qasim\":3,\n","               \"T\":1,\n","               \"ḱlew-\":2,\n","               \"◌\":1,\n","               \"m\":1,\n","               \"TGF-β\":5,\n","               \"56.º\":3,\n","               \"45.º\":3,\n","               \"34.º\":3,\n","               \"nº\":1,\n","               \"200º\":1,\n","               \"1º\":1,\n","               \"g\":1,\n","               \"š\":1,\n","               \"ž\":1,\n","               \"km²\":1,\n","               \"κανὠν\":1,\n","               \"7º\":1,\n","               \"2º\":1,\n","               \"3º\":1,\n","               \"19º\":1,\n","               \"18º\":1,\n","               \"ʱ⟩\":1,\n","               \"ð\":1,\n","               \"θ\":1,\n","               \"Ü-Tsang\":4,\n","               \"p˭\":1,\n","               \"s˭\":1,\n","               \"pʰ\":1,\n","               \"ʰp\":1,\n","               \"sʰ\":1,\n","               \"q\":1,\n","               \"Ṣalībī\":1,\n","               \"ṣalīb\":1,\n","               \"⟨bʰ⟩\":3,\n","               \"Ônibus\":1,\n","               \"2ª\":1,\n","               \"indo-arianas\":4,\n","               \"G\":1,\n","               \"⟨bd\":3,\n","               \"ɡ⟩\":2,\n","               \"tʰ\":1,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXJk5Od51VfI"},"outputs":[],"source":["def getExcecaoDicMaior(token, dic_excecao_maior):\n","\n","  valor = dic_excecao_maior.get(token)\n","  if valor != None:\n","      return valor\n","  else:\n","      return -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6TSm62Y1VfI"},"outputs":[],"source":["# Dicionário de tokens de exceções e seus deslocamentos para considerar menos tokens do BERT em relação ao spaCy\n","# A tokenização do BERT gera menos tokens que a tokenização das palavras do spaCy\n","dic_excecao_menor = {\"1°\":1,\n","                    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OYmoFVk1VfJ"},"outputs":[],"source":["def getExcecaoDicMenor(token, dic_excecao_menor):\n","\n","  valor = dic_excecao_menor.get(token)\n","  if valor != None:\n","      return valor\n","  else:\n","      return -1"]},{"cell_type":"markdown","metadata":{"id":"JzYA5pPE1VfJ"},"source":["Função que retorna os embeddings, tokens e POS da sentença com um mesmo tamanho."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9H1JlTt1VfK"},"outputs":[],"source":["# Importa a biblioteca\n","import torch\n","\n","def getTokensEmbeddingsPOSSentenca(embeddingDocumento,\n","                                   tokenBERTDocumento,\n","                                   sentenca):\n","    \"\"\"\n","      Retorna os tokens, as postagging e os embeddings dos tokens igualando a quantidade de tokens do spaCy com a tokenização do BERT de acordo com a estratégia.\n","      Usa a estratégia MEAN para calcular a média dos embeddings dos tokens que formam uma palavra.\n","      Usa a estratégia MAX para calcular o valor máximo dos embeddings dos tokens que formam uma palavra.\n","    \"\"\"\n","\n","    #Guarda os tokens e embeddings\n","    listaTokens = []\n","    lista_tokens_OOV = []\n","    lista_embeddings_MEAN = []\n","    lista_embeddings_MAX = []\n","\n","    # Gera a tokenização e POS-Tagging da sentença\n","    sentenca_token, sentenca_pos = getListaTokensPOSSentenca(sentenca)\n","\n","    # print(\"\\nsentenca          :\",sentenca)\n","    # print(\"sentenca_token      :\",sentenca_token)\n","    # print(\"len(sentenca_token) :\",len(sentenca_token))\n","    # print(\"sentenca_pos        :\",sentenca_pos)\n","    # print(\"len(sentenca_pos)   :\",len(sentenca_pos))\n","\n","    # Recupera os embeddings da sentença dos embeddings do documento\n","    embeddingSentenca = embeddingDocumento\n","    sentenca_tokenizada_BERT = tokenBERTDocumento\n","\n","    # embedding <qtde_tokens x 4096>\n","    # print(\"embeddingSentenca          :\",embeddingSentenca.shape)\n","    # print(\"sentenca_tokenizada_BERT     :\",sentenca_tokenizada_BERT)\n","    # print(\"len(sentenca_tokenizada_BERT):\",len(sentenca_tokenizada_BERT))\n","\n","    # Seleciona os pares de palavra a serem avaliadas\n","    pos_wi = 0 # Posição do token da palavra gerado pelo spaCy\n","    pos_wj = pos_wi # Posição do token da palavra gerado pelo BERT\n","    pos2 = -1\n","\n","    # Enquanto o indíce da palavra pos_wj(2a palavra) não chegou ao final da quantidade de tokens do BERT\n","    while pos_wj < len(sentenca_tokenizada_BERT):\n","\n","      # Seleciona os tokens da sentença\n","      wi = sentenca_token[pos_wi] # Recupera o token da palavra gerado pelo spaCy\n","      wi1 = \"\"\n","      pos2 = -1\n","      if pos_wi+1 < len(sentenca_token):\n","        wi1 = sentenca_token[pos_wi+1] # Recupera o próximo token da palavra gerado pelo spaCy\n","\n","        # Localiza o deslocamento da exceção\n","        pos2 = getExcecaoDicMenor(wi+wi1, dic_excecao_menor)\n","        #print(\"Exceção pos2:\", pos2)\n","\n","      wj = sentenca_tokenizada_BERT[pos_wj] # Recupera o token da palavra gerado pelo BERT\n","      # print(\"wi[\",pos_wi,\"]=\", wi)\n","      # print(\"wj[\",pos_wj,\"]=\", wj)\n","\n","      # Tratando exceções\n","      # Localiza o deslocamento da exceção\n","      pos = getExcecaoDicMaior(wi, dic_excecao_maior)\n","      #print(\"Exceção pos:\", pos)\n","\n","      if pos != -1 or pos2 != -1:\n","        if pos != -1:\n","          #print(\"Adiciona 1 Exceção palavra == wi or palavra = [UNK]:\",wi)\n","          listaTokens.append(wi)\n","          # Marca como fora do vocabulário do BERT\n","          lista_tokens_OOV.append(1)\n","          # Verifica se tem mais de um token\n","          if pos != 1:\n","            indice_token = pos_wj + pos\n","            #print(\"Calcula a média de :\", pos_wj , \"até\", indice_token)\n","            embeddings_tokens_palavra = embeddingSentenca[pos_wj:indice_token]\n","            #print(\"embeddings_tokens_palavra:\",embeddings_tokens_palavra.shape)\n","            # calcular a média dos embeddings dos tokens do BERT da palavra\n","            embedding_estrategia_MEAN = torch.mean(embeddings_tokens_palavra, dim=0)\n","            #print(\"embedding_estrategia_MEAN:\",embedding_estrategia_MEAN.shape)\n","            lista_embeddings_MEAN.append(embedding_estrategia_MEAN)\n","\n","            # calcular o máximo dos embeddings dos tokens do BERT da palavra\n","            embedding_estrategia_MAX, linha = torch.max(embeddings_tokens_palavra, dim=0)\n","            #print(\"embedding_estrategia_MAX:\",embedding_estrategia_MAX.shape)\n","            lista_embeddings_MAX.append(embedding_estrategia_MAX)\n","          else:\n","            # Adiciona o embedding do token a lista de embeddings\n","            lista_embeddings_MEAN.append(embeddingSentenca[pos_wj])\n","            lista_embeddings_MAX.append(embeddingSentenca[pos_wj])\n","\n","          # Avança para a próxima palavra e token do BERT\n","          pos_wi = pos_wi + 1\n","          pos_wj = pos_wj + pos\n","          #print(\"Proxima:\")\n","          #print(\"wi[\",pos_wi,\"]=\", sentenca_token[pos_wi])\n","          #print(\"wj[\",pos_wj,\"]=\", sentenca_tokenizada_BERT[pos_wj])\n","        else:\n","          if pos2 != -1:\n","            #print(\"Adiciona 1 Exceção palavra == wi or palavra = [UNK]:\",wi)\n","            listaTokens.append(wi+wi1)\n","            # Marca como fora do vocabulário do BERT\n","            lista_tokens_OOV.append(1)\n","            # Verifica se tem mais de um token\n","            if pos2 == 1:\n","              # Adiciona o embedding do token a lista de embeddings\n","              lista_embeddings_MEAN.append(embeddingSentenca[pos_wj])\n","              lista_embeddings_MAX.append(embeddingSentenca[pos_wj])\n","\n","            # Avança para a próxima palavra e token do BERT\n","            pos_wi = pos_wi + 2\n","            pos_wj = pos_wj + pos2\n","            #print(\"Proxima:\")\n","            #print(\"wi[\",pos_wi,\"]=\", sentenca_token[pos_wi])\n","            #print(\"wj[\",pos_wj,\"]=\", sentenca_tokenizada_BERT[pos_wj])\n","      else:\n","        # Tokens iguais adiciona a lista, o token não possui subtoken\n","        if (wi == wj or wj==\"[UNK]\"):\n","          # Adiciona o token a lista de tokens\n","          #print(\"Adiciona 2 wi==wj or wj==[UNK]:\", wi )\n","          listaTokens.append(wi)\n","          # Marca como dentro do vocabulário do BERT\n","          lista_tokens_OOV.append(0)\n","          # Adiciona o embedding do token a lista de embeddings\n","          lista_embeddings_MEAN.append(embeddingSentenca[pos_wj])\n","          lista_embeddings_MAX.append(embeddingSentenca[pos_wj])\n","          #print(\"embedding1[pos_wj]:\", embeddingSentenca[pos_wj].shape)\n","          # Avança para a próxima palavra e token do BERT\n","          pos_wi = pos_wi + 1\n","          pos_wj = pos_wj + 1\n","\n","        else:\n","          # A palavra foi tokenizada pelo Wordpice com ## ou diferente do spaCy ou desconhecida\n","          # Inicializa a palavra a ser montada\n","          palavraPOS = wj\n","          indice_token = pos_wj + 1\n","          while  ((palavraPOS != wi) and indice_token < len(sentenca_tokenizada_BERT)):\n","              if \"##\" in sentenca_tokenizada_BERT[indice_token]:\n","                # Remove os caracteres \"##\" do token\n","                parte = sentenca_tokenizada_BERT[indice_token][2:]\n","              else:\n","                parte = sentenca_tokenizada_BERT[indice_token]\n","\n","              palavraPOS = palavraPOS + parte\n","              #print(\"palavraPOS:\",palavraPOS)\n","              # Avança para o próximo token do BERT\n","              indice_token = indice_token + 1\n","\n","          #print(\"\\nMontei palavra:\",palavraPOS)\n","          if (palavraPOS == wi or palavraPOS == \"[UNK]\"):\n","              # Adiciona o token a lista\n","              #print(\"Adiciona 3 palavra == wi or palavraPOS = [UNK]:\",wi)\n","              listaTokens.append(wi)\n","              # Marca como fora do vocabulário do BERT\n","              lista_tokens_OOV.append(1)\n","              # Calcula a média dos tokens da palavra\n","              #print(\"Calcula o máximo :\", pos_wj , \"até\", indice_token)\n","              embeddings_tokens_palavra = embeddingSentenca[pos_wj:indice_token]\n","              #print(\"embeddings_tokens_palavra2:\",embeddings_tokens_palavra)\n","              #print(\"embeddings_tokens_palavra2:\",embeddings_tokens_palavra.shape)\n","\n","              # calcular a média dos embeddings dos tokens do BERT da palavra\n","              embedding_estrategia_MEAN = torch.mean(embeddings_tokens_palavra, dim=0)\n","              #print(\"embedding_estrategia_MEAN:\",embedding_estrategia_MEAN)\n","              #print(\"embedding_estrategia_MEAN.shape:\",embedding_estrategia_MEAN.shape)\n","              lista_embeddings_MEAN.append(embedding_estrategia_MEAN)\n","\n","              # calcular o valor máximo dos embeddings dos tokens do BERT da palavra\n","              embedding_estrategia_MAX, linha = torch.max(embeddings_tokens_palavra, dim=0)\n","              #print(\"embedding_estrategia_MAX:\",embedding_estrategia_MAX)\n","              #print(\"embedding_estrategia_MAX.shape:\",embedding_estrategia_MAX.shape)\n","              lista_embeddings_MAX.append(embedding_estrategia_MAX)\n","\n","          # Avança para o próximo token do spaCy\n","          pos_wi = pos_wi + 1\n","          # Pula para o próximo token do BERT\n","          pos_wj = indice_token\n","\n","    # Verificação se as listas estão com o mesmo tamanho\n","    #if (len(listaTokens) != len(sentenca_token)) or (len(lista_embeddings_MEAN) != len(sentenca_token)):\n","    if (len(listaTokens) !=  len(lista_embeddings_MEAN)):\n","       print(\"\\nsentenca                :\",sentenca)\n","       print(\"sentenca_pos            :\",sentenca_pos)\n","       print(\"sentenca_token          :\",sentenca_token)\n","       print(\"sentenca_tokenizada_BERT  :\",sentenca_tokenizada_BERT)\n","       print(\"listaTokens             :\",listaTokens)\n","       print(\"len(listaTokens)        :\",len(listaTokens))\n","       print(\"lista_embeddings_MEAN     :\",lista_embeddings_MEAN)\n","       print(\"len(lista_embeddings_MEAN):\",len(lista_embeddings_MEAN))\n","       print(\"lista_embeddings_MAX      :\",lista_embeddings_MAX)\n","       print(\"len(lista_embeddings_MAX) :\",len(lista_embeddings_MAX))\n","\n","    del embeddingSentenca\n","    del tokenBERTDocumento\n","    del sentenca_tokenizada_BERT\n","    del sentenca_token\n","\n","    return listaTokens, sentenca_pos, lista_tokens_OOV, lista_embeddings_MEAN, lista_embeddings_MAX"]},{"cell_type":"markdown","metadata":{"id":"1qezcBkxnEdR"},"source":["# 5 - Exemplo de projeção sem pooling\n","\n","Apresenta os tokens gerados pelo BERT e seus embeddings."]},{"cell_type":"markdown","metadata":{"id":"wUZYlW3yWe0G"},"source":["## 5.1 Carregamento dos arquivos de dados originais e perturbados"]},{"cell_type":"markdown","metadata":{"id":"Gc-JFgxOWe0H"},"source":["### 5.1.1 Especifica os nomes dos arquivos de dados\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1NYLehoWe0H"},"outputs":[],"source":["# Nome do arquivo\n","NOME_ARQUIVO_ORIGINAL = \"original.csv\"\n","NOME_ARQUIVO_ORIGINAL_COMPACTADO = \"original.zip\"\n","NOME_ARQUIVO_ORIGINAL_POS = \"originalpos.csv\"\n","NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO = \"originalpos.zip\"\n","\n","NOME_ARQUIVO_PERTURBADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_COMPACTADO = \"perturbado_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\"\n","NOME_ARQUIVO_PERTURBADO_POS = \"perturbadopos_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".csv\"\n","NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO = \"perturbadopos_p\" + str(model_args.documentos_perturbados) + \"_k\" + str(model_args.top_k_predicao) + \".zip\""]},{"cell_type":"markdown","metadata":{"id":"L2wh9KTVWe0H"},"source":["### 5.1.2 Cria o diretório local para receber os dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gkBBttqWe0H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728359023,"user_tz":180,"elapsed":22,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"447b2986-2fb8-4668-ccdb-b176a9fed859"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Diretório já existe: /content/SQUAD2_P\n"]}],"source":["# Import das bibliotecas.\n","import os\n","\n","# Cria o diretório para receber os arquivos Originais e Perturbados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL[:-1]\n","\n","if not os.path.exists(dirbase):\n","    # Cria o diretório\n","    os.makedirs(dirbase)\n","    logging.info(\"Diretório criado: {}\".format(dirbase))\n","else:\n","    logging.info(\"Diretório já existe: {}\".format(dirbase))"]},{"cell_type":"markdown","metadata":{"id":"eNc-QnB-We0I"},"source":["### 5.1.3 Copia e descompacta os arquivos do Google Drive para o Colaboratory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVmL7bu4We0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728361384,"user_tz":180,"elapsed":2373,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"0bfe37ae-8776-4cb6-a91f-aabfed81c4e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Terminei a cópia!\n"]}],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","  !cp \"$DIRETORIO_DRIVE$NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO\" \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a cópia!\")"]},{"cell_type":"markdown","metadata":{"id":"vsMwewWsWe0I"},"source":["Descompacta os arquivos.\n","\n","Usa o unzip para descompactar:\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens\n","*   `-d` Diretório de destino\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZU3jM6kWe0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728363201,"user_tz":180,"elapsed":1829,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"b6e5a386-eb0c-4318-962e-c8710393b0d4"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Terminei a descompactação!\n"]}],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_ORIGINAL_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","  !unzip -o -j -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PERTURBADO_POS_COMPACTADO\" -d \"$DIRETORIO_LOCAL\"\n","\n","  logging.info(\"Terminei a descompactação!\")"]},{"cell_type":"markdown","metadata":{"id":"721SJjoIWe0I"},"source":["### 5.1.4 Carregamento das lista com os dados dos arquivos originais e perturbados"]},{"cell_type":"markdown","metadata":{"id":"c01p4BFgWe0I"},"source":["#### Carrega o arquivo dos dados originais e POS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vL2mUp1NWe0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728363202,"user_tz":180,"elapsed":27,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"d37e5903-3c5c-44e6-88a7-2a6ef7c4c065"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO ORIGINAIS: 1419.\n","INFO:root:TERMINADO ORIGINAIS POS: 1419.\n"]}],"source":["#Biblioteca\n","import pandas as pd\n","\n","# Abre o arquivo e retorna o DataFrame\n","lista_documentos_originais = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_originais_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_ORIGINAL_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","logging.info(\"TERMINADO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9uMCuCdWe0J","colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"status":"ok","timestamp":1663728363203,"user_tz":180,"elapsed":14,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"1312e8bb-67e3-427d-fdcb-da65a97f341c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                           id  \\\n","561  57324e4cb9d445190005ea08   \n","536  5733d858d058e614000b63c8   \n","900  5a42d0914a4859001aac733f   \n","102  56df93a338dc42170015207a   \n","25   5726a9f3f1498d1400e8e680   \n","\n","                                             sentencas  \\\n","561                      ['Quando o Japão se rendeu?']   \n","536             ['O que significa o lema de Montana?']   \n","900  ['Quais fenômenos assimétricos são comuns na m...   \n","102  ['Quanto Wilber disse que Bell deu a ele para ...   \n","25   ['Quanto da floresta tropical permanece hoje n...   \n","\n","                                             documento  respondivel  \n","561                          Quando o Japão se rendeu?            1  \n","536                 O que significa o lema de Montana?            1  \n","900  Quais fenômenos assimétricos são comuns na mec...            0  \n","102  Quanto Wilber disse que Bell deu a ele para ex...            1  \n","25   Quanto da floresta tropical permanece hoje na ...            1  "],"text/html":["\n","  <div id=\"df-11a868f2-9807-47bb-b34e-170d376be64d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentencas</th>\n","      <th>documento</th>\n","      <th>respondivel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>561</th>\n","      <td>57324e4cb9d445190005ea08</td>\n","      <td>['Quando o Japão se rendeu?']</td>\n","      <td>Quando o Japão se rendeu?</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>536</th>\n","      <td>5733d858d058e614000b63c8</td>\n","      <td>['O que significa o lema de Montana?']</td>\n","      <td>O que significa o lema de Montana?</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>900</th>\n","      <td>5a42d0914a4859001aac733f</td>\n","      <td>['Quais fenômenos assimétricos são comuns na m...</td>\n","      <td>Quais fenômenos assimétricos são comuns na mec...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>102</th>\n","      <td>56df93a338dc42170015207a</td>\n","      <td>['Quanto Wilber disse que Bell deu a ele para ...</td>\n","      <td>Quanto Wilber disse que Bell deu a ele para ex...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>5726a9f3f1498d1400e8e680</td>\n","      <td>['Quanto da floresta tropical permanece hoje n...</td>\n","      <td>Quanto da floresta tropical permanece hoje na ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11a868f2-9807-47bb-b34e-170d376be64d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-11a868f2-9807-47bb-b34e-170d376be64d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-11a868f2-9807-47bb-b34e-170d376be64d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2271}],"source":["lista_documentos_originais.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9tu5sYTWe0J","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1663728363622,"user_tz":180,"elapsed":427,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"6f3d0d66-9cef-4884-dd06-d77a843be02b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                            id  \\\n","320   5acfce2e77cf76001a6860d4   \n","1232  5a60de02e9e1cc001a33cdb6   \n","835   5728b8862ca10214002da658   \n","5     5acd38ac07355d001abf3981   \n","824   56cff221234ae51400d9c140   \n","\n","                                          pos_documento  \n","320   [[['Quantos', 'trabalhos', 'expostos', 'no', '...  \n","1232  [[['O', 'que', 'causa', 'a', 'falta', 'de', 'u...  \n","835   [[['Que', 'governante', 'se', 'opôs', 'ao', 'u...  \n","5     [[['O', 'que', 'são', 'neurônios', 'dinâmicos'...  \n","824   [[['Qual', 'dos', 'maiores', 'fabricantes', 'd...  "],"text/html":["\n","  <div id=\"df-edb1a165-c512-451f-aaf8-e9d6b8ed7287\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>pos_documento</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>320</th>\n","      <td>5acfce2e77cf76001a6860d4</td>\n","      <td>[[['Quantos', 'trabalhos', 'expostos', 'no', '...</td>\n","    </tr>\n","    <tr>\n","      <th>1232</th>\n","      <td>5a60de02e9e1cc001a33cdb6</td>\n","      <td>[[['O', 'que', 'causa', 'a', 'falta', 'de', 'u...</td>\n","    </tr>\n","    <tr>\n","      <th>835</th>\n","      <td>5728b8862ca10214002da658</td>\n","      <td>[[['Que', 'governante', 'se', 'opôs', 'ao', 'u...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5acd38ac07355d001abf3981</td>\n","      <td>[[['O', 'que', 'são', 'neurônios', 'dinâmicos'...</td>\n","    </tr>\n","    <tr>\n","      <th>824</th>\n","      <td>56cff221234ae51400d9c140</td>\n","      <td>[[['Qual', 'dos', 'maiores', 'fabricantes', 'd...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edb1a165-c512-451f-aaf8-e9d6b8ed7287')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-edb1a165-c512-451f-aaf8-e9d6b8ed7287 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-edb1a165-c512-451f-aaf8-e9d6b8ed7287');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2272}],"source":["lista_documentos_originais_pos.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"6pKh_9zjWe0J"},"source":["#### Corrigir os tipos de colunas dos dados originais e POS\n","\n","Em dados originais:\n","- coluna 1 - `sentenças` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados originais pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMe8s7NyWe0J"},"outputs":[],"source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","def corrigirTipoDadosColunasOriginais(lista_documentos_originais, lista_documentos_originais_pos):\n","\n","  # Corrige os tipos dos dados\n","  tipos = {\"id\": str}\n","  lista_documentos_originais = lista_documentos_originais.astype(tipos)\n","  lista_documentos_originais_pos = lista_documentos_originais_pos.astype(tipos)\n","\n","  # Verifica se o tipo da coluna não é list e converte\n","  lista_documentos_originais[\"sentencas\"] = lista_documentos_originais[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","  lista_documentos_originais_pos[\"pos_documento\"] = lista_documentos_originais_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","  logging.info(\"TERMINADO CORREÇÃO ORIGINAIS: {}.\".format(len(lista_documentos_originais)))\n","  logging.info(\"TERMINADO CORREÇÃO ORIGINAIS POS: {}.\".format(len(lista_documentos_originais_pos)))\n","\n","  return lista_documentos_originais, lista_documentos_originais_pos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HO01sA2fZcr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728363624,"user_tz":180,"elapsed":35,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"966232db-4ea7-473b-f910-4b3efdf45318"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO CORREÇÃO ORIGINAIS: 1419.\n","INFO:root:TERMINADO CORREÇÃO ORIGINAIS POS: 1419.\n"]}],"source":["lista_documentos_originais, lista_documentos_originais_pos = corrigirTipoDadosColunasOriginais(lista_documentos_originais, lista_documentos_originais_pos)"]},{"cell_type":"markdown","metadata":{"id":"8yyRt4jnYxsU"},"source":["#### Criando dados indexados originais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9INo4nBS8aQ","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1663728363624,"user_tz":180,"elapsed":22,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"8374cbed-97fd-45b1-a638-2478adb8cac7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                  sentencas  \\\n","id                                                                            \n","5a8d89b5df8bba001a0f9afb  [O formulário Edna do Link é mais rápido do qu...   \n","5acfa4e977cf76001a6856da  [Quais dois ministros lutaram pelo poder em An...   \n","5ad19f40645df0001a2d213b  [O que Irving Langmuir descobriu que aumentari...   \n","56ce66aeaab44d1400b8875a  [Em que ano a célula solar de silício cristali...   \n","5acdabd307355d001abf48f0  [Desde que ano foi levantada a idéia de um tún...   \n","\n","                                                                  documento  \\\n","id                                                                            \n","5a8d89b5df8bba001a0f9afb  O formulário Edna do Link é mais rápido do que...   \n","5acfa4e977cf76001a6856da   Quais dois ministros lutaram pelo poder em Anne?   \n","5ad19f40645df0001a2d213b  O que Irving Langmuir descobriu que aumentaria...   \n","56ce66aeaab44d1400b8875a  Em que ano a célula solar de silício cristalin...   \n","5acdabd307355d001abf48f0  Desde que ano foi levantada a idéia de um túne...   \n","\n","                          respondivel  \n","id                                     \n","5a8d89b5df8bba001a0f9afb            0  \n","5acfa4e977cf76001a6856da            0  \n","5ad19f40645df0001a2d213b            0  \n","56ce66aeaab44d1400b8875a            1  \n","5acdabd307355d001abf48f0            0  "],"text/html":["\n","  <div id=\"df-aa79ab11-3c8e-4c8e-a408-63607572e7de\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentencas</th>\n","      <th>documento</th>\n","      <th>respondivel</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb</th>\n","      <td>[O formulário Edna do Link é mais rápido do qu...</td>\n","      <td>O formulário Edna do Link é mais rápido do que...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5acfa4e977cf76001a6856da</th>\n","      <td>[Quais dois ministros lutaram pelo poder em An...</td>\n","      <td>Quais dois ministros lutaram pelo poder em Anne?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5ad19f40645df0001a2d213b</th>\n","      <td>[O que Irving Langmuir descobriu que aumentari...</td>\n","      <td>O que Irving Langmuir descobriu que aumentaria...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>56ce66aeaab44d1400b8875a</th>\n","      <td>[Em que ano a célula solar de silício cristali...</td>\n","      <td>Em que ano a célula solar de silício cristalin...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5acdabd307355d001abf48f0</th>\n","      <td>[Desde que ano foi levantada a idéia de um tún...</td>\n","      <td>Desde que ano foi levantada a idéia de um túne...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa79ab11-3c8e-4c8e-a408-63607572e7de')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-aa79ab11-3c8e-4c8e-a408-63607572e7de button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-aa79ab11-3c8e-4c8e-a408-63607572e7de');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2275}],"source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_originais_indexado = lista_documentos_originais.set_index([\"id\"])\n","lista_documentos_originais_indexado.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j70x_r30T_bx","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1663728363627,"user_tz":180,"elapsed":23,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"5276607a-f1fe-482d-cc73-32f3c6c51f0d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                              pos_documento\n","id                                                                         \n","5a8d89b5df8bba001a0f9afb  [[[O, formulário, Edna, do, Link, é, mais, ráp...\n","5acfa4e977cf76001a6856da  [[[Quais, dois, ministros, lutaram, pelo, pode...\n","5ad19f40645df0001a2d213b  [[[O, que, Irving, Langmuir, descobriu, que, a...\n","56ce66aeaab44d1400b8875a  [[[Em, que, ano, a, célula, solar, de, silício...\n","5acdabd307355d001abf48f0  [[[Desde, que, ano, foi, levantada, a, idéia, ..."],"text/html":["\n","  <div id=\"df-012fd613-9bfc-4178-bbb2-c6a8e600c771\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos_documento</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb</th>\n","      <td>[[[O, formulário, Edna, do, Link, é, mais, ráp...</td>\n","    </tr>\n","    <tr>\n","      <th>5acfa4e977cf76001a6856da</th>\n","      <td>[[[Quais, dois, ministros, lutaram, pelo, pode...</td>\n","    </tr>\n","    <tr>\n","      <th>5ad19f40645df0001a2d213b</th>\n","      <td>[[[O, que, Irving, Langmuir, descobriu, que, a...</td>\n","    </tr>\n","    <tr>\n","      <th>56ce66aeaab44d1400b8875a</th>\n","      <td>[[[Em, que, ano, a, célula, solar, de, silício...</td>\n","    </tr>\n","    <tr>\n","      <th>5acdabd307355d001abf48f0</th>\n","      <td>[[[Desde, que, ano, foi, levantada, a, idéia, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-012fd613-9bfc-4178-bbb2-c6a8e600c771')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-012fd613-9bfc-4178-bbb2-c6a8e600c771 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-012fd613-9bfc-4178-bbb2-c6a8e600c771');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2276}],"source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_originais_pos_indexado = lista_documentos_originais_pos.set_index([\"id\"])\n","lista_documentos_originais_pos_indexado.head()"]},{"cell_type":"markdown","metadata":{"id":"7YfVcrB5We0J"},"source":["#### Carrega o arquivo dos dados perturbados e POS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WoQ_CEgWWe0J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728363865,"user_tz":180,"elapsed":260,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"7e214393-4152-4ef6-d6ff-c6d20563639c"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO PERTURBADOS: 28380.\n","INFO:root:TERMINADO PERTURBADOS POS: 28380.\n"]}],"source":["# Abre o arquivo e retorna o DataFrame\n","lista_documentos_perturbados = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO, sep=\";\", encoding=\"UTF-8\")\n","lista_documentos_perturbados_pos = pd.read_csv(DIRETORIO_LOCAL + NOME_ARQUIVO_PERTURBADO_POS, sep=\";\", encoding=\"UTF-8\")\n","\n","logging.info(\"TERMINADO PERTURBADOS: {}.\".format(len(lista_documentos_perturbados)))\n","logging.info(\"TERMINADO PERTURBADOS POS: {}.\".format(len(lista_documentos_perturbados_pos)))"]},{"cell_type":"markdown","metadata":{"id":"qzTPk_BsFdzc"},"source":["Alguns csv estão com os nomes das colunas errados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlJ7P-kzFYmR"},"outputs":[],"source":["lista_documentos_perturbados = lista_documentos_perturbados.rename(columns={'documentoPerturbado': 'documento_perturbado'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqMHTHBaWe0J","colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"status":"ok","timestamp":1663728364261,"user_tz":180,"elapsed":401,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"1fd34212-847f-4366-df7f-c37031fb96ca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     id  \\\n","15271  572635e3271a42140099d78a_pert_11   \n","7649    5726d7ebf1498d1400e8ecd4_pert_9   \n","22258  570c2effec8fbc190045bd52_pert_18   \n","20880   572995f63f37b319004784ab_pert_0   \n","4169    5aced34532bba1001ae4b612_pert_9   \n","\n","                                              perturbado  \\\n","15271    ['Como um campeão quer perder um campeonato ?']   \n","7649   ['A \" dança do tapa \" provavelmente imita os m...   \n","22258        ['A que foi útil o Programa Top Hoodlum ?']   \n","20880  ['Em que medida é a arquitetura neoclássica ma...   \n","4169                 ['O que está sendo desenvolvido ?']   \n","\n","                                    documento_perturbado  \\\n","15271        Como um campeão quer perder um campeonato ?   \n","7649   A \" dança do tapa \" provavelmente imita os mov...   \n","22258            A que foi útil o Programa Top Hoodlum ?   \n","20880  Em que medida é a arquitetura neoclássica mais...   \n","4169                     O que está sendo desenvolvido ?   \n","\n","                                               sentencas  \n","15271  [['Como um campeão [MASK] perder um campeonato...  \n","7649   [['A \" dança do tapa \" provavelmente imita os ...  \n","22258  [['A que foi [MASK] o Programa Top Hoodlum ?',...  \n","20880  [['Em que [MASK] é a arquitetura neoclássica m...  \n","4169   [['O que está sendo [MASK] ?', 'descartado', '...  "],"text/html":["\n","  <div id=\"df-9e4acb66-a83c-4f39-91c6-e0e6db5f2117\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>perturbado</th>\n","      <th>documento_perturbado</th>\n","      <th>sentencas</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15271</th>\n","      <td>572635e3271a42140099d78a_pert_11</td>\n","      <td>['Como um campeão quer perder um campeonato ?']</td>\n","      <td>Como um campeão quer perder um campeonato ?</td>\n","      <td>[['Como um campeão [MASK] perder um campeonato...</td>\n","    </tr>\n","    <tr>\n","      <th>7649</th>\n","      <td>5726d7ebf1498d1400e8ecd4_pert_9</td>\n","      <td>['A \" dança do tapa \" provavelmente imita os m...</td>\n","      <td>A \" dança do tapa \" provavelmente imita os mov...</td>\n","      <td>[['A \" dança do tapa \" provavelmente imita os ...</td>\n","    </tr>\n","    <tr>\n","      <th>22258</th>\n","      <td>570c2effec8fbc190045bd52_pert_18</td>\n","      <td>['A que foi útil o Programa Top Hoodlum ?']</td>\n","      <td>A que foi útil o Programa Top Hoodlum ?</td>\n","      <td>[['A que foi [MASK] o Programa Top Hoodlum ?',...</td>\n","    </tr>\n","    <tr>\n","      <th>20880</th>\n","      <td>572995f63f37b319004784ab_pert_0</td>\n","      <td>['Em que medida é a arquitetura neoclássica ma...</td>\n","      <td>Em que medida é a arquitetura neoclássica mais...</td>\n","      <td>[['Em que [MASK] é a arquitetura neoclássica m...</td>\n","    </tr>\n","    <tr>\n","      <th>4169</th>\n","      <td>5aced34532bba1001ae4b612_pert_9</td>\n","      <td>['O que está sendo desenvolvido ?']</td>\n","      <td>O que está sendo desenvolvido ?</td>\n","      <td>[['O que está sendo [MASK] ?', 'descartado', '...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e4acb66-a83c-4f39-91c6-e0e6db5f2117')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9e4acb66-a83c-4f39-91c6-e0e6db5f2117 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9e4acb66-a83c-4f39-91c6-e0e6db5f2117');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2279}],"source":["lista_documentos_perturbados.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMVevEq0We0J","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1663728364265,"user_tz":180,"elapsed":16,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"537acb1f-89b1-46c3-b1f3-cabf994151ed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     id  \\\n","5946    5ad1a38a645df0001a2d2162_pert_6   \n","13364   5ad27baad7d075001a42965d_pert_4   \n","26443   5729499d1d04691400779253_pert_3   \n","6737   572fa080947a6a140053cae0_pert_17   \n","27251  56dff2277aa994140058e29c_pert_11   \n","\n","                                           pos_documento  \n","5946   [[['Quanta', 'água', 'aumentou', 'significativ...  \n","13364  [[['Como', 'eles', 'não', 'sabia', 'que', 'seu...  \n","26443  [[['Qual', 'é', 'o', 'nome', 'do', 'casal', 'd...  \n","6737   [[['Os', 'Estados', 'Unidos', 'e', 'muitos', '...  \n","27251  [[['Que', 'característica', 'notável', 'do', '...  "],"text/html":["\n","  <div id=\"df-d6a8bdb7-bba0-4c02-9f39-f5a68c6cb653\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>pos_documento</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5946</th>\n","      <td>5ad1a38a645df0001a2d2162_pert_6</td>\n","      <td>[[['Quanta', 'água', 'aumentou', 'significativ...</td>\n","    </tr>\n","    <tr>\n","      <th>13364</th>\n","      <td>5ad27baad7d075001a42965d_pert_4</td>\n","      <td>[[['Como', 'eles', 'não', 'sabia', 'que', 'seu...</td>\n","    </tr>\n","    <tr>\n","      <th>26443</th>\n","      <td>5729499d1d04691400779253_pert_3</td>\n","      <td>[[['Qual', 'é', 'o', 'nome', 'do', 'casal', 'd...</td>\n","    </tr>\n","    <tr>\n","      <th>6737</th>\n","      <td>572fa080947a6a140053cae0_pert_17</td>\n","      <td>[[['Os', 'Estados', 'Unidos', 'e', 'muitos', '...</td>\n","    </tr>\n","    <tr>\n","      <th>27251</th>\n","      <td>56dff2277aa994140058e29c_pert_11</td>\n","      <td>[[['Que', 'característica', 'notável', 'do', '...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6a8bdb7-bba0-4c02-9f39-f5a68c6cb653')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d6a8bdb7-bba0-4c02-9f39-f5a68c6cb653 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d6a8bdb7-bba0-4c02-9f39-f5a68c6cb653');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2280}],"source":["lista_documentos_perturbados_pos.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"O8g6aX51We0K"},"source":["#### Corrigir os tipos de colunas dos dados perturbados e POS\n","\n","Em dados perturbados:\n","- coluna 1 - `perturbado` carregadas do arquivo vem como string e não como lista.\n","- coluna 3 - `sentencas` carregadas do arquivo vem como string e não como lista.\n","\n","Em dados perturbados pos:\n","- coluna 1 - `pos_documento` carregadas do arquivo vem como string e não como lista."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lzu2zU9MWe0K"},"outputs":[],"source":["# Import das bibliotecas.\n","import ast # Biblioteca para conversão de string em lista\n","\n","def corrigirTipoDadosColunasPerturbados(lista_documentos_perturbados, lista_documentos_perturbados_pos):\n","\n","  # Corrige os tipos dos dados\n","  tipos = {\"id\": str}\n","  lista_documentos_perturbados = lista_documentos_perturbados.astype(tipos)\n","  lista_documentos_perturbados_pos = lista_documentos_perturbados_pos.astype(tipos)\n","\n","  # Verifica se o tipo da coluna não é list e converte\n","  lista_documentos_perturbados[\"perturbado\"] = lista_documentos_perturbados[\"perturbado\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","  lista_documentos_perturbados[\"sentencas\"] = lista_documentos_perturbados[\"sentencas\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","  lista_documentos_perturbados_pos[\"pos_documento\"] = lista_documentos_perturbados_pos[\"pos_documento\"].apply(lambda x: ast.literal_eval(x) if type(x)!=list else x)\n","\n","  logging.info(\"TERMINADO CORREÇÃO PERTURBADO: {}.\".format(len(lista_documentos_perturbados)))\n","  logging.info(\"TERMINADO CORREÇÃO PERTURBADO POS: {}.\".format(len(lista_documentos_perturbados_pos)))\n","\n","  return lista_documentos_perturbados, lista_documentos_perturbados_pos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SREBF6gTfhZC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728367215,"user_tz":180,"elapsed":2963,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"b8368e9b-00c5-494c-b4ae-3a26c864e0f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO CORREÇÃO PERTURBADO: 28380.\n","INFO:root:TERMINADO CORREÇÃO PERTURBADO POS: 28380.\n"]}],"source":[" lista_documentos_perturbados, lista_documentos_perturbados_pos = corrigirTipoDadosColunasPerturbados(lista_documentos_perturbados, lista_documentos_perturbados_pos)"]},{"cell_type":"markdown","metadata":{"id":"Ix-Q5fZXY3HR"},"source":["#### Criando dados indexados perturbados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqRQnYUtSxzB","colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"status":"ok","timestamp":1663728367216,"user_tz":180,"elapsed":11,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"108b75f0-1d86-427d-8863-a31c3e11802b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                        perturbado  \\\n","id                                                                                   \n","5a8d89b5df8bba001a0f9afb_pert_0  [O form Edna do Link é mais rápido do que outr...   \n","5a8d89b5df8bba001a0f9afb_pert_1  [O Form Edna do Link é mais rápido do que outr...   \n","5a8d89b5df8bba001a0f9afb_pert_2  [O Can Edna do Link é mais rápido do que outro...   \n","5a8d89b5df8bba001a0f9afb_pert_3  [O da Edna do Link é mais rápido do que outro ...   \n","5a8d89b5df8bba001a0f9afb_pert_4  [O Cada Edna do Link é mais rápido do que outr...   \n","\n","                                                              documento_perturbado  \\\n","id                                                                                   \n","5a8d89b5df8bba001a0f9afb_pert_0  O form Edna do Link é mais rápido do que outro...   \n","5a8d89b5df8bba001a0f9afb_pert_1  O Form Edna do Link é mais rápido do que outro...   \n","5a8d89b5df8bba001a0f9afb_pert_2  O Can Edna do Link é mais rápido do que outro ...   \n","5a8d89b5df8bba001a0f9afb_pert_3  O da Edna do Link é mais rápido do que outro f...   \n","5a8d89b5df8bba001a0f9afb_pert_4  O Cada Edna do Link é mais rápido do que outro...   \n","\n","                                                                         sentencas  \n","id                                                                                  \n","5a8d89b5df8bba001a0f9afb_pert_0  [[O [MASK] Edna do Link é mais rápido do que o...  \n","5a8d89b5df8bba001a0f9afb_pert_1  [[O [MASK] Edna do Link é mais rápido do que o...  \n","5a8d89b5df8bba001a0f9afb_pert_2  [[O [MASK] Edna do Link é mais rápido do que o...  \n","5a8d89b5df8bba001a0f9afb_pert_3  [[O [MASK] Edna do Link é mais rápido do que o...  \n","5a8d89b5df8bba001a0f9afb_pert_4  [[O [MASK] Edna do Link é mais rápido do que o...  "],"text/html":["\n","  <div id=\"df-3222179d-0ddc-4d24-b7cc-c3bef5118e6a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>perturbado</th>\n","      <th>documento_perturbado</th>\n","      <th>sentencas</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_0</th>\n","      <td>[O form Edna do Link é mais rápido do que outr...</td>\n","      <td>O form Edna do Link é mais rápido do que outro...</td>\n","      <td>[[O [MASK] Edna do Link é mais rápido do que o...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_1</th>\n","      <td>[O Form Edna do Link é mais rápido do que outr...</td>\n","      <td>O Form Edna do Link é mais rápido do que outro...</td>\n","      <td>[[O [MASK] Edna do Link é mais rápido do que o...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_2</th>\n","      <td>[O Can Edna do Link é mais rápido do que outro...</td>\n","      <td>O Can Edna do Link é mais rápido do que outro ...</td>\n","      <td>[[O [MASK] Edna do Link é mais rápido do que o...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_3</th>\n","      <td>[O da Edna do Link é mais rápido do que outro ...</td>\n","      <td>O da Edna do Link é mais rápido do que outro f...</td>\n","      <td>[[O [MASK] Edna do Link é mais rápido do que o...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_4</th>\n","      <td>[O Cada Edna do Link é mais rápido do que outr...</td>\n","      <td>O Cada Edna do Link é mais rápido do que outro...</td>\n","      <td>[[O [MASK] Edna do Link é mais rápido do que o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3222179d-0ddc-4d24-b7cc-c3bef5118e6a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3222179d-0ddc-4d24-b7cc-c3bef5118e6a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3222179d-0ddc-4d24-b7cc-c3bef5118e6a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2283}],"source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_perturbados_indexado = lista_documentos_perturbados.set_index([\"id\"])\n","lista_documentos_perturbados_indexado.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0aDUbeZT1M8","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1663728367216,"user_tz":180,"elapsed":7,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"fabd79a2-15a9-4eea-c9b9-05dbb04ec52e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                     pos_documento\n","id                                                                                \n","5a8d89b5df8bba001a0f9afb_pert_0  [[[O, form, Edna, do, Link, é, mais, rápido, d...\n","5a8d89b5df8bba001a0f9afb_pert_1  [[[O, Form, Edna, do, Link, é, mais, rápido, d...\n","5a8d89b5df8bba001a0f9afb_pert_2  [[[O, Can, Edna, do, Link, é, mais, rápido, do...\n","5a8d89b5df8bba001a0f9afb_pert_3  [[[O, da, Edna, do, Link, é, mais, rápido, do,...\n","5a8d89b5df8bba001a0f9afb_pert_4  [[[O, Cada, Edna, do, Link, é, mais, rápido, d..."],"text/html":["\n","  <div id=\"df-16fbf761-f263-4fd4-8723-13e9d754e290\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos_documento</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_0</th>\n","      <td>[[[O, form, Edna, do, Link, é, mais, rápido, d...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_1</th>\n","      <td>[[[O, Form, Edna, do, Link, é, mais, rápido, d...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_2</th>\n","      <td>[[[O, Can, Edna, do, Link, é, mais, rápido, do...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_3</th>\n","      <td>[[[O, da, Edna, do, Link, é, mais, rápido, do,...</td>\n","    </tr>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb_pert_4</th>\n","      <td>[[[O, Cada, Edna, do, Link, é, mais, rápido, d...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16fbf761-f263-4fd4-8723-13e9d754e290')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-16fbf761-f263-4fd4-8723-13e9d754e290 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-16fbf761-f263-4fd4-8723-13e9d754e290');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2284}],"source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_perturbados_pos_indexado = lista_documentos_perturbados_pos.set_index([\"id\"])\n","lista_documentos_perturbados_pos_indexado.head()"]},{"cell_type":"markdown","metadata":{"id":"m-vP_FnPWe0K"},"source":["### 5.1.5 Agrupar os dados originais e perturbados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUJBBHy8We0K"},"outputs":[],"source":["# Import das bibliotecas.\n","import ast\n","from tqdm.notebook import tqdm as tqdm_notebook\n","\n","def agruparDadosOriginaisPerturbados(lista_documentos_originais, lista_documentos_perturbados_indexado):\n","\n","  print(\"Processando\",len(lista_documentos_originais),\"documentos originais\")\n","\n","  lista_documentos_agrupados = []\n","\n","  # Se tem algum id no lista do filtro seleciona os documentos originais\n","  if len(FILTRO_DO) != 0:\n","    lista_filtro = lista_documentos_originais[lista_documentos_originais['id'].isin(FILTRO_DO)]\n","\n","    # Barra de progresso dos documentos\n","    lista_documentos_originais_bar = tqdm_notebook(lista_filtro.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(lista_filtro))\n","  else:\n","    # Barra de progresso dos documentos\n","    lista_documentos_originais_bar = tqdm_notebook(lista_documentos_originais.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(lista_documentos_originais))\n","\n","  # Percorre os documentos\n","  for i, linha_documento in lista_documentos_originais_bar:\n","      #if i < 2:\n","      #print(\"linha_documento:\",linha_documento)\n","      # Recupera o id do documento\n","      id_documento = linha_documento[0]\n","\n","      # Carrega a lista das sentenças do documento\n","      lista_sentenca_documento = linha_documento[1]\n","      #print(\"\\nlista_sentenca_documento:\",lista_sentenca_documento)\n","      #print(\"len(lista_sentenca_documento):\",len(lista_sentenca_documento))\n","\n","      # Adiciona o original a lista dos dados agrupados, considerando como coerente(1)\n","      lista_documentos_agrupados.append([id_documento, lista_sentenca_documento, linha_documento[2], 1])\n","\n","      # Percorre os documentos perturbados apartir do original\n","      for j in range(0, model_args.documentos_perturbados):\n","\n","        # Id do documento perturbado\n","        id_perturbado = str(id_documento) + \"_pert_\" + str(j)\n","\n","        # localiza o documento perturbado\n","        #documento_perturbado = lista_documentos_perturbados.loc[lista_documentos_perturbados['id']==id_perturbado].values[0]\n","        documento_perturbado = lista_documentos_perturbados_indexado.loc[id_perturbado]\n","        # Recupera a sentença do documento perturbado\n","        lista_perturbado = documento_perturbado[0]\n","\n","        # Adiciona o perturbado a lista dos dados agrupados considerando como incoerente(0)\n","        lista_documentos_agrupados.append([id_perturbado, lista_perturbado, documento_perturbado[1], 0])\n","\n","  logging.info(\"TERMINADO AGRUPAMENTO: {}.\".format(len(lista_documentos_agrupados)))\n","\n","  # Cria o dataframe da lista\n","  lista_documentos_agrupados = pd.DataFrame(lista_documentos_agrupados, columns = [\"id\",\"sentencas\",\"documento\",\"classe\"])\n","\n","  # Corrige os tipos dos dados da lista agrupada\n","  tipos = {\"id\": str, \"sentencas\": object, \"documento\": str, \"classe\": int}\n","\n","  lista_documentos_agrupados = lista_documentos_agrupados.astype(tipos)\n","\n","  return lista_documentos_agrupados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wY3dqPGVdoHp","colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["95946eb45bca4fca96ce614b93378b80","305312e902b846609a074c5647d0e0d4","566418936e534519ac0c64031ccc1ccf","14f0159ec2084f608177e6d2b6692a65","a5243dd1665c4d3abf215013b0140a7e","4252425986864cfbb568f7c15ac4d1e8","19114a345f3a4314909761dcb09ee186","b513d7f8578e4d3ab68def2af690a2be","39c23cfa81af490da400ee3c632bd461","6957f7500d144e4f886c27f8d0d3f891","935a7643802847f2b4e3c62c32cb4727"]},"executionInfo":{"status":"ok","timestamp":1663728367831,"user_tz":180,"elapsed":621,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"e83b8f73-a07d-4bce-e4fb-b0ebf28c7d33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Analisando documentos originais e perturbados\n","Processando 1419 documentos originais\n"]},{"output_type":"display_data","data":{"text/plain":["Documentos:   0%|          | 0/1 [00:00<?, ? documento/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95946eb45bca4fca96ce614b93378b80"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO AGRUPAMENTO: 21.\n"]}],"source":["# Importa das bibliotecas\n","import pandas as pd\n","\n","print(\"Analisando documentos originais e perturbados\")\n","# Concatena as listas de documentos originais e perturbados\n","lista_documentos_agrupados = agruparDadosOriginaisPerturbados(lista_documentos_originais, lista_documentos_perturbados_indexado)\n","lista_documentos_agrupados_pos = pd.concat([lista_documentos_originais_pos, lista_documentos_perturbados_pos])\n","\n","# Corrige o tipo de dado da coluna id da lista\n","tipos = {\"id\": str}\n","lista_documentos_agrupados_pos = lista_documentos_agrupados_pos.astype(tipos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFvbzDbwdrYx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728367832,"user_tz":180,"elapsed":120,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"b15cdeb0-ed1c-416a-c963-ad764c404adc"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO AGRUPAMENTO: 21.\n"]}],"source":["logging.info(\"TERMINADO AGRUPAMENTO: {}.\".format(len(lista_documentos_agrupados)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5ZV4jzAWe0K","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1663728367832,"user_tz":180,"elapsed":31,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"fb454fd9-9d71-4060-8d6e-804c43730122"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                  id  \\\n","10   5728046cff5b5019007d9b05_pert_9   \n","13  5728046cff5b5019007d9b05_pert_12   \n","16  5728046cff5b5019007d9b05_pert_15   \n","7    5728046cff5b5019007d9b05_pert_6   \n","18  5728046cff5b5019007d9b05_pert_17   \n","\n","                                    sentencas  \\\n","10   [Como os bytes de bit são organizados ?]   \n","13   [Como os bytes de bit são construídos ?]   \n","16        [Como os bytes de bit são feitos ?]   \n","7      [Como os bytes de bit são definidos ?]   \n","18  [Como os bytes de bit são transmitidos ?]   \n","\n","                                  documento  classe  \n","10   Como os bytes de bit são organizados ?       0  \n","13   Como os bytes de bit são construídos ?       0  \n","16        Como os bytes de bit são feitos ?       0  \n","7      Como os bytes de bit são definidos ?       0  \n","18  Como os bytes de bit são transmitidos ?       0  "],"text/html":["\n","  <div id=\"df-7446a5c3-eff2-4e6c-8215-1d012c56ac9e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentencas</th>\n","      <th>documento</th>\n","      <th>classe</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10</th>\n","      <td>5728046cff5b5019007d9b05_pert_9</td>\n","      <td>[Como os bytes de bit são organizados ?]</td>\n","      <td>Como os bytes de bit são organizados ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>5728046cff5b5019007d9b05_pert_12</td>\n","      <td>[Como os bytes de bit são construídos ?]</td>\n","      <td>Como os bytes de bit são construídos ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>5728046cff5b5019007d9b05_pert_15</td>\n","      <td>[Como os bytes de bit são feitos ?]</td>\n","      <td>Como os bytes de bit são feitos ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>5728046cff5b5019007d9b05_pert_6</td>\n","      <td>[Como os bytes de bit são definidos ?]</td>\n","      <td>Como os bytes de bit são definidos ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>5728046cff5b5019007d9b05_pert_17</td>\n","      <td>[Como os bytes de bit são transmitidos ?]</td>\n","      <td>Como os bytes de bit são transmitidos ?</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7446a5c3-eff2-4e6c-8215-1d012c56ac9e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7446a5c3-eff2-4e6c-8215-1d012c56ac9e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7446a5c3-eff2-4e6c-8215-1d012c56ac9e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2288}],"source":["lista_documentos_agrupados.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4mCqTRecWe0L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728367833,"user_tz":180,"elapsed":30,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"75f80543-3193-41c1-922f-8f5ec951a8f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:TERMINADO AGRUPAMENTO POS: 29799.\n"]}],"source":["logging.info(\"TERMINADO AGRUPAMENTO POS: {}.\".format(len(lista_documentos_agrupados_pos)))"]},{"cell_type":"markdown","metadata":{"id":"viicg1E7mXLK"},"source":["#### Criar dados indexados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YBdkvoPm2vO","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1663728367834,"user_tz":180,"elapsed":23,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"f6ca5f33-7d36-492a-e22e-c4d7f21de3d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                  sentencas  \\\n","id                                                                            \n","5728046cff5b5019007d9b05           [Como os bytes de bit são transmitidos?]   \n","5728046cff5b5019007d9b05_pert_0  [Como os bytes de bit são classificados ?]   \n","5728046cff5b5019007d9b05_pert_1       [Como os bytes de bit são escritos ?]   \n","5728046cff5b5019007d9b05_pert_2       [Como os bytes de bit são gravados ?]   \n","5728046cff5b5019007d9b05_pert_3       [Como os bytes de bit são formados ?]   \n","\n","                                                                documento  \\\n","id                                                                          \n","5728046cff5b5019007d9b05           Como os bytes de bit são transmitidos?   \n","5728046cff5b5019007d9b05_pert_0  Como os bytes de bit são classificados ?   \n","5728046cff5b5019007d9b05_pert_1       Como os bytes de bit são escritos ?   \n","5728046cff5b5019007d9b05_pert_2       Como os bytes de bit são gravados ?   \n","5728046cff5b5019007d9b05_pert_3       Como os bytes de bit são formados ?   \n","\n","                                 classe  \n","id                                       \n","5728046cff5b5019007d9b05              1  \n","5728046cff5b5019007d9b05_pert_0       0  \n","5728046cff5b5019007d9b05_pert_1       0  \n","5728046cff5b5019007d9b05_pert_2       0  \n","5728046cff5b5019007d9b05_pert_3       0  "],"text/html":["\n","  <div id=\"df-56b0731d-22ea-46dc-ba5b-0a77691049dc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentencas</th>\n","      <th>documento</th>\n","      <th>classe</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5728046cff5b5019007d9b05</th>\n","      <td>[Como os bytes de bit são transmitidos?]</td>\n","      <td>Como os bytes de bit são transmitidos?</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5728046cff5b5019007d9b05_pert_0</th>\n","      <td>[Como os bytes de bit são classificados ?]</td>\n","      <td>Como os bytes de bit são classificados ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5728046cff5b5019007d9b05_pert_1</th>\n","      <td>[Como os bytes de bit são escritos ?]</td>\n","      <td>Como os bytes de bit são escritos ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5728046cff5b5019007d9b05_pert_2</th>\n","      <td>[Como os bytes de bit são gravados ?]</td>\n","      <td>Como os bytes de bit são gravados ?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5728046cff5b5019007d9b05_pert_3</th>\n","      <td>[Como os bytes de bit são formados ?]</td>\n","      <td>Como os bytes de bit são formados ?</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56b0731d-22ea-46dc-ba5b-0a77691049dc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-56b0731d-22ea-46dc-ba5b-0a77691049dc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-56b0731d-22ea-46dc-ba5b-0a77691049dc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2290}],"source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_agrupados_indexado = lista_documentos_agrupados.set_index([\"id\"])\n","lista_documentos_agrupados_indexado.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQjlOJzOmbsp","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1663728367834,"user_tz":180,"elapsed":22,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"51e9bcbf-fbc0-4352-8453-a44846d1377c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                              pos_documento\n","id                                                                         \n","5a8d89b5df8bba001a0f9afb  [[[O, formulário, Edna, do, Link, é, mais, ráp...\n","5acfa4e977cf76001a6856da  [[[Quais, dois, ministros, lutaram, pelo, pode...\n","5ad19f40645df0001a2d213b  [[[O, que, Irving, Langmuir, descobriu, que, a...\n","56ce66aeaab44d1400b8875a  [[[Em, que, ano, a, célula, solar, de, silício...\n","5acdabd307355d001abf48f0  [[[Desde, que, ano, foi, levantada, a, idéia, ..."],"text/html":["\n","  <div id=\"df-306d0e73-0fae-4e0a-9806-e13e2cddfead\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos_documento</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5a8d89b5df8bba001a0f9afb</th>\n","      <td>[[[O, formulário, Edna, do, Link, é, mais, ráp...</td>\n","    </tr>\n","    <tr>\n","      <th>5acfa4e977cf76001a6856da</th>\n","      <td>[[[Quais, dois, ministros, lutaram, pelo, pode...</td>\n","    </tr>\n","    <tr>\n","      <th>5ad19f40645df0001a2d213b</th>\n","      <td>[[[O, que, Irving, Langmuir, descobriu, que, a...</td>\n","    </tr>\n","    <tr>\n","      <th>56ce66aeaab44d1400b8875a</th>\n","      <td>[[[Em, que, ano, a, célula, solar, de, silício...</td>\n","    </tr>\n","    <tr>\n","      <th>5acdabd307355d001abf48f0</th>\n","      <td>[[[Desde, que, ano, foi, levantada, a, idéia, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-306d0e73-0fae-4e0a-9806-e13e2cddfead')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-306d0e73-0fae-4e0a-9806-e13e2cddfead button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-306d0e73-0fae-4e0a-9806-e13e2cddfead');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2291}],"source":["# Especifica o(s) campo(s) indexado(s) e faz uma cópia da lista indexada\n","lista_documentos_agrupados_pos_indexado = lista_documentos_agrupados_pos.set_index([\"id\"])\n","lista_documentos_agrupados_pos_indexado.head()"]},{"cell_type":"markdown","source":["### 5.1.6 Funções auxiliares"],"metadata":{"id":"pnY7O9zb8n8Z"}},{"cell_type":"markdown","source":["#### getIndicePalavraPerturbada\n","\n","Retorna o índice da palavra perturbada em um documento"],"metadata":{"id":"KhhvmLDQbg4z"}},{"cell_type":"code","source":["def getIndicePalavraPerturbada(_id_perturbado):\n","\n","  # print(\"_id_perturbado:\",_id_perturbado)\n","\n","  # localiza os dados do documento perturbado mascarado\n","  reg_documento_perturbado = lista_documentos_perturbados_indexado.loc[_id_perturbado]\n","\n","  # Recupera a lista das sentenças perturbadas\n","  lista_sentencas_mascarada = reg_documento_perturbado[2]\n","\n","  # Índice da sentença perturbada\n","  index_sentenca = -1\n","\n","  # Percorre as sentenças para encontrar a sentença perturbada\n","  for i, linha in enumerate(lista_sentencas_mascarada):\n","\n","    # Identifica a sentença mascarada que foi perturbada\n","    if 'MASK' in linha[0] :\n","      # Recupera a palavra mascarada sentença do documento perturbado\n","      index_sentenca = i\n","      sentenca_mascarada = linha[0]\n","      palavra_mascarada = linha[1]\n","      token_predito = linha[2]\n","      peso_predito = linha[3]\n","\n","  # localiza os dados do documento perturbado pos\n","  reg_documento_perturbado_pos = lista_documentos_perturbados_pos_indexado.loc[_id_perturbado]\n","  # print(\"reg_documento_perturbado_pos:\",reg_documento_perturbado_pos)\n","\n","  # Recupera as POS Tagging do documento perturbado\n","  tokens_perturbado_index_palavra = []\n","\n","  # Recupera os pos das sentenças\n","  pos_documento_perturbado = reg_documento_perturbado_pos['pos_documento']\n","  # print(\"pos_documento_perturbado:\",pos_documento_perturbado)\n","\n","  # Percorre as sentenças do documento\n","  for i, linha1 in enumerate(pos_documento_perturbado):\n","    # print(\"linha1:\", linha1)\n","\n","    # Percorre os tokens da sentença\n","    for j, linha2 in enumerate(linha1[0]):\n","      # print(\"linha2:\", linha2)\n","      # Localiza o indice da palavra perturbada na sentença\n","      if token_predito == linha2:\n","        # Guarda o indice da palavra perturbada\n","        tokens_perturbado_index_palavra.append(j)\n","\n","  # Verifica se encontrou o índice da palavra perturbada\n","  if len(tokens_perturbado_index_palavra) != 0:\n","      # Possui somente uma palavra perturbada\n","      if len(tokens_perturbado_index_palavra) == 1:\n","        return tokens_perturbado_index_palavra[0]\n","      else:\n","        return tokens_perturbado_index_palavra\n","  else:\n","    # Não encontrou o índice da palavra perturbada\n","    return -1"],"metadata":{"id":"509NcnQNUnDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getIndicePerturbacao(_id_documento):\n","\n","  id_documento_perturbado = \"\"\n","\n","  # Verifica o tipo do documento\n","  #if int(_id_documento)/2 == 1:\n","  if \"_pert_\" in _id_documento:\n","    # Documento perturbado\n","    id_documento_perturbado = _id_documento\n","\n","  else:\n","    # Pega o primeiro documento perturbado para localizar posição\n","    id_documento_perturbado = _id_documento + \"_pert_0\"\n","    #id_documento_perturbado = str(int(_id_documento) + 1)\n","\n","  # Retorna o índice\n","  index_perturbacao = getIndicePalavraPerturbada(id_documento_perturbado)\n","\n","  return index_perturbacao"],"metadata":{"id":"20I7k_2M-rin"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuM-kq1upR3M"},"source":["## 5.2 Gera os arquivos para o Embedding Projector"]},{"cell_type":"markdown","metadata":{"id":"8FRocbK4_wTk"},"source":["### 5.2.1 Cria o diretório para os arquivos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaGALkXc_zLl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728367836,"user_tz":180,"elapsed":22,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"c0e39950-8705-4e04-8bec-c5317374dba6"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Diretório já existe: /content/SQUAD2_P/projector\n"]}],"source":["# Importando as bibliotecas.\n","import os\n","\n","# Cria o diretório para receber os arquivos Originais e Permutados\n","# Diretório a ser criado\n","dirbase = DIRETORIO_LOCAL + \"projector\"\n","\n","if not os.path.exists(dirbase):\n","    # Cria o diretório\n","    os.makedirs(dirbase)\n","    logging.info(\"Diretório criado: {}\".format(dirbase))\n","else:\n","    logging.info(\"Diretório já existe: {}\".format(dirbase))"]},{"cell_type":"markdown","metadata":{"id":"eFugAWlm1VfN"},"source":["### 5.2.2 Gera os embeddings dos documentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maBbfHFj1VfN","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["31fdf67158d446d8a9d1dd7c7a45b563","ae1caa61ed664577877d58db5fac04cf","1948bb78a78c4ab2b0026c1ebaabdc0f","4a26579cf3d540199ddbef76259f8600","f91a3d82ad974e178472cf6506669915","eaf8957f801b4ec1be732d2bf7cbafdf","414b70eb176e4febb86627b68166838d","84dcd319bc8d423e804040a3c9439a16","afcdaeb99ecb410897c83061de935e5d","dd6adc2228c0422da5f5a2a2800a1129","49cd605274e44de2991c15bfcb60f1cf"]},"executionInfo":{"status":"ok","timestamp":1663728377057,"user_tz":180,"elapsed":9237,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"ad487658-57bb-4f17-b962-1f0a5414cdc8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Documentos:   0%|          | 0/21 [00:00<?, ? documento/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31fdf67158d446d8a9d1dd7c7a45b563"}},"metadata":{}}],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","\n","lista_embeddings = []\n","lista_embeddings_documento = []\n","lista_documentos = []\n","lista_documentos_tokenizado = []\n","lista_documentos_tokenizado_oov = []\n","lista_documentos_pos = []\n","lista_documentos_classe = []\n","lista_documentos_id = []\n","lista_documentos_origem = []\n","\n","maior_sequencia = 0\n","\n","total_tokens = 0\n","\n","if CLASSE_DOCUMENTO != 2:\n","  documentos = lista_documentos_agrupados.loc[lista_documentos_agrupados['classe'] == CLASSE_DOCUMENTO]\n","else:\n","  documentos = lista_documentos_agrupados\n","\n","# Barra de progresso dos documentos\n","documentos_bar = tqdm_notebook(documentos.iterrows(), desc=f\"Documentos\", unit=f\" documento\", total=len(documentos))\n","\n","# Percorre os documentos\n","for i, linha_documento in documentos_bar:\n","\n","    # Recupera o id do documento\n","    id_documento = linha_documento[0]\n","    # print(\"id_documento:\",id_documento)\n","    # print(\"linha_documento['documento']:\", linha_documento['documento'])\n","\n","    # Recupera a classe documento (1-original 0-perturbado)\n","    classe = linha_documento['classe']\n","    #print(\"classe:\",classe)\n","\n","    # Localiza a POSTagging do documento agrupado\n","    lista_pos_documento = lista_documentos_agrupados_pos_indexado.loc[id_documento][0]\n","    # print(\"lista_pos_documento:\",lista_pos_documento)\n","    # print(\"len(lista_pos_documento):\",len(lista_pos_documento))\n","\n","    # Troca o documento por uma versão da concatenação das palavras geradas pelo spaCy\n","    # Percorre a lista_pos concatenando a posição 0 dos tokens\n","    documento_concatenado = \" \".join(concatenaListas(lista_pos_documento, pos=0))\n","    # print(\"documento_concatenado:\", documento_concatenado)\n","    documento = documento_concatenado\n","\n","    if CLASSE_DOCUMENTO != 1:\n","      # Recupera a posição do traço no id do arquivo\n","      traco_ix = id_documento.find(\"_\")\n","      if traco_ix != -1:\n","        # Recupera o id da perturbacao até a posição do traço até o fim\n","        id_perturbacao = id_documento[:traco_ix]\n","      else:\n","        id_perturbacao = id_documento\n","\n","    if POOLING_TOKENS == 0:\n","\n","        # Recupera os embeddings\n","        if ESTRATEGIA_EMBEDDING == 1:\n","          # Gera embeddings da última camada do BERT\n","          token_embeddings, documento_tokenizado =  getEmbeddingsUltimaCamada(documento, model, tokenizer)\n","        else:\n","          # Gera embeddings concatenando as 4 últimas camadas do BERT\n","          token_embeddings, documento_tokenizado = getEmbeddingsConcat4UltimasCamadas(documento, model, tokenizer)\n","\n","        # Guarda o maior tamanho de documento\n","        if len(documento_tokenizado) > maior_sequencia:\n","            maior_sequencia =  len(documento_tokenizado)\n","\n","        # Guarda o total de tokens dos documentos\n","        total_tokens = total_tokens + len(documento_tokenizado)\n","\n","        # Guarda os embeddings e o documento tokenizado\n","        lista_embeddings.append(token_embeddings)\n","        # Guarda os embeddings do documento consolidado pela média e removendo os tokens [CLS] e [SEP]\n","        lista_embeddings_documento.append(torch.mean(token_embeddings[1:-1], dim=0))\n","        lista_documentos.append(documento)\n","        lista_documentos_tokenizado.append(documento_tokenizado)\n","        lista_documentos_classe.append(classe)\n","        lista_documentos_id.append(id_documento)\n","        if CLASSE_DOCUMENTO != 1:\n","          lista_documentos_origem.append(id_perturbacao)\n","\n","    else:\n","        # Recupera os embeddings\n","        if ESTRATEGIA_EMBEDDING == 1:\n","          # Gera embeddings da última camada do BERT\n","          token_embeddings, documento_tokenizado =  getEmbeddingsUltimaCamada(documento, model, tokenizer)\n","        else:\n","          # Gera embeddings concatenando as 4 últimas camadas do BERT\n","          token_embeddings, documento_tokenizado = getEmbeddingsConcat4UltimasCamadas(documento, model, tokenizer)\n","\n","        # Combina os embeddings de palavras fora do vocabulário do BERT\n","        listaTokens, listaPOS, lista_tokens_OOV, listaEmbeddingsMEAN, listaEmbeddingsMAX =  getTokensEmbeddingsPOSSentenca(token_embeddings[1:-1],\n","                                                                                                        documento_tokenizado[1:-1],\n","                                                                                                        documento)\n","\n","        # Guarda o maior tamanho de documento\n","        if len(listaTokens) > maior_sequencia:\n","            maior_sequencia =  len(listaTokens)\n","\n","        # Guarda o total de tokens dos documentos\n","        total_tokens = total_tokens + len(listaTokens)\n","\n","        # Guarda os embeddings e os os outros dados do documento\n","        lista_embeddings.append(listaEmbeddingsMEAN)\n","        # Guarda os embeddings do documento consolidado pela média e removendo os tokens [CLS] e [SEP]\n","        lista_embeddings_documento.append(torch.mean(token_embeddings[1:-1], dim=0))\n","        lista_documentos.append(documento)\n","        lista_documentos_tokenizado.append(listaTokens)\n","        lista_documentos_tokenizado_oov.append(lista_tokens_OOV)\n","        lista_documentos_pos.append(listaPOS)\n","        lista_documentos_classe.append(classe)\n","        lista_documentos_id.append(id_documento)\n","        if CLASSE_DOCUMENTO != 1:\n","          lista_documentos_origem.append(id_perturbacao)"]},{"cell_type":"markdown","metadata":{"id":"VOyrUs1d1VfO"},"source":["Mostra um documento processado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLp3DWq61VfO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728377058,"user_tz":180,"elapsed":46,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"ac62b498-1c48-4a49-cb81-1bde7e37b622"},"outputs":[{"output_type":"stream","name":"stdout","text":["8\n","['Como', 'os', 'bytes', 'de', 'bit', 'são', 'transmitidos', '?']\n","1\n"]}],"source":["print(len(lista_embeddings[0]))\n","print(lista_documentos_tokenizado[0])\n","print(lista_documentos_classe[0])"]},{"cell_type":"markdown","metadata":{"id":"apK3F5W41VfO"},"source":["Quantidade de tokens nos documentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQvbl4XE1VfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728377493,"user_tz":180,"elapsed":465,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"a0b569e6-eb1c-4247-fb1f-603dc26ef92f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Quantidade de tokens: 168\n"]}],"source":["print(\"Quantidade de tokens:\", total_tokens)"]},{"cell_type":"markdown","metadata":{"id":"qvVjozxA1VfP"},"source":["Maior tamanho  de documento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gvZGxIp1VfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728377493,"user_tz":180,"elapsed":21,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"a20fdec3-16f9-4a7a-ab4f-b2966617cb00"},"outputs":[{"output_type":"stream","name":"stdout","text":["max_seq_length: 8\n"]}],"source":["print(\"max_seq_length:\", maior_sequencia)"]},{"cell_type":"markdown","metadata":{"id":"za-9WAYO1VfQ"},"source":["### 5.2.3 Gera os arquivos para o Embedding Projector"]},{"cell_type":"markdown","metadata":{"id":"R9zqngHokA0-"},"source":["Gera o sufixo do nome do arquivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNiX8OU7K4-G"},"outputs":[],"source":["def getSufixoNomeArquivo():\n","\n","  sufixo_arquivo = \"_\"\n","\n","  # Documento perturbados\n","  if CLASSE_DOCUMENTO == 0:\n","      sufixo_arquivo = sufixo_arquivo + \"PERTDO\" + \"_P\" + str(DOCUMENTOS_PERTURBADOS)\n","  else:\n","    # Documento originais\n","    if CLASSE_DOCUMENTO == 1:\n","      sufixo_arquivo = sufixo_arquivo + \"DO\"\n","    else:\n","      # Documento originais e perturbados\n","      if CLASSE_DOCUMENTO == 2:\n","        sufixo_arquivo = sufixo_arquivo + \"DO_PERTDO\"  + \"_P\" + str(DOCUMENTOS_PERTURBADOS) + \"_CLASSE\"\n","\n","  # Sem pooling dos tokens\n","  if POOLING_TOKENS == 0:\n","    # Tamanho dos embeddings\n","    sufixo_arquivo = sufixo_arquivo + \"_\" + str(lista_embeddings[0].size()[1]) + TAMANHO_BERT\n","\n","    # Não possui o prefixo pooling\n","  else:\n","    # Com pooling dos tokens\n","    if POOLING_TOKENS == 1:\n","      sufixo_arquivo = sufixo_arquivo + \"_\" + str(lista_embeddings[0][0].size()[0]) + TAMANHO_BERT\n","\n","      # Adiciona o prefixo\n","      sufixo_arquivo = sufixo_arquivo + \"_POOL\"\n","\n","  return sufixo_arquivo"]},{"cell_type":"markdown","metadata":{"id":"uCR6vo371VfQ"},"source":["Arquivos com os valores dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbILMYpG1VfQ","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["2910dbdc94d14e71b458e84a36a48bf5","1a69dcccd32e4210b6c512e3d89f959e","30d8770f43604bf6b337950b3c13b9cb","4f970be5c04f4bf197df36ccf2551a4b","2421ede0ae2e424ab45e2053a9c19454","adfc49b3d7e74aff8f0450a96b12d04a","2205e9db175e4c8c8b92cb641ba8aa3c","a8afe154a80945f9a773c4837a50b580","a80fffa293554cec9e7e8c9893d4a659","caf21b1bdf1548729fd4363daf8ff64c","53e85f354c66411d9773b6d77c2f0d2c"]},"executionInfo":{"status":"ok","timestamp":1663728377855,"user_tz":180,"elapsed":375,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"a8754c68-b7cd-4bf8-93f0-1a9e477734f8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Embeddings:   0%|          | 0/21 [00:00<?, ? embedding/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2910dbdc94d14e71b458e84a36a48bf5"}},"metadata":{}}],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","import csv\n","\n","# Recupera o sufixo do nome do arquivo\n","sufixo_arquivo = getSufixoNomeArquivo()\n","#print(\"sufixo_arquivo:\", sufixo_arquivo)\n","\n","NOME_ARQUIVO_RECORD =  DIRETORIO_LOCAL + \"projector/\" + FILTRO_STR[IDDO] + \"records_token_sentenca\" + sufixo_arquivo + \".tsv\"\n","\n","# Abre o arquivo\n","with open(NOME_ARQUIVO_RECORD, 'w', encoding='utf8') as tsvfile:\n","  # Cria um arquivo separado por tab\n","    writer = csv.writer(tsvfile, delimiter='\\t')\n","\n","    # Barra de progresso dos embedings\n","    lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","    # Percorre os embeddings\n","    for i, documento_embedding in lista_embeddings_bar:\n","\n","      if POOLING_TOKENS == 0:\n","        # Converte os tensores em numpy array\n","        documento_embedding_np =  documento_embedding.numpy()\n","\n","        # Qtde de tokens do documento\n","        length = len(lista_documentos_tokenizado[i])\n","\n","        # Escreve no arquivo os embeddings das palavras\n","        writer.writerows(documento_embedding_np[:length])\n","\n","      else:\n","        # Converte os tensores em numpy array\n","        documento_embedding_np = []\n","        for linha in documento_embedding:\n","            novo = linha.numpy()\n","            documento_embedding_np.append(novo)\n","\n","        # Qtde de tokens do documento\n","        length = len(lista_documentos_tokenizado[i])\n","\n","        # Escreve no arquivo os embeddings das palavras\n","        writer.writerows(documento_embedding_np[:length])\n","\n","      # Escreve no arquivo os embeddings do documento\n","      writer.writerows([lista_embeddings_documento[i].numpy()])"]},{"cell_type":"markdown","metadata":{"id":"mb580LQT1VfR"},"source":["Arquivo com os metadados dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XskFjQ5v1VfR","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["df5f41b76cf34adfa249330e6d59c8b7","7b56492f257946d592396235e8d09855","8bfa8c6bd1304c01b2557fb545402893","d76457b211e2493d953e5dd3c2b34d33","fd12e5aadf5d4198b5a6d46c07854fd8","87c3ea2009254516948d1be0c397959e","48f17d147a654044981ab9dc72e9d4db","3a806a2c2f354caa97cb431701ea84b2","5d8189ac12ea49278b6da9baba56599d","7a32d0547ab74435b372f2168f3f2c1c","b83ee1fbd5b94ff49fbbbec90116f6c9"]},"executionInfo":{"status":"ok","timestamp":1663728377856,"user_tz":180,"elapsed":13,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"92e6a4ea-f104-490e-e5fc-ad4c58c43f34"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Embeddings:   0%|          | 0/21 [00:00<?, ? embedding/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df5f41b76cf34adfa249330e6d59c8b7"}},"metadata":{}}],"source":["# Import das bibliotecas.\n","from tqdm.notebook import tqdm as tqdm_notebook\n","import csv\n","\n","# Recupera o sufixo do nome do arquivo\n","sufixo_arquivo = getSufixoNomeArquivo()\n","#print(\"sufixo_arquivo:\", sufixo_arquivo)\n","\n","NOME_ARQUIVO_META =  DIRETORIO_LOCAL + \"projector/\" + \"DO\" + FILTRO_DO[0] + \"_meta_token_sentenca\" + sufixo_arquivo + \".tsv\"\n","\n","# Abre o arquivo\n","with open(NOME_ARQUIVO_META, 'w', encoding='utf8') as tsvfile:\n","    # Define o escritor do arquivo\n","    writer = csv.writer(tsvfile, delimiter='\\t')\n","\n","    # Cabeçalho do arquivo\n","    # Sem pooling\n","    if POOLING_TOKENS == 0:\n","\n","      # Sem classe\n","      if CLASSE_DOCUMENTO != 2:\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Index\", \"__next__\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                  # Transforma o conta_proximo que é o indicador da sequência em string\n","                  proximo = str(conta_proximo)\n","\n","                  # Incrementa o contador da sequência\n","                  conta_proximo = conta_proximo + 1\n","\n","                  # Monta o registro a ser salvo\n","                  s = [lista_documentos_tokenizado[i][j],\n","                       lista_documentos_id[i],\n","                       lista_documentos_origem[i],\n","                       str(j),\n","                       proximo,\n","                       \"0\",\n","                       lista_documentos_classe[i],\n","                       lista_documentos[i]]\n","\n","                  # Escreve o registro no arquivo\n","                  writer.writerow(s)\n","\n","              # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","              proximo = \"\"\n","\n","              # Escreve o rótulo do documento\n","              # Monta o registro a ser salvo\n","              s = [str(i),\n","                   lista_documentos_id[i],\n","                   lista_documentos_origem[i],\n","                   str(j),\n","                   proximo,\n","                   \"1\",\n","                   lista_documentos_classe[i]+2,\n","                   lista_documentos[i]]\n","\n","              # Escreve o registro no arquivo\n","              writer.writerow(s)\n","\n","        else:\n","          # Sem o link de ligação dos tokens da sentença\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Index\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     str(j),\n","                     \"0\",\n","                     lista_documentos_classe[i],\n","                     lista_documentos[i]]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","          # Escreve o rótulo do documento\n","          # Monta o registro a ser salvo\n","          s = [str(i),\n","               lista_documentos_id[i],\n","               lista_documentos_origem[i],\n","               \"-1\",\n","               \"1\",\n","               lista_documentos_classe[i]+2,\n","               lista_documentos[i]]\n","\n","          # Escreve o registro no arquivo\n","          writer.writerow(s)\n","\n","      else:\n","        # Com classe\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"__next__\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                  # Transforma o conta_proximo que é o indicador da sequência em string\n","                  proximo = str(conta_proximo)\n","\n","                  # Incrementa o contador da sequência\n","                  conta_proximo = conta_proximo + 1\n","\n","                  # Identifica a posição da palavra selecionada para perturbação\n","                  perturbada = \"0\"\n","                  if indice_palavra_perturbada == j:\n","                    perturbada = \"1\"\n","\n","                  # Monta o registro a ser salvo\n","                  s = [lista_documentos_tokenizado[i][j],\n","                       lista_documentos_id[i],\n","                       lista_documentos_origem[i],\n","                       lista_documentos_classe[i],\n","                       perturbada,\n","                       \"-1\",\n","                       proximo,\n","                       \"0\",\n","                       lista_documentos_classe[i],\n","                       lista_documentos[i]\n","                      ]\n","\n","                  # Escreve o registro no arquivo\n","                  writer.writerow(s)\n","\n","              # Escreve o rótulo do documento\n","              # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","              proximo = \"\"\n","\n","              # Incrementa o contador da sequência\n","              conta_proximo = conta_proximo + 1\n","\n","              # Monta o registro a ser salvo\n","              s = [str(i),\n","                   lista_documentos_id[i],\n","                   lista_documentos_origem[i],\n","                   lista_documentos_classe[i],\n","                   \"-1\",\n","                   \"-1\",\n","                   proximo,\n","                   \"1\",\n","                   lista_documentos_classe[i]+2,\n","                   lista_documentos[i]\n","                   ]\n","\n","              # Escreve o registro no arquivo\n","              writer.writerow(s)\n","\n","        else:\n","          # Com Classe\n","          #Sem o link\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Identifica a posição da palavra selecionada para perturbação\n","                perturbada = \"0\"\n","                if indice_palavra_perturbada == j:\n","                  perturbada = \"1\"\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     lista_documentos_classe[i],\n","                     perturbada,\n","                     str(j),\n","                     \"0\",\n","                     lista_documentos_classe[i],\n","                     lista_documentos[i]]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","              # Escreve o rótulo do documento\n","              # Monta o registro a ser salvo\n","              s = [str(i),\n","                   lista_documentos_id[i],\n","                   lista_documentos_origem[i],\n","                   lista_documentos_classe[i],\n","                   \"-1\",\n","                   \"-1\",\n","                   \"1\",\n","                   lista_documentos_classe[i]+2,\n","                   lista_documentos[i]]\n","\n","              # Escreve o registro no arquivo\n","              writer.writerow(s)\n","\n","    else:\n","      # Com polling\n","      # Sem classe\n","      if CLASSE_DOCUMENTO != 2:\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Index\", \"__next__\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Transforma o conta_proximo que é o indicador da sequência em string\n","                proximo = str(conta_proximo)\n","\n","                # Incrementa o contador da sequência\n","                conta_proximo = conta_proximo + 1\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_pos[i][j],\n","                     lista_documentos_tokenizado_oov[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     str(j),\n","                     proximo,\n","                     \"0\",\n","                     lista_documentos_classe[i],\n","                     lista_documentos[i]]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","              # Escreve o rótulo do documento\n","              # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","              proximo = \"\"\n","\n","              # Incrementa o contador da sequência\n","              conta_proximo = conta_proximo + 1\n","\n","              # Monta o registro a ser salvo\n","              s = [str(i),\n","                   lista_documentos_pos[i],\n","                   lista_documentos_tokenizado_oov[i],\n","                   lista_documentos_id[i],\n","                   lista_documentos_origem[i],\n","                   \"-1\",\n","                   proximo,\n","                   \"1\",\n","                   lista_documentos_classe[i]+2,\n","                   lista_documentos[i]]\n","\n","              # Escreve o registro no arquivo\n","              writer.writerow(s)\n","\n","        else:\n","            # Sem link de próximo\n","\n","            # Escreve o cabeçalho do arquivo\n","            writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Index\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","            # Barra de progresso dos embedings\n","            lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","            # Contador da sequência\n","            conta_proximo = 1\n","\n","            # Percorre os embeddings\n","            for i, documento_embedding in lista_embeddings_bar:\n","\n","                # Qtde de tokens do documento\n","                length = len(lista_documentos_tokenizado[i])\n","\n","                # Escreve a palavra e sua sentença\n","                for j in range(length):\n","\n","                  # Monta o registro a ser salvo\n","                  s = [lista_documentos_tokenizado[i][j],\n","                       lista_documentos_pos[i][j],\n","                       lista_documentos_tokenizado_oov[i][j],\n","                       lista_documentos_id[i],\n","                       lista_documentos_origem[i],\n","                       str(j),\n","                       \"0\",\n","                       lista_documentos_classe[i],\n","                       lista_documentos[i]\n","                       ]\n","\n","                  # Escreve o registro no arquivo\n","                  writer.writerow(s)\n","\n","            # Escreve o rótulo do documento\n","            # Monta o registro a ser salvo\n","            s = [str(i),\n","                 lista_documentos_pos[i],\n","                 lista_documentos_tokenizado_oov[i],\n","                 lista_documentos_id[i],\n","                 lista_documentos_origem[i],\n","                 str(i),\n","                 \"1\",\n","                 lista_documentos_classe[i]+2,\n","                 lista_documentos[i]\n","                 ]\n","\n","            # Escreve o registro no arquivo\n","            writer.writerow(s)\n","      else:\n","        # Com classe\n","\n","        # Com o link da sequência de tokens da sentença\n","        if LIGACAO_PROXIMO_TOKEN == True:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"__next__\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\"])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Contador da sequência\n","          conta_proximo = 1\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","\n","                # Transforma o conta_proximo que é o indicador da sequência em string\n","                proximo = str(conta_proximo)\n","\n","                # Incrementa o contador da sequência\n","                conta_proximo = conta_proximo + 1\n","\n","                # Identifica a posição da palavra selecionada para perturbação\n","                perturbada = \"0\"\n","                if indice_palavra_perturbada == j:\n","                  perturbada = \"1\"\n","\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_pos[i][j],\n","                     lista_documentos_tokenizado_oov[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     lista_documentos_classe[i],\n","                     perturbada,\n","                     str(j),\n","                     proximo,\n","                     \"0\",\n","                     lista_documentos_classe[i],\n","                     lista_documentos[i]\n","                    ]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","              # Escreve o rótulo do documento\n","              # Se chegou no último token da sequência coloca Branco para a próximo palavra\n","              proximo = \"\"\n","\n","              # Incrementa o contador da sequência\n","              conta_proximo = conta_proximo + 1\n","\n","              # Monta o registro a ser salvo\n","              s = [str(i),\n","                   lista_documentos_pos[i],\n","                   lista_documentos_tokenizado_oov[i],\n","                   lista_documentos_id[i],\n","                   lista_documentos_origem[i],\n","                   lista_documentos_classe[i],\n","                   \"-1\",\n","                   \"-1\",\n","                   proximo,\n","                   \"1\",\n","                   lista_documentos_classe[i]+2,\n","                   lista_documentos[i]\n","                  ]\n","\n","              # Escreve o registro no arquivo\n","              writer.writerow(s)\n","\n","        else:\n","\n","          # Escreve o cabeçalho do arquivo\n","          writer.writerow([\"Token\", \"POS-Tag\", \"OOV\", \"Id\", \"Origem\", \"Classe\", \"Perturbada\", \"Index\", \"Granularidade\", \"Tipo_Texto\", \"Sentença\" ])\n","\n","          # Barra de progresso dos embedings\n","          lista_embeddings_bar = tqdm_notebook(enumerate(lista_embeddings), desc=f\"Embeddings\", unit=f\" embedding\", total=len(lista_embeddings))\n","\n","          # Percorre os embeddings\n","          for i, documento_embedding in lista_embeddings_bar:\n","\n","              # Qtde de tokens do documento\n","              length = len(lista_documentos_tokenizado[i])\n","\n","              # Procura o índice da palavra selecionada para perturbação no documento\n","              indice_palavra_perturbada = getIndicePerturbacao(lista_documentos_id[i])\n","\n","              # Identifica a posição da palavra selecionada para perturbação\n","              perturbada = \"0\"\n","              if indice_palavra_perturbada == j:\n","                perturbada = \"1\"\n","\n","              # Escreve a palavra e sua sentença\n","              for j in range(length):\n","                # Monta o registro a ser salvo\n","                s = [lista_documentos_tokenizado[i][j],\n","                     lista_documentos_pos[i][j],\n","                     lista_documentos_tokenizado_oov[i][j],\n","                     lista_documentos_id[i],\n","                     lista_documentos_origem[i],\n","                     lista_documentos_classe[i],\n","                     perturbada,\n","                     str(j),\n","                     \"0\",\n","                     lista_documentos_classe[i],\n","                     lista_documentos[i],\n","                    ]\n","\n","                # Escreve o registro no arquivo\n","                writer.writerow(s)\n","\n","              # Escreve o rótulo do documento\n","              # Monta o registro a ser salvo\n","              s = [str(i),\n","                   lista_documentos_pos[i],\n","                   lista_documentos_tokenizado_oov[i],\n","                   lista_documentos_id[i],\n","                   lista_documentos_origem[i],\n","                   lista_documentos_classe[i],\n","                   \"-1\",\n","                   \"-1\",\n","                   \"1\",\n","                   lista_documentos_classe[i]+2,\n","                   lista_documentos[i],\n","                   ]\n","\n","              # Escreve o registro no arquivo\n","              writer.writerow(s)"]},{"cell_type":"markdown","metadata":{"id":"S23zNSjM1VfR"},"source":["Faça o download dos arquivos **records_token_4096.tsv** e **meta_token_4096.tsv** e carregue em https://projector.tensorflow.org/ na opção load.\n","\n","Faça o download dos arquivos gerados pelo notebook clicando na lateral esquerda no ícone \"Arquivos\".\n","\n","Carrega os arquivos na ferramenta através do link \"Load\". Na opção existe um link botão para carregar o arquivo dos embeddings e um outro botão para carregar os metadados.\n","\n","Você também pode utilizar um link a um arquivo de configuração config.json com a referência aos arquivos em algum repositório publico na internet, por exemplo github ou gist\n","\n","Aqui um exemplo.\n","\n","https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/osmarbraz/cohebertv1projecao/main/config.json\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t56ovS3Kt-W0"},"source":["### 5.3.4 Compacta e copia o arquivo do projetor para uma pasta do GoogleDrive"]},{"cell_type":"markdown","metadata":{"id":"MnElyyGbcmlV"},"source":["Compacta o arquivo gerado da comparação para facilitar o envio para o GoogleDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGK7bu_P2F-m"},"outputs":[],"source":["# Nome do arquivo\n","NOME_ARQUIVO_PROJECTOR_COMPACTADO = \"projector.zip\""]},{"cell_type":"markdown","metadata":{"id":"5vZq_1sEtsB0"},"source":["Compacta os arquivos.\n","\n","Usa o zip para compactar:\n","*   `-r` Compacta o diretório\n","*   `-o` sobrescreve o arquivo se existir\n","*   `-j` Não cria nenhum diretório\n","*   `-q` Desliga as mensagens\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NunMOJWR2O8H"},"outputs":[],"source":["!zip -r -o -q \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PROJECTOR_COMPACTADO\" \"$DIRETORIO_LOCAL\"\"/projector/\""]},{"cell_type":"markdown","metadata":{"id":"sw3p4ydkt-W-"},"source":["Copia o arquivo compactado para o GoogleDrive\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5c8sv10t-W_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728402999,"user_tz":180,"elapsed":421,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"e143ecc4-32ee-4d8c-f9f1-88241a2d8ab7"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Terminei a cópia\n"]}],"source":["# Se estiver executando no Google Colaboratory\n","if IN_COLAB:\n","    # Copia o arquivo original\n","    # !cp \"$DIRETORIO_LOCAL$NOME_ARQUIVO_PROJECTOR_COMPACTADO\" \"$DIRETORIO_DRIVE\"\n","\n","    logging.info(\"Terminei a cópia\")"]},{"cell_type":"markdown","metadata":{"id":"NYBu_-xVdHe4"},"source":["## 5.3 Projeção dos embeddings"]},{"cell_type":"markdown","metadata":{"id":"68qT3LkGtRgg"},"source":["### Configuração"]},{"cell_type":"markdown","metadata":{"id":"PHLxY3piwAB-"},"source":["Verifica a versão do tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5A6v8akfdG6j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663728402999,"user_tz":180,"elapsed":305,"user":{"displayName":"Osmar Oliveira Braz Junior","userId":"03010865824982624199"}},"outputId":"83187c47-0174-4099-b9c4-f5168ce49444"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}],"source":["try:\n","  # %tensorflow_version só existe no Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"Xh5VnBHxtUS7"},"source":["Importa a biblioteca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nABca5ukdPE_"},"outputs":[],"source":["# Importa de biblioteca\n","from tensorboard.plugins import projector"]},{"cell_type":"markdown","metadata":{"id":"ix1XJaMqtWqv"},"source":["### Configura o diretório dos logs e arquivos de configuração\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj1SzSWAfZIJ"},"outputs":[],"source":["# Configure um diretório de logs\n","log_dir =\"/content/projector/\"\n","if not os.path.exists(log_dir):\n","    os.makedirs(log_dir)"]},{"cell_type":"markdown","metadata":{"id":"bI0SITtUsZMx"},"source":["### Cria os arquivos de configuração dos embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U9hYPvidYls"},"outputs":[],"source":["# Configura o projetor\n","config = projector.ProjectorConfig()\n","\n","# Configuração do primeiro conjunto de embeddings sem pooling\n","embedding = config.embeddings.add()\n","# Nome do tensor\n","embedding.tensor_name = \"Cohebert: concat 4 últimas camadas pool BERTimbau large\"\n","# Caminho para os metadados\n","embedding.metadata_path = NOME_ARQUIVO_META\n","# Caminho para os tensores\n","embedding.tensor_path = NOME_ARQUIVO_RECORD\n","# Salva o arquivo de configuração\n","projector.visualize_embeddings(log_dir, config)"]},{"cell_type":"markdown","metadata":{"id":"SkrnWNMLvpCq"},"source":["### Mata o processo\n","\n","Se executar novamente o notebook é necessário matar o processo do tensorprojector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgTL4Ns2gINB"},"outputs":[],"source":["# Mata o processo do tensorboard\n","#!kill 407"]},{"cell_type":"markdown","metadata":{"id":"xlXJ7-Uytj3f"},"source":["### Visualizando a projeção\n","\n","Na caixa de seleção selecione \"PROJECTOR\" no lugar de \"INACTIVE\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoAAOlWXdf0t"},"outputs":[],"source":["# Agora execute o tensorboard nos dados de log que acabamos de salvar.\n","# %tensorboard --logdir /content/projector"]}]}